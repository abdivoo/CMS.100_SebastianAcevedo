year,title,body,clean_body,sentiment_label,sentiment_score,tone,ethical_issues,ethical_principles,recommendations,technologies,issue_count,principle_count,recommendation_count,technology_count
2021,Unknown Title,"Dateline: ATLANTA, April 15, 2021 
Body
PR Newswire
 OneTrust, the #1 fastest-growing company on the Inc. 500 and category-defining enterprise platform to operationalize trust, today announced it has completed the acquisition of ethics and compliance leaderConvercent.
Integrating Convercent into OneTrust brings the leading ethics and compliance technology into the OneTrust platform.
Join OneTrust CEO Kabir Barday and Convercent CEO Patrick Quinlan atCONVERGE 21: Workshop Editionas they lay out their joint vision for the future of ethics and compliance
The Convercent technology, 150 employees, 750 customers, and global CONVERGE community will become core to the ethics and compliance offering. The acquisition brings the leading ethics and compliance technology into OneTrust's technology platform of trust, bringing together privacy, data governance, GRC, third-party risk, ESG and ethics and compliance together into a single operational workflow.
Trust and transparency are a competitive advantage and market differentiator. Research firm IDC1 defines organizational trust as ""the intersection of three crucial strategy elements: technology, business, and culture,"" and building and maintaining trust involves bridging consumer expectations, regulatory drivers, and investor expectations. As companies invest in digital transformation, AI, machine learning, and other modern company initiatives, they much ensure innovation builds on, and doesn't violate, user trust and brand promises. 
OneTrust helps organizations operationalize trust. The technology centralizes workflows across privacy, security, data governance, GRC, ethics and compliance, third-party risk, and ESG programs into a unified platform. Integrating Convercent into OneTrust brings the leading ethics and compliance technology into the OneTrust platform. More than 750 enterprise customers, including Airbnb, Under Armour, Kimberly-Clark, and Time Warner, use the Convercent Ethics Cloud Platform, including whistleblowing, policy management, disclosure management, analytics and benchmarking, and learning tools.
""Becoming a part of OneTrust is the beginning of our next chapter in driving ethics into the center of business,"" saidPatrick Quinlan, Convercent CEO. ""We now have broader investment into our platform, our customers, and our community to deliver on our mission to make the world a better place for all stakeholders.""
""We're excited to officially welcome the Convercent team and customers, and the CONVERGE community, into OneTrust,"" saidKabir Barday, OneTrust CEO. ""Together, we're able to deliver for our customers a unified experience that brings together the best of privacy, security, and trust with the leading ethics and compliance platform.""
To learn more about the Convercent acquisition and technology platform of trust,read our blog. To discover how to grow the impact and ROI of your ethics and compliance program, register forCONVERGE21: The Workshop Editiontaking place May 5-6, 2021.
1Source: IDC Perspective, ""Leading with Trust: Enabling the Future of Work"" - Doc # US47359921, April 2021
OneTrust, Convercent, and CONVERGE are registered trademarks or trademarks of OneTrust LLC or its subsidiaries in the United States and other jurisdictions.
About OneTrust
OneTrust is the #1 fastest-growing company on Inc. 500 and the category-defining enterprise platform to operationalize trust. More than 8,000 customers, including half of the Fortune 500, use OneTrust to make trust a competitive differentiator, implementing central agile workflows across privacy, security, data governance, GRC, third-party risk, ethics and compliance, and ESG programs.
The OneTrust platform is backed by 140 patents and powered by the OneTrust Athena AI and robotic automation engine, and capabilities include:
OneTrust Privacy - Privacy Management Software OneTrust DataDiscovery - AI-Powered Discovery and Classification OneTrust DataGovernance - Data Intelligence Software OneTrust Vendorpedia - Third-Party Risk Exchange OneTrust GRC - Integrated Risk Management Software OneTrust Ethics - Ethics and Compliance Software OneTrust PreferenceChoice - Consent and Preference Management Software OneTrust ESG – Environmental, Social & Governance Software
OneTrust has raised $920 million infundingat a $5.3 billion valuation from Insight Partners, Coatue, TCV, SoftBank Vision Fund 2, and Franklin Templeton. OneTrust's fast-growing team of 1,500 employees is co-headquartered in Atlanta and London with additional offices in Bangalore, Melbourne, Denver, Seattle, San Francisco, New York, São Paulo, Munich, Paris, Hong Kong, and Bangkok.
To learn more, visitOneTrust.comor connect onLinkedIn, Twitter, and YouTube.
Media ContactGabrielle Ferree
+1 770-294-4668
media@onetrust.com
 View original content to download multimedia:http://www.prnewswire.com/news-releases/onetrust-completes-acquisition-of-ethics-and-compliance-leader-convercent-301269579.html
SOURCE OneTrust
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: PRESS RELEASES (91%); BUSINESS ETHICS (90%); COMPANY LISTS & RANKINGS (90%); ETHICS (90%); ASSOCIATIONS & ORGANIZATIONS (89%); BENCHMARKING (78%); BUSINESS ANALYTICS (78%); ESG FACTORS (77%); FUTURE OF WORK (77%); WHISTLEBLOWERS (77%); ESG FACTORS - GOVERNANCE (75%); TRADEMARKS (75%); MACHINE LEARNING (73%); TEACHING MATERIALS & MEDIA (73%); ARTIFICIAL INTELLIGENCE (71%); ONETRUST-Convercent (%); TNM Acquisitions; Mergers; Takeovers (%)
Company:  KIMBERLY-CLARK CORP (54%);  UNDER ARMOUR INC (54%); OneTrust
Ticker: KMB (NYSE) (54%); UA (NYSE) (54%)
Industry: NAICS339113 SURGICAL APPLIANCE & SUPPLIES MANUFACTURING (54%); NAICS322291 SANITARY PAPER PRODUCT MANUFACTURING (54%); SIC3842 ORTHOPEDIC, PROSTHETIC, & SURGICAL APPLIANCES & SUPPLIES (54%); SIC2844 PERFUMES, COSMETICS, & OTHER TOILET PREPARATIONS (54%); SIC2676 SANITARY PAPER PRODUCTS (54%); NAICS315240 WOMEN'S, GIRLS' & INFANTS' CUT & SEW APPAREL MANUFACTURING (54%); NAICS315220 MEN'S & BOYS' CUT & SEW APPAREL MANUFACTURING (54%); BUSINESS ANALYTICS (78%); DATA ANALYTICS (78%); DIGITALIZATION & DIGITAL TRANSFORMATION (78%); INFORMATION TECHNOLOGY INDUSTRY (78%); ON DEMAND SERVICES (78%); MACHINE LEARNING (73%); ARTIFICIAL INTELLIGENCE (71%); CPR Computer; Electronics Products (%); STW Computer Software (%)
Geographic: ATLANTA, GA, USA (59%); GEORGIA, USA (79%); UNITED STATES (79%); Georgia
Load-Date: April 15, 2021","PR Newswire
 OneTrust, the #1 fastest-growing company on the Inc. 500 and category-defining enterprise platform to operationalize trust, today announced it has completed the acquisition of ethics and compliance leaderConvercent.
Integrating Convercent into OneTrust brings the leading ethics and compliance technology into the OneTrust platform.
Join OneTrust CEO Kabir Barday and Convercent CEO Patrick Quinlan atCONVERGE 21: Workshop Editionas they lay out their joint vision for the future of ethics and compliance
The Convercent technology, 150 employees, 750 customers, and global CONVERGE community will become core to the ethics and compliance offering. The acquisition brings the leading ethics and compliance technology into OneTrust's technology platform of trust, bringing together privacy, data governance, GRC, third-party risk, ESG and ethics and compliance together into a single operational workflow.
Trust and transparency are a competitive advantage and market differentiator. Research firm IDC1 defines organizational trust as ""the intersection of three crucial strategy elements: technology, business, and culture,"" and building and maintaining trust involves bridging consumer expectations, regulatory drivers, and investor expectations. As companies invest in digital transformation, AI, machine learning, and other modern company initiatives, they much ensure innovation builds on, and doesn't violate, user trust and brand promises. 
OneTrust helps organizations operationalize trust. The technology centralizes workflows across privacy, security, data governance, GRC, ethics and compliance, third-party risk, and ESG programs into a unified platform. Integrating Convercent into OneTrust brings the leading ethics and compliance technology into the OneTrust platform. More than 750 enterprise customers, including Airbnb, Under Armour, Kimberly-Clark, and Time Warner, use the Convercent Ethics Cloud Platform, including whistleblowing, policy management, disclosure management, analytics and benchmarking, and learning tools.
""Becoming a part of OneTrust is the beginning of our next chapter in driving ethics into the center of business,"" saidPatrick Quinlan, Convercent CEO. ""We now have broader investment into our platform, our customers, and our community to deliver on our mission to make the world a better place for all stakeholders.""
""We're excited to officially welcome the Convercent team and customers, and the CONVERGE community, into OneTrust,"" saidKabir Barday, OneTrust CEO. ""Together, we're able to deliver for our customers a unified experience that brings together the best of privacy, security, and trust with the leading ethics and compliance platform.""
To learn more about the Convercent acquisition and technology platform of trust,read our blog. To discover how to grow the impact and ROI of your ethics and compliance program, register forCONVERGE21: The Workshop Editiontaking place May 5-6, 2021.
1Source: IDC Perspective, ""Leading with Trust: Enabling the Future of Work"" - Doc # US47359921, April 2021
OneTrust, Convercent, and CONVERGE are registered trademarks or trademarks of OneTrust LLC or its subsidiaries in the United States and other jurisdictions.
About OneTrust
OneTrust is the #1 fastest-growing company on Inc. 500 and the category-defining enterprise platform to operationalize trust. More than 8,000 customers, including half of the Fortune 500, use OneTrust to make trust a competitive differentiator, implementing central agile workflows across privacy, security, data governance, GRC, third-party risk, ethics and compliance, and ESG programs.
The OneTrust platform is backed by 140 patents and powered by the OneTrust Athena AI and robotic automation engine, and capabilities include:
OneTrust Privacy - Privacy Management Software OneTrust DataDiscovery - AI-Powered Discovery and Classification OneTrust DataGovernance - Data Intelligence Software OneTrust Vendorpedia - Third-Party Risk Exchange OneTrust GRC - Integrated Risk Management Software OneTrust Ethics - Ethics and Compliance Software OneTrust PreferenceChoice - Consent and Preference Management Software OneTrust ESG – Environmental, Social & Governance Software
OneTrust has raised $920 million infundingat a $5.3 billion valuation from Insight Partners, Coatue, TCV, SoftBank Vision Fund 2, and Franklin Templeton. OneTrust's fast-growing team of 1,500 employees is co-headquartered in Atlanta and London with additional offices in Bangalore, Melbourne, Denver, Seattle, San Francisco, New York, São Paulo, Munich, Paris, Hong Kong, and Bangkok.
To learn more, visitOneTrust.comor connect onLinkedIn, Twitter, and YouTube.
Media ContactGabrielle Ferree
+1 770-294-4668
media@onetrust.com
 View original content to download multimedia:http://www.prnewswire.com/news-releases/onetrust-completes-acquisition-of-ethics-and-compliance-leader-convercent-301269579.html
SOURCE OneTrust
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: PRESS RELEASES (91%); BUSINESS ETHICS (90%); COMPANY LISTS & RANKINGS (90%); ETHICS (90%); ASSOCIATIONS & ORGANIZATIONS (89%); BENCHMARKING (78%); BUSINESS ANALYTICS (78%); ESG FACTORS (77%); FUTURE OF WORK (77%); WHISTLEBLOWERS (77%); ESG FACTORS - GOVERNANCE (75%); TRADEMARKS (75%); MACHINE LEARNING (73%); TEACHING MATERIALS & MEDIA (73%); ARTIFICIAL INTELLIGENCE (71%); ONETRUST-Convercent (%); TNM Acquisitions; Mergers; Takeovers (%)
Company:  KIMBERLY-CLARK CORP (54%);  UNDER ARMOUR INC (54%); OneTrust
Ticker: KMB (NYSE) (54%); UA (NYSE) (54%)
Industry: NAICS339113 SURGICAL APPLIANCE & SUPPLIES MANUFACTURING (54%); NAICS322291 SANITARY PAPER PRODUCT MANUFACTURING (54%); SIC3842 ORTHOPEDIC, PROSTHETIC, & SURGICAL APPLIANCES & SUPPLIES (54%); SIC2844 PERFUMES, COSMETICS, & OTHER TOILET PREPARATIONS (54%); SIC2676 SANITARY PAPER PRODUCTS (54%); NAICS315240 WOMEN'S, GIRLS' & INFANTS' CUT & SEW APPAREL MANUFACTURING (54%); NAICS315220 MEN'S & BOYS' CUT & SEW APPAREL MANUFACTURING (54%); BUSINESS ANALYTICS (78%); DATA ANALYTICS (78%); DIGITALIZATION & DIGITAL TRANSFORMATION (78%); INFORMATION TECHNOLOGY INDUSTRY (78%); ON DEMAND SERVICES (78%); MACHINE LEARNING (73%); ARTIFICIAL INTELLIGENCE (71%); CPR Computer; Electronics Products (%); STW Computer Software (%)
Geographic: ATLANTA, GA, USA (59%); GEORGIA, USA (79%); UNITED STATES (79%); Georgia
Load-Date: April 15, 2021",positive,0.590331494808197,balanced/neutral,"['privacy', 'transparency', 'security', 'consent']",[],"['policy', 'governance', 'compliance']",['machine learning'],4,0,3,1
2021,Unknown Title,"Byline: States News Service
Dateline: Paris, France 
Body
The following information was released by the United Nations Educational, Scientific and Cultural Organization (UNESCO):
Research ethics standards promote responsible conduct of research in science and are essential to the development of Science, Technology and Innovation (STI) at all levels. Standards advance the aims of scientific inquiry to foster a research environment that enables scientists to work together toward common goals and boosts public confidence in scientific knowledge and progress for the public good.
More than 100 young students and researchers from the region participated in an online training course on ""Research Ethics Standards: Refashioning Scientific Dialogue"" held during 11th -14th October 2021.
The training aimed at improving the knowledge and practice of research ethics standards, and included lectures on topics ranging from the core values of research, leadership, ethics, roles of bioethics committee, dual-use of biotechnology and ethics, and publication ethics.
Building on a UNESCO initiative in support of the Instituto Nacional de ciencia y teknologia Timor-Leste (INCT) with engagement by Dr. Zabta Khan Shinwari, co-chair of the World Commission on the Ethics of Scientific Knowledge and Technology, and Dr. Ali Talha from Lady Reading Hospital Medical Teaching Institution Pakistan, the lectures will be accessible as an online course on the UNESCO Jakarta e-learning platform.
INCT will officially launch the STI research's ethical standards and online course during the celebration of World Science Day 2021 and invite relevant stakeholders in Timor-Leste to take part. Contributing to the attainment of SDGs 4, 9, and 17, the training highlighted the need to enhance scientific research to encourage innovation towards inclusive development.
For further information, please contact Dr Ai Sugiura (a.sugiura@uneso.org).
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ETHICS (97%); DISTANCE LEARNING (90%); EXPERIMENTATION & RESEARCH (90%); UNITED NATIONS (90%); UNITED NATIONS INSTITUTIONS (90%); ASSOCIATIONS & ORGANIZATIONS (89%); BIOETHICS (78%); SCIENCE & TECHNOLOGY (78%); SUSTAINABLE DEVELOPMENT GOALS (78%); COMPUTER BASED TRAINING (77%); WEB BASED TRAINING (77%); BIOTECHNOLOGY & GENETIC SCIENCE (73%)
Industry: PHARMACEUTICALS & BIOTECHNOLOGY (78%); SUSTAINABLE DEVELOPMENT GOALS (78%)
Geographic: PARIS, FRANCE (59%); TIMOR-LESTE (92%); FRANCE (79%); PAKISTAN (79%)
Load-Date: December 9, 2021","The following information was released by the United Nations Educational, Scientific and Cultural Organization (UNESCO):
Research ethics standards promote responsible conduct of research in science and are essential to the development of Science, Technology and Innovation (STI) at all levels. Standards advance the aims of scientific inquiry to foster a research environment that enables scientists to work together toward common goals and boosts public confidence in scientific knowledge and progress for the public good.
More than 100 young students and researchers from the region participated in an online training course on ""Research Ethics Standards: Refashioning Scientific Dialogue"" held during 11th -14th October 2021.
The training aimed at improving the knowledge and practice of research ethics standards, and included lectures on topics ranging from the core values of research, leadership, ethics, roles of bioethics committee, dual-use of biotechnology and ethics, and publication ethics.
Building on a UNESCO initiative in support of the Instituto Nacional de ciencia y teknologia Timor-Leste (INCT) with engagement by Dr. Zabta Khan Shinwari, co-chair of the World Commission on the Ethics of Scientific Knowledge and Technology, and Dr. Ali Talha from Lady Reading Hospital Medical Teaching Institution Pakistan, the lectures will be accessible as an online course on the UNESCO Jakarta e-learning platform.
INCT will officially launch the STI research's ethical standards and online course during the celebration of World Science Day 2021 and invite relevant stakeholders in Timor-Leste to take part. Contributing to the attainment of SDGs 4, 9, and 17, the training highlighted the need to enhance scientific research to encourage innovation towards inclusive development.
For further information, please contact Dr Ai Sugiura (a.sugiura@uneso.org).
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ETHICS (97%); DISTANCE LEARNING (90%); EXPERIMENTATION & RESEARCH (90%); UNITED NATIONS (90%); UNITED NATIONS INSTITUTIONS (90%); ASSOCIATIONS & ORGANIZATIONS (89%); BIOETHICS (78%); SCIENCE & TECHNOLOGY (78%); SUSTAINABLE DEVELOPMENT GOALS (78%); COMPUTER BASED TRAINING (77%); WEB BASED TRAINING (77%); BIOTECHNOLOGY & GENETIC SCIENCE (73%)
Industry: PHARMACEUTICALS & BIOTECHNOLOGY (78%); SUSTAINABLE DEVELOPMENT GOALS (78%)
Geographic: PARIS, FRANCE (59%); TIMOR-LESTE (92%); FRANCE (79%); PAKISTAN (79%)
Load-Date: December 9, 2021",neutral,0.5883200168609619,balanced/neutral,[],[],"['standards', 'need to']",[],0,0,2,0
2021,Unknown Title,"Body
The article, AI sans ethics can endanger everyone (Feb 2), raises good points.
 No one can disagree that frameworks on the use of artificial intelligence (AI) need to balance public interest and innovation.
 There are areas that need more light shed on them.
 First, there is no clear and lasting definition of what AI is. The term has been around since 1955, but the concept arguably existed before then and has gone through numerous inventions and reinventions since.
 Perhaps the best definition is from Tesler's Theorem: ""AI is whatever hasn't been done yet.""
 Or maybe everything a computer does that we don't understand (yet).
 In the context of ethics, AI definitely is not just an ""algorithm"". Other than the fact that everything a programmable computer does is ultimately made up of algorithms, this oversimplifies the issue.
 For ethics in AI to be relevant, there has to be a degree of autonomous operation.
 However, I find the most important point raised in the article is the implication that human oversight (and explainability) will help keep AI ethical. This is necessary but not sufficient.
 This is best understood by referring to the article, How we fool ourselves - from forgeries to Covid-19 denial (Feb 1). This amply demonstrates how easily humans are led astray by their emotions and biases.
 Biases were not invented by computers or AI, far from it. They are learnt from computers' human masters.
 If anything, judicious use of automation and AI can reduce rather than increase bias.
  The examples given in the ethics article of bad outcomes are not a reflection of issues with AI itself, but of a failure to recognise biases in the data.
 A better example is how young children learn bad habits from the adults around them. Computers are the same.
 In studying the ethics of AI, we need to pay less attention to algorithms, and more attention to the humans who create, influence and use them.
Ian Selbie
Classification
Language: ENGLISH
Publication-Type: Newspaper
Subject: ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ETHICS (90%); HUMAN IN THE LOOP (78%); COUNTERFEITING & FORGERY (73%); CHILDREN, ADOLESCENTS & TEENS (62%); HOLDING COMPANIES (58%); COVID CORONAVIRUS (51%); COVID-19 CORONAVIRUS (51%)
Industry: ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%)
Load-Date: February 3, 2021","The article, AI sans ethics can endanger everyone (Feb 2), raises good points.
 No one can disagree that frameworks on the use of artificial intelligence (AI) need to balance public interest and innovation.
 There are areas that need more light shed on them.
 First, there is no clear and lasting definition of what AI is. The term has been around since 1955, but the concept arguably existed before then and has gone through numerous inventions and reinventions since.
 Perhaps the best definition is from Tesler's Theorem: ""AI is whatever hasn't been done yet.""
 Or maybe everything a computer does that we don't understand (yet).
 In the context of ethics, AI definitely is not just an ""algorithm"". Other than the fact that everything a programmable computer does is ultimately made up of algorithms, this oversimplifies the issue.
 For ethics in AI to be relevant, there has to be a degree of autonomous operation.
 However, I find the most important point raised in the article is the implication that human oversight (and explainability) will help keep AI ethical. This is necessary but not sufficient.
 This is best understood by referring to the article, How we fool ourselves - from forgeries to Covid-19 denial (Feb 1). This amply demonstrates how easily humans are led astray by their emotions and biases.
 Biases were not invented by computers or AI, far from it. They are learnt from computers' human masters.
 If anything, judicious use of automation and AI can reduce rather than increase bias.
  The examples given in the ethics article of bad outcomes are not a reflection of issues with AI itself, but of a failure to recognise biases in the data.
 A better example is how young children learn bad habits from the adults around them. Computers are the same.
 In studying the ethics of AI, we need to pay less attention to algorithms, and more attention to the humans who create, influence and use them.
Ian Selbie
Classification
Language: ENGLISH
Publication-Type: Newspaper
Subject: ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ETHICS (90%); HUMAN IN THE LOOP (78%); COUNTERFEITING & FORGERY (73%); CHILDREN, ADOLESCENTS & TEENS (62%); HOLDING COMPANIES (58%); COVID CORONAVIRUS (51%); COVID-19 CORONAVIRUS (51%)
Industry: ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%)
Load-Date: February 3, 2021",neutral,0.6759783029556274,balanced/neutral,"['bias', 'explainability']",[],"['oversight', 'need to']",['algorithm'],2,0,2,1
2021,Unknown Title,"Dateline: SINGAPORE, Nov. 11, 2021 
Body
PR Newswire
SenseTime, a leading global artificial intelligence (AI) software company, today released the ""AI Sustainable Development Report 2021-2022: AI Ethics for Balanced Development"" and its ""Best Practice Cases"". Launched in conjunction with SFF (Singapore FinTech Festival) x SWITCH in Singapore, the two assets reinforce SenseTime's positive progress towards its long-term goal of contributing towards the sustainable and ethical development of AI.
The ""AI Ethics for Balanced Development"" report follows SenseTime's ""Code of Ethics for AI Sustainable Development"" whitepaper, which was published in June 2020, to further crystalize SenseTime's three core ethical principles of responsible AI: sustainability, human-centric approach, and controllable technology. In addition to this, the ""Best Practice Cases"" is the first set of case studies in the market to document positive developments in the ever-evolving AI industry.
Releasing the ""AI Ethics for Balanced Development"" report and ""Best Practice Cases"" at the SFF x SWITCH event, Mr. Tian Feng, Dean of Intelligent Industry Research Institute, SenseTime, said, ""As AI technology immerses into our everyday lives, people may be exposed to the emergence of new risks. The launch of these two reports demonstrates our commitment to educate both the industry and the public at large, on how the technology can be harnessed ethically. We will continue developing AI technology that advances economies, society and humanity, tackle global challenges as well as The United Nations Sustainable Development Goals (SDGs) such as the COVID-19 pandemic and climate change.""
SFF x SWITCH is week-long festival for the global innovation community, featuring exciting innovation and technology activities. During the conference, SWITCH and Artificial Intelligence International Institute (AIII) co-hosted the ""China Singapore Technovation Ecosystem Summit"", bringing together key players for more sharing and dialogue to foster more sustainable development and collaboration, where SenseTime unveiled its ""AI Ethics for Balanced Development"" report and ""Best Practice Cases"".
Three fundamental principles to make AI fair, safe, controllable
With the rapid development of the technology, AI companies have a responsibility to work with different sectors of the community to monitor, govern and improve potential ethical risks. Over the past few years, SenseTime has been committed to promoting the progress of AI governance. Its whitepaper ""Code of Ethics for Sustainable AI Development"", which laid down the foundation for its AI ethics discourse, was selected by the United Nations as one of the key publication references in its ""United Nations Resource Guide on AI Strategies"".
To align with the development of the technology since its first whitepaper, SenseTime has refined its three core ethical principles for responsible AI in the latest ""AI Ethics for Balanced Development"" report, which calls on institutions, enterprises and individuals to respect and recognise the balance between innovation and ethics:
Sustainability: AI can play an important role augmenting global efforts to address climate change and other sustainability challenges. The United Nations has introduced the 2030 Agenda for Sustainable Development with a global framework to achieve sustainable development in three dimensions – economic, social and environmental – by 2030. As technology continues to evolve, people have realized that AI can play a pivotal role in achieving the Sustainable Development Goals and benefit society. SenseTime's AI technology is widely adopted by infrastructure companies to promote energy efficiency and clean energy. For example, China Southern Power Grid, one of two major power grids in the country, harnesses SenseTime's SenseMARS solution to review its load forecasting. The application of AI in power grids empowers its operators to better optimise temperature and energy efficiency. This is one example of how AI can contribute to the Sustainable Development Goals, by supporting affordable, reliable and sustainable energy supply.  Human-Centric Approach: The purpose of AI is to augment humans and create new value. A broad range of factors such as human rights, privacy protection, inclusiveness and openness must be considered before applying the technology. SenseTime believes that the benefits brought by AI technology must be shared with everyone and must not be limited to select communities or groups. It strives to promote the practice of this approach in various fields to fulfil its social responsibility as a leader in the AI industry, vision and mission.In July 2021, SenseTime established a mixed reality (MR) virtual experience center at BilibiliWorld 2021. With the rise of the ""metaverse"" concept, SenseTime is well-placed to realise its potential through its proprietary AI+MR technology. By creating an extensive and diversified holographic digital world, SenseTime is set to open infinite opportunities for innovation in offline animation exhibitions and interactive entertainment.Controllable Technology: Safeguarding data and promoting trusted use has become a common goal for all technology companies. Key attributes such as verifiability, certification, creditability and responsibility must be in place to ensure adequate levels of safety to build trust; relevant regulation must be founded upon controllable technology to uphold AI ethics.SenseTime has been actively promoting the importance of protecting privacy in the use of AI technology. In 2020, SenseTime published a research paper with the Center for Computational Biomedicine Imaging and Modelling in the Computer Science Department at Rutgers University–New Brunswick at the European Conference on Computer Vision (ECCV), in which the structure of a distributed generative adversarial network (GAN) was adopted to achieve federated learning. By using a distributed asynchronized discriminator GAN with a GAN formed by a central generator, this research allows a central generator to function under which no private data are provided. This set of research methods inherited the strengths of federated learning and effectively solved the problem of privacy protection in medical data.
Robust internal initiatives to reinforce AI ethics efforts
In adherence with the three core ethical principles of responsible AI, SenseTime has implemented a number of initiatives in aspects of its operations. In January 2020, it established the Committee of AI Ethics and Governance, which comprises six members, including two external advisors, who are academic experts in the field of AI ethics, and four senior management members. As the company's primary organization at the ethical level, the AI Ethics Committee is mainly responsible for AI ethics-related responsibilities, including principle formulation, concept publicity, and promoting the implementation of AI ethics measures, and determining SenseTime's goals, policies, work guidance and implementation measures concerning AI ethics.
SenseTime also collaborates with third-party institutions and think tanks from around the world to keep abreast of the latest developments in the AI industry, and to maintain neutrality and objectivity in the implementation of its AI ethics practice.
Sustainability is a never-ending goal, and it will always be a fundamental strategy for SenseTime as it expands and diversifies its technology offerings. Together with partners, SenseTime will continue to work to promote fair, responsible, and legally complaint applications of AI technology.
About SenseTime
SenseTime is a leading AI software company focused on creating a better AI-empowered future through innovation. Upholding a vision of advancing the interconnection of the physical and digital worlds with AI, driving sustainable productivity growth and seamless interactive experiences, SenseTime is committed to advancing the state of the art in AI research, developing scalable and affordable AI software platforms that benefit businesses, people and society, and attracting and nurturing top talents, shaping the future together.
With our roots in the academic world, we invest in our original and cutting-edge research that allows us to offer and continuously improve industry-leading, full-stack AI capabilities, covering key fields across perception intelligence, decision intelligence, AI-enabled content generation and AI-enabled content enhancement, as well as key capabilities in AI chips, sensors and computing infrastructure. Our proprietary AI infrastructure, SenseCore, allows us to develop powerful and efficient AI software platforms that are scalable and adaptable for a wide range of applications.
Today, our technologies are trusted by customers and partners in many industry verticals including Smart Business, Smart City, Smart Life and Smart Auto.
We have offices in markets including Hong Kong, Mainland China, Taiwan, Macau, Japan, Singapore, Saudi Arabia, the United Arab Emirates, Malaysia and South Korea, etc., as well as presences in Thailand, Indonesia and the Philippines. For more information, please visit SenseTime'swebsiteas well as itsLinkedIn,TwitterandFacebook pages.
SOURCE SenseTime
CONTACT: John Dai, SenseTime Group Limited, Email: johndai@sensetime.com  
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ARTIFICIAL INTELLIGENCE ETHICS (94%); ETHICS (94%); SUSTAINABLE DEVELOPMENT (94%); BEST PRACTICES (91%); ARTIFICIAL INTELLIGENCE (90%); PRESS RELEASES (90%); SUSTAINABILITY (90%); SUSTAINABLE DEVELOPMENT GOALS (89%); UNITED NATIONS (89%); UNITED NATIONS INSTITUTIONS (87%); CASE STUDIES (78%); CONFERENCES & CONVENTIONS (78%); FESTIVALS (77%); TRENDS & EVENTS (77%); RESEARCH INSTITUTES (73%); COVID CORONAVIRUS (72%); COVID-19 CORONAVIRUS (63%); INFECTIOUS DISEASE (63%); EPIDEMICS (50%); SenseTime (%); SVY Surveys, polls & research studies (%)
Company:  BEST INC (90%); SenseTime
Ticker: BEST (NYSE) (90%)
Industry: NAICS453998 ALL OTHER MISCELLANEOUS STORE RETAILERS (EXCEPT TOBACCO STORES) (90%); SIC5999 MISCELLANEOUS RETAIL STORES, NEC (90%); ARTIFICIAL INTELLIGENCE ETHICS (94%); SUSTAINABLE DEVELOPMENT (94%); ARTIFICIAL INTELLIGENCE (90%); SOFTWARE SERVICES & APPLICATIONS (90%); SUSTAINABLE DEVELOPMENT GOALS (89%); SOFTWARE MAKERS (78%); FESTIVALS (77%); COMPUTER SOFTWARE (73%); FNT Financial Technology (%); FIN Banking; Financial Services (%); STW Computer Software (%); CPR Computer; Electronics Products (%)
Geographic: ASIA (91%); SINGAPORE (91%); Singapore
Load-Date: November 11, 2021","PR Newswire
SenseTime, a leading global artificial intelligence (AI) software company, today released the ""AI Sustainable Development Report 2021-2022: AI Ethics for Balanced Development"" and its ""Best Practice Cases"". Launched in conjunction with SFF (Singapore FinTech Festival) x SWITCH in Singapore, the two assets reinforce SenseTime's positive progress towards its long-term goal of contributing towards the sustainable and ethical development of AI.
The ""AI Ethics for Balanced Development"" report follows SenseTime's ""Code of Ethics for AI Sustainable Development"" whitepaper, which was published in June 2020, to further crystalize SenseTime's three core ethical principles of responsible AI: sustainability, human-centric approach, and controllable technology. In addition to this, the ""Best Practice Cases"" is the first set of case studies in the market to document positive developments in the ever-evolving AI industry.
Releasing the ""AI Ethics for Balanced Development"" report and ""Best Practice Cases"" at the SFF x SWITCH event, Mr. Tian Feng, Dean of Intelligent Industry Research Institute, SenseTime, said, ""As AI technology immerses into our everyday lives, people may be exposed to the emergence of new risks. The launch of these two reports demonstrates our commitment to educate both the industry and the public at large, on how the technology can be harnessed ethically. We will continue developing AI technology that advances economies, society and humanity, tackle global challenges as well as The United Nations Sustainable Development Goals (SDGs) such as the COVID-19 pandemic and climate change.""
SFF x SWITCH is week-long festival for the global innovation community, featuring exciting innovation and technology activities. During the conference, SWITCH and Artificial Intelligence International Institute (AIII) co-hosted the ""China Singapore Technovation Ecosystem Summit"", bringing together key players for more sharing and dialogue to foster more sustainable development and collaboration, where SenseTime unveiled its ""AI Ethics for Balanced Development"" report and ""Best Practice Cases"".
Three fundamental principles to make AI fair, safe, controllable
With the rapid development of the technology, AI companies have a responsibility to work with different sectors of the community to monitor, govern and improve potential ethical risks. Over the past few years, SenseTime has been committed to promoting the progress of AI governance. Its whitepaper ""Code of Ethics for Sustainable AI Development"", which laid down the foundation for its AI ethics discourse, was selected by the United Nations as one of the key publication references in its ""United Nations Resource Guide on AI Strategies"".
To align with the development of the technology since its first whitepaper, SenseTime has refined its three core ethical principles for responsible AI in the latest ""AI Ethics for Balanced Development"" report, which calls on institutions, enterprises and individuals to respect and recognise the balance between innovation and ethics:
Sustainability: AI can play an important role augmenting global efforts to address climate change and other sustainability challenges. The United Nations has introduced the 2030 Agenda for Sustainable Development with a global framework to achieve sustainable development in three dimensions – economic, social and environmental – by 2030. As technology continues to evolve, people have realized that AI can play a pivotal role in achieving the Sustainable Development Goals and benefit society. SenseTime's AI technology is widely adopted by infrastructure companies to promote energy efficiency and clean energy. For example, China Southern Power Grid, one of two major power grids in the country, harnesses SenseTime's SenseMARS solution to review its load forecasting. The application of AI in power grids empowers its operators to better optimise temperature and energy efficiency. This is one example of how AI can contribute to the Sustainable Development Goals, by supporting affordable, reliable and sustainable energy supply.  Human-Centric Approach: The purpose of AI is to augment humans and create new value. A broad range of factors such as human rights, privacy protection, inclusiveness and openness must be considered before applying the technology. SenseTime believes that the benefits brought by AI technology must be shared with everyone and must not be limited to select communities or groups. It strives to promote the practice of this approach in various fields to fulfil its social responsibility as a leader in the AI industry, vision and mission.In July 2021, SenseTime established a mixed reality (MR) virtual experience center at BilibiliWorld 2021. With the rise of the ""metaverse"" concept, SenseTime is well-placed to realise its potential through its proprietary AI+MR technology. By creating an extensive and diversified holographic digital world, SenseTime is set to open infinite opportunities for innovation in offline animation exhibitions and interactive entertainment.Controllable Technology: Safeguarding data and promoting trusted use has become a common goal for all technology companies. Key attributes such as verifiability, certification, creditability and responsibility must be in place to ensure adequate levels of safety to build trust; relevant regulation must be founded upon controllable technology to uphold AI ethics.SenseTime has been actively promoting the importance of protecting privacy in the use of AI technology. In 2020, SenseTime published a research paper with the Center for Computational Biomedicine Imaging and Modelling in the Computer Science Department at Rutgers University–New Brunswick at the European Conference on Computer Vision (ECCV), in which the structure of a distributed generative adversarial network (GAN) was adopted to achieve federated learning. By using a distributed asynchronized discriminator GAN with a GAN formed by a central generator, this research allows a central generator to function under which no private data are provided. This set of research methods inherited the strengths of federated learning and effectively solved the problem of privacy protection in medical data.
Robust internal initiatives to reinforce AI ethics efforts
In adherence with the three core ethical principles of responsible AI, SenseTime has implemented a number of initiatives in aspects of its operations. In January 2020, it established the Committee of AI Ethics and Governance, which comprises six members, including two external advisors, who are academic experts in the field of AI ethics, and four senior management members. As the company's primary organization at the ethical level, the AI Ethics Committee is mainly responsible for AI ethics-related responsibilities, including principle formulation, concept publicity, and promoting the implementation of AI ethics measures, and determining SenseTime's goals, policies, work guidance and implementation measures concerning AI ethics.
SenseTime also collaborates with third-party institutions and think tanks from around the world to keep abreast of the latest developments in the AI industry, and to maintain neutrality and objectivity in the implementation of its AI ethics practice.
Sustainability is a never-ending goal, and it will always be a fundamental strategy for SenseTime as it expands and diversifies its technology offerings. Together with partners, SenseTime will continue to work to promote fair, responsible, and legally complaint applications of AI technology.
About SenseTime
SenseTime is a leading AI software company focused on creating a better AI-empowered future through innovation. Upholding a vision of advancing the interconnection of the physical and digital worlds with AI, driving sustainable productivity growth and seamless interactive experiences, SenseTime is committed to advancing the state of the art in AI research, developing scalable and affordable AI software platforms that benefit businesses, people and society, and attracting and nurturing top talents, shaping the future together.
With our roots in the academic world, we invest in our original and cutting-edge research that allows us to offer and continuously improve industry-leading, full-stack AI capabilities, covering key fields across perception intelligence, decision intelligence, AI-enabled content generation and AI-enabled content enhancement, as well as key capabilities in AI chips, sensors and computing infrastructure. Our proprietary AI infrastructure, SenseCore, allows us to develop powerful and efficient AI software platforms that are scalable and adaptable for a wide range of applications.
Today, our technologies are trusted by customers and partners in many industry verticals including Smart Business, Smart City, Smart Life and Smart Auto.
We have offices in markets including Hong Kong, Mainland China, Taiwan, Macau, Japan, Singapore, Saudi Arabia, the United Arab Emirates, Malaysia and South Korea, etc., as well as presences in Thailand, Indonesia and the Philippines. For more information, please visit SenseTime'swebsiteas well as itsLinkedIn,TwitterandFacebook pages.
SOURCE SenseTime
CONTACT: John Dai, SenseTime Group Limited, Email: johndai@sensetime.com  
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ARTIFICIAL INTELLIGENCE ETHICS (94%); ETHICS (94%); SUSTAINABLE DEVELOPMENT (94%); BEST PRACTICES (91%); ARTIFICIAL INTELLIGENCE (90%); PRESS RELEASES (90%); SUSTAINABILITY (90%); SUSTAINABLE DEVELOPMENT GOALS (89%); UNITED NATIONS (89%); UNITED NATIONS INSTITUTIONS (87%); CASE STUDIES (78%); CONFERENCES & CONVENTIONS (78%); FESTIVALS (77%); TRENDS & EVENTS (77%); RESEARCH INSTITUTES (73%); COVID CORONAVIRUS (72%); COVID-19 CORONAVIRUS (63%); INFECTIOUS DISEASE (63%); EPIDEMICS (50%); SenseTime (%); SVY Surveys, polls & research studies (%)
Company:  BEST INC (90%); SenseTime
Ticker: BEST (NYSE) (90%)
Industry: NAICS453998 ALL OTHER MISCELLANEOUS STORE RETAILERS (EXCEPT TOBACCO STORES) (90%); SIC5999 MISCELLANEOUS RETAIL STORES, NEC (90%); ARTIFICIAL INTELLIGENCE ETHICS (94%); SUSTAINABLE DEVELOPMENT (94%); ARTIFICIAL INTELLIGENCE (90%); SOFTWARE SERVICES & APPLICATIONS (90%); SUSTAINABLE DEVELOPMENT GOALS (89%); SOFTWARE MAKERS (78%); FESTIVALS (77%); COMPUTER SOFTWARE (73%); FNT Financial Technology (%); FIN Banking; Financial Services (%); STW Computer Software (%); CPR Computer; Electronics Products (%)
Geographic: ASIA (91%); SINGAPORE (91%); Singapore
Load-Date: November 11, 2021",positive,0.6187137365341187,balanced/neutral,"['privacy', 'safety', 'human rights']",[],"['regulation', 'governance', 'framework', 'certification', 'must']",['computer vision'],3,0,5,1
2021,Unknown Title,"Byline: States News Service
Dateline: GARRISON, NY 
Body
The following information was released by the Hastings Center:
The Hastings Center is pleased to announce the election of 24 new fellows. Hastings Center fellows are a group of more than 200 individuals of outstanding accomplishment whose work has informed scholarship and public understanding of complex ethical issues in health, health care, science, and technology. The new fellows come from six countries and a range of disciplines, including medicine, nursing, philosophy, law, American studies, and theater. Their research and other activities encompass diverse areas such as critical care medicine, conflicts of interests, clinical research ethics, genomics, artificial intelligence, philosophy of race, health equity, and social justice.
Laurie A. Badzek, LLM, JD, MS, RN, FNAP, FAAN, is dean and professor at Penn State's College of Nursing. She specializes in genomics education and competency, health care ethics and law, nursing practice, and ethical decision-making. Badzek served as director of the American Nurses Association (ANA) Center for Ethics and Human Rights for over 18 years. She led two substantial revisions to the Code of Ethics for Nursing (2001, 2005) before stepping down in 2017. A tireless champion of using genomics in nursing to enhance patient care, Badzek was a principal investigator of one of the largest U.S. studies examining the translation of genomics to bedside care. She is the recipient of numerous awards, including the International Society of Nurses in Genetics International Genomics Research Award, the American Society of Bioethics and Humanities Cornerstone Award and the ANA Leadership in Ethics Award. She is involved in the development of the Global Genomics Nursing Alliance (G2NA.org) and serves on its steering committee. She is also a steering member on the Chinese University of Hong Kong Asian Pacific Genomic and Genetic Nursing Centre. https://www.nursing.psu.edu/directory/badzek/
Mary Catherine Beach, MD, MPH, is a professor of medicine at the Johns Hopkins University School of Medicine and a core faculty member of the Berman Institute of Bioethics at Johns Hopkins. Dr. Beach has been a Greenwall fellow, a health policy fellow in the office of Senator Hillary Rodham Clinton, and a recipient of the Robert Wood Johnson Foundation's Generalist Physician Faculty Scholar Award. Dr. Beach's scholarship about respect and relationships in health care encompasses both empirical and conceptual dimensions. Her empirical work focuses primarily on respect and communication between patients and clinicians. For the last several years, most of Dr. Beach's research has centered on people living with HIV/AIDS and sickle cell disease, and on how respect is conveyed (or not) in patient medical records. https://bioethics.jhu.edu/people/profile/mary-catherine-beach/
Nikola Biller-Andorno, Dr. med. Dr. phil., is a professor at the University of Zurich, where she directs the Institute of Biomedical Ethics and History of Medicine, a WHO Collaborating Centre for Bioethics. She co-leads the PhD program in biomedical ethics and law and serves as vice-president of the clinical ethics committee of the University Hospital Zurich. She is also a member of the Swiss National Research Council and the Swiss Academy of Medical Sciences. From 2009 to 2011 she served as president of the International Association of Bioethics. She has been a Commonwealth Fund Harkness Fellow (2012- 2013), a Safra Network Fellow (2013-2014) and a visiting professor of biomedical ethics at Harvard University (2012-2014). In 2016, she was elected as Fellow of the Collegium Helveticum, an Institute of Advanced Study sponsored by the University of Zurich, the Federal Institute of Technology Zurich, and the Zurich University of the Arts. https://www.ibme.uzh.ch/en/Biomedical-Ethics/Team/Chair-and-Secretariat/biller-andorno.html
Eric Campbell, PhD, is professor of medicine and director of research at the Center for Bioethics and Humanities at University of Colorado School of Medicine Anschutz. He is a survey scientist who conducts research at the intersection of health policy and bioethics. In recent years, Dr. Campbell's research has focused on professionalism in medicine, conflicts of interest, medical aid-in-dying, equitable care for people with disabilities, and research integrity. His work has been supported by the National Institutes of Health and independent foundations including, the Greenwall Foundation and the Commonwealth Fund. His publications have appeared in JAMA, the New England Journal of Medicine, Annals of Internal Medicine, Health Affairs, and other leading medical and ethics journals. He serves regularly as a resource on bioethics and policy issues for leading media outlets including NPR, CBS, NBC, CNN, Time, the New York Times, Washington Post, and the Boston Globe. https://www.cuanschutz.edu/centers/bioethicshumanities/facultystaff/eric-g-campbell
Mildred Cho, PhD, is a professor in the Division of Medical Genetics of the Department of Pediatrics and in the Division of Primary Care and Population Health of the Department of Medicine at Stanford University. She is also associate director of the Stanford Center for Biomedical Ethics and director of the Center for Integration of Research on Genetics and Ethics. Dr. Cho's major areas of interest are the ethical and social impacts of genetic research and its applications, including precision medicine, gene therapy, the human microbiome, and synthetic biology. Her recent interests include the implications of applying data science, artificial intelligence, and mobile technologies to genomic and health data. https://profiles.stanford.edu/mildred-cho?tab=bio
Bryan Doerries is a New York-based writer, director, and translator who currently serves as the artistic director of Theater of War Productions, a company that presents dramatic readings of seminal plays and texts to frame community conversations about pressing issues of public health and social justice. A self-described evangelist for ancient stories and their relevance to our lives today, Doerries uses age-old approaches to help individuals and communities heal from trauma and loss. His books include The Theater of War: What Ancient Greek Tragedies Can Teach Us Today (Knopf), The Odyssey of Sergeant Jack Brennan (Pantheon), All That You've Seen Here Is God (Vintage), and Oedipus Trilogy (Vintage). Among his awards, he has received an honorary Doctorate of Humane Letters from Kenyon College, and in March 2017, he was named Public Artist in Residence for the City of New York. www.theaterofwar.com
Lisa Eckenwiler, PhD, is a professor of philosophy and, as of January 2022, philosophy department chair at George Mason University. Her research centers broadly on vulnerability and global structural health injustice. She has special interests in humanitarian health ethics, especially issues facing migrants. Some of her current projects focus on the ethical closure of humanitarian projects, a new theoretical framework for humanitarian health ethics (""an ethic of the temporary""), and a collection of case studies and original essays on migration and structural health justice. She also has a large body of work on the ethical significance of place for health justice. She is at work on a book tentatively entitled Placemaking for Health Justice. Her published books include Long-term Care, Globalization and Justice: Migrant Care Workers, Aging, and Families (Johns Hopkins University Press, 2012) and The Ethics of Bioethics: Mapping the Moral Landscape, co-edited with Felicia Cohn (Johns Hopkins University Press, 2007). Dr. Eckenwiler is a co-founder and current chair of the Resisting Borders network; a founding member of the Independent Resource Group for Global Health Justice; a member of the Humanitarian Health Ethics Network. She is a Collaborator with the Ethics and Health section at le Centre de Recherche en Ethiqueat the University of Montreal, and serves as Vice President of the International Association of Bioethics. https://philosophy.gmu.edu/people/leckenwi
Bernice Simone Elger, Dr. med, MA, is a professor at the University of Basel, where she directs the Institute for Biomedical Ethics (IBMB), and a professor in the Center for Legal Medicine at the University of Geneva. She is the president of the XVI World Congress of Bioethics and she served as President of the Swiss Bioethics Society in 2018. She has published widely in medical and ethical journals on many topics, including ethics in genetics, clinical ethics, paternalism and autonomy in medicine, care at the end of life, and the ethics of research involving biobanks and human tissue, as well as about human rights questions related to medical ethics in correctional health care. In 2010, she was awarded the Swiss prize for research in primary care. Her other awards include the University of Geneva's Prix Bizot for her work on biobanks, the Award of the Medical Faculty for her doctorate about medical paternalism, and the Prix Arditi en ethique for her work on predictive medicine. https://ibmb.unibas.ch/en/persons/bernice-simone-elger/cv-of-prof-dr-med-bernice-simone-elger/
Holly Fernandez Lynch, JD, MBE, is an assistant professor of medical ethics in the Department of Medical Ethics and Health Policy at the Perelman School of Medicine, University of Pennsylvania, with a secondary appointment as an assistant professor of law. Professor Fernandez Lynch's scholarship focuses on clinical research ethics and regulation, access to investigational medicines outside clinical trials, Food and Drug Administration policy, and the ethics of gatekeeping in health care. She is the founder and co-chair of the Consortium to Advance Effective Research Ethics Oversight, a group working to evaluate and improve IRB effectiveness. Professor Fernandez Lynch served as a member of the Department of Health and Human Services Secretary's Advisory Committee on Human Research Protections from 2014 to 2019. She currently serves on the boards of Public Responsibility in Medicine and Research (PRIMandR) and the American Society for Law, Medicine, and Ethics. She was a Greenwall Faculty Scholar in 2019 and received the inaugural Baruch A. Brody Award in 2020. https://medicalethicshealthpolicy.med.upenn.edu/faculty-all/holly-fernandez-lynch
Jill A. Fisher, PhD, is professor of social medicine and core faculty in the University of North Carolina Center for Bioethics. Her scholarship and teaching interests center upon how social inequalities are produced or exploited by commercialized medicine in the United States. Dr. Fisher's NIH-funded research examines how clinical trials are conducted and who participates in them as researchers and human subjects. In her 2009 book Medical Research for Hire (Rutgers University Press), she shows how clinical trials have become a revenue stream for physicians and an important source of medical ""care"" for uninsured patients. Her more recent work has explored healthy volunteers' participation in clinical trials. Her 2020 book Adverse Events (NYU Press) analyzes healthy volunteers' participation in drug trials through the lenses of stigma and social inequality. Dr. Fisher has also published on the social construction of Munchausen syndrome, tattooing as a cultural practice, gender and science, hospital tracking and location technologies, nonhuman animal research, and qualitative methods. https://www.jillfisher.net
Faith Fletcher, PhD, MA, is an assistant professor in the Center for Medical Ethics and Health Policy at Baylor College of Medicine, and a senior advisor to The Hastings Center. Nationally, Dr. Fletcher is contributing to critical conversations around health equity, structural racism, medical mistrust, and trustworthiness. Her integrated public health and bioethics research agenda investigates the health care and research experiences of traditionally marginalized populations to inform ethically grounded and community-centered strategies. Dr. Fletcher received training through Fordham University's HIV Research Ethics Training Institute and is a contributor to the American Public Health Association's new Code of Public Health Ethics. She is the outgoing co-chair of the American Society for Bioethics and Humanities RACE Affinity Group, a national special interest group committed to promoting bioethics discourse and collaboration around social and structural disadvantage. In 2017, Dr. Fletcher was named one of the National Minority Quality Forum's 40 under 40 Leaders in Health for her commitment to improving access to scientific research and quality health care for minoritized populations. This prestigious award acknowledges the next generation of leaders primed to reduce health inequities. https://www.bcm.edu/people-search/faith-fletcher-72936
Ann Gallagher, PhD, is head of nursing and professor of care education, ethics, and research at University of Exeter in the U.K. and a visiting scholar at the National Center for Bioethics in Research and Health Care at Tuskegee University in Alabama. She is also a working group member of the Nuffield Council on Bioethics in-depth inquiry on the Future of Ageing. Her research areas include dignity in residential care, innovative approaches to ethics education in elder care, ethics and professional regulation, virtue ethics, love in professional life, professionalism in paramedic practice, and compassion in care. An international leader in ethics as applied to nursing and care, she is committed to professional education, innovation, scholarship, and research that enables individuals, families and communities to flourish. She is a Fellow of the Royal College of Nursing, a Fulbright Scholar, and editor-in-chief of the international journal Nursing Ethics. She is author of over 170 publications, including six books, most recently Slow Ethics and the Art of Care (Emerald Insight, 2020). https://medicine.exeter.ac.uk/people/profile/index.php?web_id=Ann_Gallagher
Anthony Ryan Hatch, PhD, associate professor and chair of the science and society program at Wesleyan University. He teaches, conducts research, and lectures widely on health systems, medical technology, and social inequalities. Dr. Hatch is the author of Silent Cells: The Secret Drugging of Captive America (Minnesota, 2019) and Blood Sugar: Racial Pharmacology and Food Justice in Black America (Minnesota, 2016). In addition to his research, scholarly writing, and teaching, Dr. Hatch is the founding director of Black Box Labs, a research and training laboratory that introduces students to qualitative research methods through faculty-student-community research collaborations. The goal of the lab is to use the methodological insights of science and technology studies to advance just science and social justice. He has been involved in supporting Wesleyan's Center for Prison Education and sustainability and environmental justice pedagogy. https://www.wesleyan.edu/academics/faculty/ahatch/profile.html
Anita Ho, PhD, MPH, is a bioethicist and health services researcher with training and experience in philosophy, clinical and organizational ethics, public health, and business. She is the Northern California regional director of ethics for Providence, a bioethics faculty member at both University of British Columbia and the University of California, San Francisco, and a scientist at the Centre for Health Evaluation and Outcomes Science in Canada. An international scholar with more than 70 publications, she is particularly interested in systemic and social justice issues arising in health care, domestically and internationally. Her current research focuses on ethical dimensions of utilizing innovative and artificial intelligence in health care, research and trial design ethics, supportive decision-making, and end-of-life decisions. She is completing a book for Oxford University Press on the ethics of using artificial intelligence in health monitoring. https://www.spph.ubc.ca/person/anita-ho/
Ping Jia, MA, is founder of the Health Governance Initiative, a nonprofit organization and civic think tank focusing on scientific public health, law, and policy. He is a senior research fellow at the Ren Bioethics Institute at Renmin University in Beijing; a senior research fellow at the Center for the Study of Bioethics at Huazhong University of Science and Technology in Wuhan; and vice president of Chinese Society for Bioethics. Since 2007, he has been Asia Society's Asia 21 fellow. Since 2009, he has been a Young Global Leader of the World Economic Forum. He has written extensively on law, public policy, and social affairs related to public health, biomedicine, and scientific research in China. He drafted the first nationwide compensation plan for blood transfusion-related HIV/AIDS in China in 2011. Since 2014, Jia has helped develop annual satellite sessions on ethics, law, and social problems for China's National Academy Plenary on HIV/AIDS and other infectious diseases sponsored by United Nations and the Chinese government. Jia provided numerous policy recommendations and reports on combating Covid-19, including health system reformation, disease tracking, access to vaccines, and an ethical framework for human challenge trials in China. Jia is an advisory committee member of the Ministry of Science and Technology on National Emergency Management and Scientific Ethics.
Alexander A. Kon, MD, HEC-C, FAAP, FCCM, is the medical director of the pediatric intensive care unit and inpatient pediatrics at Community Children's and chair of the Department of Women's and Children's Services at Community Medical Center in Missoula, Mont. He is also a voluntary clinical professor of pediatrics and affiliate faculty in the Department of Bioethics and Humanities at the University of Washington School of Medicine. He is a pediatric intensivist and bioethicist, and has held numerous national leadership positions, including serving as the president of the American Society for Bioethics and Humanities, chair of the ethics committee of the Society of Critical Care Medicine, and lead of the U.S. Military Health System Healthcare Ethics Working Group. Dr. Kon's work focuses primarily on decision-making for critically ill infants, children, and adults. Dr. Kon introduced the concept of informed nondissent, in which doctors bear the burden of making difficult choices when surrogates prefer to cede authority. This novel concept has been endorsed by major health care organizations and is being adopted nationally. Alexander Kon MD, FAAP, FCCM Community Children's (communitychildrens.org)
Douglas J. Opel, MD, MPH, is an associate professor in the Department of Pediatrics and an adjunct associate professor in the Department of Bioethics and Humanities at the University of Washington School of Medicine. He is also interim director of the Treuman Katz Center for Pediatric Bioethics at Seattle Children's Research Institute and past director of clinical ethics at Seattle Children's Hospital. Dr. Opel's research interests include provider-parent communication, medical decision-making, and public health ethics, with a primary goal of his research being to improve child health by optimizing pediatric decision-making and the provider-parent relationship.He has participated in several national interdisciplinary efforts to address vaccine confidence, public trust in vaccines, and vaccine mandates. Dr. Opel currently sits on the Vaccine Confidence Subcommittee of the Department of Health and Human Services' National Vaccine Advisory Committee, as well as the Lancet Commission on Vaccine Refusal Acceptance, and Demand in the United States. He also served on the American Academy of Pediatrics Committee on Bioethics from 2015 to 2021. Dr. Opel has published articles on pediatric and bioethical issues in major journals, and his work has been featured in major media outlets such as NPR, The New Yorker, and the Los Angeles Times. Douglas J. Opel MD MPH, UW Seattle Children's Profile
Quayshawn Spencer, PhD, is the Robert S. Blank Presidential Associate Professor of Philosophy at the University of Pennsylvania. He specializes in metaphysical issues in the philosophy of science, philosophy of biology, and philosophy of race. Some major issues he's published on are what is the nature of a biologically real object, what is the nature of a scientifically real object, what is the nature of U.S. race talk, and whether there exists any biological racial classification that is useful in medical research. His most recent book is What Is Race? Four Philosophical Views (Oxford University Press, 2019) https://philosophy.sas.upenn.edu/people/quayshawn-spencer
Daniel Fu-Chang Tsai, MD, PhD, is a professor in the Graduate Institute of Medical Education and Bioethics at National Taiwan University College of medicine, an attending physician in the Department of Medical Research at National Taiwan University Hospital, and director of the Center for Biomedical Ethics at National Taiwan University. He president of the Taiwan Association of Institutional Review Boards and a member of an expert advisory committee to the Taiwan government on the Covid-19 pandemic. He served as vice president of the International Association of Bioethics in 2016 to 2017 and is a member of the Merck Ethics Advisory Panel. He was awarded honorary membership by the UNESCO chair in bioethics in 2015 and Goldman-Berland Lectureship in Palliative Medicine in 2019. He has published 20 books and 15 book chapters on bioethics, informed consent, clinical ethics committee, case analysis in medical ethics, research ethics, big data research, and family medicine in Chinese and English. His research interests include cross-cultural bioethics, genetic ethics, transplantation ethics, clinical ethics and ethics consultation, research ethics, research integrity, and medical ethics education. https://scholars.lib.ntu.edu.tw/cris/rp/rp06186
Connie M. Ulrich, PhD, RN, FAAN, MSN, is the Lillian S. Brunner Endowed Chair in Medical and Surgical Nursing and a Professor of Medical Ethics and Health Policy and Nursing at the University of Pennsylvania School of Nursing with a secondary appointment in the Perelman School of Medicine, department of medical ethics and health policy. Dr. Ulrich's research focuses on advancing empirical bioethics in both clinical practice and research. Her current work includes a project that aims to better understand the role and responsibilities of clinical ethicists during Covid-19 and the ethical challenges they face in supporting clinicians, patients, and families. She is also the principal investigator of a bioethics educational grant that aims to develop and train nurse and physician bioethicists in Tanzania. Dr. Ulrich has served on several data safety and monitoring boards appointed by the National Institutes of Health and other organizations. In 2015, she provided testimony to the Presidential Commission for the Study of Bioethics on the value of nursing to the public and to public discourse on ethical issues; the ethical issues that nurses encounter that require bioethics education; and the role of bioethics education in preparing the next generation of nursing professionals. https://www.nursing.upenn.edu/details/profiles.php?id=58
Rebecca L. Walker, PhD, is a professor of social medicine and of philosophy at the University of North Carolina at Chapel Hill, where she is also faculty in the Center for Bioethics, a fellow in the Parr Center for Ethics, and adjunct professor of public policy. Her contributions to bioethics have been in animal research ethics, human healthy volunteer research, genomic advances, respect for autonomy, and health justice. Her work on the ethics of biomedical research using nonhuman animals departs from the typical focus on questions of rights or welfare, which tend to lead to polarizing perspectives. Instead, she promotes a virtue ethical approach, which allows for attention to the ethical issues arising internally to animal research practices. Among her publications, Dr. Walker has co-edited the third edition of the Social Medicine Reader (Duke University Press, 2019), Health Inequalities and Justice: New Conversations Across the Disciplines (UNC Press 2016) and Working Virtue: Virtue Ethics and Contemporary Moral Problems (Oxford University Press, 2007). https://rebeccawalker.web.unc.edu
Laura Wexler, PhD, is the Charles H. Farnam Professor of American studies and women's, gender, and sexuality studies at Yale. She writes about intersections of race, gender, sexuality, class, and nation within the photographic cultures of the United States, and yet views bioethics through a broad global lens. Among her many books and other publications, while simultaneously viewing challenges to human flourishing through a broad global lens. Among her many books and other publications, she is the author of the award-winning book, Tender Violence: Domestic Visions in an Age of U.S. Imperialism. She is the founding director of the Photographic Memory Workshop at Yale, a founding member of the steering committee of the Feminist Technology Network (FemTechNet), and a member of the Photogrammar Project, which has made a web-based interactive research system for visualizing the more than 170,00 photographs created by the Farm Security Administration and Office of War Information between 1935 and 1945. Her work introduces photography and visual culture as a medium through which powerful transdisciplinary conversations about race, gender, sexuality, class, and nation are generated, and new possibilities for analysis and expression emerge across the disciplines. https://wgss.yale.edu/people/laura-wexler
Douglas White, MD, MAS, is vice chair and a professor of critical care medicine, medicine, and clinical and translational science in the University of Pittsburgh School of Medicine. He holds the UPMC Endowed Chair for Ethics in Critical Care Medicine. He is the founding director of the University of Pittsburgh Program on Ethics and Decision-Making in Critical Illness, the nation's first research program focused on ethical issues in ICUs. His research team uses empirical methods and normative analysis to examine ethical challenges that arise in the care of critically ill patients. He has received numerous honors for his scholarly contributions to the field, including induction into the American Society for Clinical Investigation and the Greensick Award for Ethics from the Society of Critical Care Medicine. He chaired the National Institutes of Health's Societal and Ethical Issues in Research study section and co-chaired a National Academy of Medicine panel on improving collaboration between clinicians, patients, and family members. He has served as an external advisor to the World Health Organization, the California Department of Public Health, and the Commonwealth of Pennsylvania. https://www.ccm.pitt.edu/?q=content/white-douglas
Bryn Williams-Jones, PhD, is professor of bioethics and director of the Department of Social and Preventive Medicine in the School of Public Health at the University of Montreal. Dr. Williams-Jones is interested in the socio-ethical and policy implications of health innovations in diverse contexts. His work examines the conflicts that arise in academic research and professional practice with a view to developing ethical tools to manage these conflicts when they cannot be avoided. Current projects focus on issues in professional ethics, public health ethics, research integrity, and ethics education. Dr. Williams-Jones heads the Laboratory for Experimental Bioethics (LaBioethX); shares responsibility for the Ethics, Governance, and Democracy branch of the International Observatory on the Societal Impacts of AI and Digital Technology (OBVIA); and is editor-in-chief of the Canadian Journal of Bioethics. Bryn WILLIAMS-JONES La recherche Universite de Montreal (umontreal.ca)
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: BIOTECHNOLOGY & GENETIC SCIENCE (95%); ETHICS (94%); NURSES & NURSING (92%); GENOMICS (91%); HUMAN SUBJECTS (90%); INVESTIGATIONS (90%); BIOETHICS (89%); COLLEGE & UNIVERSITY PROFESSORS (89%); SCHOLARSHIPS & GRANTS (89%); LEGISLATIVE BODIES (79%); HEALTH CARE INFORMATION (78%); HEALTH CARE POLICY (78%); AIDS & HIV (77%); BIOMEDICINE (77%); COLLEGES & UNIVERSITIES (77%); DISEASES & DISORDERS (77%); HEALTH CARE REGULATION & POLICY (77%); HEALTH EQUITY (77%); LEVELS OF CARE (77%); MEDICAL ETHICS (77%); MEDICAL RESEARCH (77%); MEDICAL SCIENCE (77%); PUBLIC POLICY (77%); SOCIAL JUSTICE (77%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (77%); EXPERIMENTATION & RESEARCH (76%); HUMANITIES & SOCIAL SCIENCE (76%); MEDICAL RECORDS (73%); FOUNDATIONS (72%); ETHNIC & CULTURAL STUDIES (71%); MEDICAL EDUCATION (70%); ARTIFICIAL INTELLIGENCE (55%); SICKLE CELL DISEASE (50%)
Organization: HASTINGS CENTER (94%); AMERICAN NURSES ASSOCIATION (56%)
Industry: NURSES & NURSING (92%); COLLEGE & UNIVERSITY PROFESSORS (89%); HEALTH CARE INFORMATION (78%); HEALTH CARE POLICY (78%); NURSING & RESIDENTIAL CARE FACILITIES (78%); BIOMEDICINE (77%); COLLEGES & UNIVERSITIES (77%); HEALTH CARE REGULATION & POLICY (77%); HEALTH EQUITY (77%); MEDICAL RECORDS (73%); ARTIFICIAL INTELLIGENCE (55%)
Person: HILLARY RODHAM CLINTON (78%)
Geographic: UNITED STATES (93%); ASIA (79%)
Load-Date: December 1, 2021","The following information was released by the Hastings Center:
The Hastings Center is pleased to announce the election of 24 new fellows. Hastings Center fellows are a group of more than 200 individuals of outstanding accomplishment whose work has informed scholarship and public understanding of complex ethical issues in health, health care, science, and technology. The new fellows come from six countries and a range of disciplines, including medicine, nursing, philosophy, law, American studies, and theater. Their research and other activities encompass diverse areas such as critical care medicine, conflicts of interests, clinical research ethics, genomics, artificial intelligence, philosophy of race, health equity, and social justice.
Laurie A. Badzek, LLM, JD, MS, RN, FNAP, FAAN, is dean and professor at Penn State's College of Nursing. She specializes in genomics education and competency, health care ethics and law, nursing practice, and ethical decision-making. Badzek served as director of the American Nurses Association (ANA) Center for Ethics and Human Rights for over 18 years. She led two substantial revisions to the Code of Ethics for Nursing (2001, 2005) before stepping down in 2017. A tireless champion of using genomics in nursing to enhance patient care, Badzek was a principal investigator of one of the largest U.S. studies examining the translation of genomics to bedside care. She is the recipient of numerous awards, including the International Society of Nurses in Genetics International Genomics Research Award, the American Society of Bioethics and Humanities Cornerstone Award and the ANA Leadership in Ethics Award. She is involved in the development of the Global Genomics Nursing Alliance (G2NA.org) and serves on its steering committee. She is also a steering member on the Chinese University of Hong Kong Asian Pacific Genomic and Genetic Nursing Centre. https://www.nursing.psu.edu/directory/badzek/
Mary Catherine Beach, MD, MPH, is a professor of medicine at the Johns Hopkins University School of Medicine and a core faculty member of the Berman Institute of Bioethics at Johns Hopkins. Dr. Beach has been a Greenwall fellow, a health policy fellow in the office of Senator Hillary Rodham Clinton, and a recipient of the Robert Wood Johnson Foundation's Generalist Physician Faculty Scholar Award. Dr. Beach's scholarship about respect and relationships in health care encompasses both empirical and conceptual dimensions. Her empirical work focuses primarily on respect and communication between patients and clinicians. For the last several years, most of Dr. Beach's research has centered on people living with HIV/AIDS and sickle cell disease, and on how respect is conveyed (or not) in patient medical records. https://bioethics.jhu.edu/people/profile/mary-catherine-beach/
Nikola Biller-Andorno, Dr. med. Dr. phil., is a professor at the University of Zurich, where she directs the Institute of Biomedical Ethics and History of Medicine, a WHO Collaborating Centre for Bioethics. She co-leads the PhD program in biomedical ethics and law and serves as vice-president of the clinical ethics committee of the University Hospital Zurich. She is also a member of the Swiss National Research Council and the Swiss Academy of Medical Sciences. From 2009 to 2011 she served as president of the International Association of Bioethics. She has been a Commonwealth Fund Harkness Fellow (2012- 2013), a Safra Network Fellow (2013-2014) and a visiting professor of biomedical ethics at Harvard University (2012-2014). In 2016, she was elected as Fellow of the Collegium Helveticum, an Institute of Advanced Study sponsored by the University of Zurich, the Federal Institute of Technology Zurich, and the Zurich University of the Arts. https://www.ibme.uzh.ch/en/Biomedical-Ethics/Team/Chair-and-Secretariat/biller-andorno.html
Eric Campbell, PhD, is professor of medicine and director of research at the Center for Bioethics and Humanities at University of Colorado School of Medicine Anschutz. He is a survey scientist who conducts research at the intersection of health policy and bioethics. In recent years, Dr. Campbell's research has focused on professionalism in medicine, conflicts of interest, medical aid-in-dying, equitable care for people with disabilities, and research integrity. His work has been supported by the National Institutes of Health and independent foundations including, the Greenwall Foundation and the Commonwealth Fund. His publications have appeared in JAMA, the New England Journal of Medicine, Annals of Internal Medicine, Health Affairs, and other leading medical and ethics journals. He serves regularly as a resource on bioethics and policy issues for leading media outlets including NPR, CBS, NBC, CNN, Time, the New York Times, Washington Post, and the Boston Globe. https://www.cuanschutz.edu/centers/bioethicshumanities/facultystaff/eric-g-campbell
Mildred Cho, PhD, is a professor in the Division of Medical Genetics of the Department of Pediatrics and in the Division of Primary Care and Population Health of the Department of Medicine at Stanford University. She is also associate director of the Stanford Center for Biomedical Ethics and director of the Center for Integration of Research on Genetics and Ethics. Dr. Cho's major areas of interest are the ethical and social impacts of genetic research and its applications, including precision medicine, gene therapy, the human microbiome, and synthetic biology. Her recent interests include the implications of applying data science, artificial intelligence, and mobile technologies to genomic and health data. https://profiles.stanford.edu/mildred-cho?tab=bio
Bryan Doerries is a New York-based writer, director, and translator who currently serves as the artistic director of Theater of War Productions, a company that presents dramatic readings of seminal plays and texts to frame community conversations about pressing issues of public health and social justice. A self-described evangelist for ancient stories and their relevance to our lives today, Doerries uses age-old approaches to help individuals and communities heal from trauma and loss. His books include The Theater of War: What Ancient Greek Tragedies Can Teach Us Today (Knopf), The Odyssey of Sergeant Jack Brennan (Pantheon), All That You've Seen Here Is God (Vintage), and Oedipus Trilogy (Vintage). Among his awards, he has received an honorary Doctorate of Humane Letters from Kenyon College, and in March 2017, he was named Public Artist in Residence for the City of New York. www.theaterofwar.com
Lisa Eckenwiler, PhD, is a professor of philosophy and, as of January 2022, philosophy department chair at George Mason University. Her research centers broadly on vulnerability and global structural health injustice. She has special interests in humanitarian health ethics, especially issues facing migrants. Some of her current projects focus on the ethical closure of humanitarian projects, a new theoretical framework for humanitarian health ethics (""an ethic of the temporary""), and a collection of case studies and original essays on migration and structural health justice. She also has a large body of work on the ethical significance of place for health justice. She is at work on a book tentatively entitled Placemaking for Health Justice. Her published books include Long-term Care, Globalization and Justice: Migrant Care Workers, Aging, and Families (Johns Hopkins University Press, 2012) and The Ethics of Bioethics: Mapping the Moral Landscape, co-edited with Felicia Cohn (Johns Hopkins University Press, 2007). Dr. Eckenwiler is a co-founder and current chair of the Resisting Borders network; a founding member of the Independent Resource Group for Global Health Justice; a member of the Humanitarian Health Ethics Network. She is a Collaborator with the Ethics and Health section at le Centre de Recherche en Ethiqueat the University of Montreal, and serves as Vice President of the International Association of Bioethics. https://philosophy.gmu.edu/people/leckenwi
Bernice Simone Elger, Dr. med, MA, is a professor at the University of Basel, where she directs the Institute for Biomedical Ethics (IBMB), and a professor in the Center for Legal Medicine at the University of Geneva. She is the president of the XVI World Congress of Bioethics and she served as President of the Swiss Bioethics Society in 2018. She has published widely in medical and ethical journals on many topics, including ethics in genetics, clinical ethics, paternalism and autonomy in medicine, care at the end of life, and the ethics of research involving biobanks and human tissue, as well as about human rights questions related to medical ethics in correctional health care. In 2010, she was awarded the Swiss prize for research in primary care. Her other awards include the University of Geneva's Prix Bizot for her work on biobanks, the Award of the Medical Faculty for her doctorate about medical paternalism, and the Prix Arditi en ethique for her work on predictive medicine. https://ibmb.unibas.ch/en/persons/bernice-simone-elger/cv-of-prof-dr-med-bernice-simone-elger/
Holly Fernandez Lynch, JD, MBE, is an assistant professor of medical ethics in the Department of Medical Ethics and Health Policy at the Perelman School of Medicine, University of Pennsylvania, with a secondary appointment as an assistant professor of law. Professor Fernandez Lynch's scholarship focuses on clinical research ethics and regulation, access to investigational medicines outside clinical trials, Food and Drug Administration policy, and the ethics of gatekeeping in health care. She is the founder and co-chair of the Consortium to Advance Effective Research Ethics Oversight, a group working to evaluate and improve IRB effectiveness. Professor Fernandez Lynch served as a member of the Department of Health and Human Services Secretary's Advisory Committee on Human Research Protections from 2014 to 2019. She currently serves on the boards of Public Responsibility in Medicine and Research (PRIMandR) and the American Society for Law, Medicine, and Ethics. She was a Greenwall Faculty Scholar in 2019 and received the inaugural Baruch A. Brody Award in 2020. https://medicalethicshealthpolicy.med.upenn.edu/faculty-all/holly-fernandez-lynch
Jill A. Fisher, PhD, is professor of social medicine and core faculty in the University of North Carolina Center for Bioethics. Her scholarship and teaching interests center upon how social inequalities are produced or exploited by commercialized medicine in the United States. Dr. Fisher's NIH-funded research examines how clinical trials are conducted and who participates in them as researchers and human subjects. In her 2009 book Medical Research for Hire (Rutgers University Press), she shows how clinical trials have become a revenue stream for physicians and an important source of medical ""care"" for uninsured patients. Her more recent work has explored healthy volunteers' participation in clinical trials. Her 2020 book Adverse Events (NYU Press) analyzes healthy volunteers' participation in drug trials through the lenses of stigma and social inequality. Dr. Fisher has also published on the social construction of Munchausen syndrome, tattooing as a cultural practice, gender and science, hospital tracking and location technologies, nonhuman animal research, and qualitative methods. https://www.jillfisher.net
Faith Fletcher, PhD, MA, is an assistant professor in the Center for Medical Ethics and Health Policy at Baylor College of Medicine, and a senior advisor to The Hastings Center. Nationally, Dr. Fletcher is contributing to critical conversations around health equity, structural racism, medical mistrust, and trustworthiness. Her integrated public health and bioethics research agenda investigates the health care and research experiences of traditionally marginalized populations to inform ethically grounded and community-centered strategies. Dr. Fletcher received training through Fordham University's HIV Research Ethics Training Institute and is a contributor to the American Public Health Association's new Code of Public Health Ethics. She is the outgoing co-chair of the American Society for Bioethics and Humanities RACE Affinity Group, a national special interest group committed to promoting bioethics discourse and collaboration around social and structural disadvantage. In 2017, Dr. Fletcher was named one of the National Minority Quality Forum's 40 under 40 Leaders in Health for her commitment to improving access to scientific research and quality health care for minoritized populations. This prestigious award acknowledges the next generation of leaders primed to reduce health inequities. https://www.bcm.edu/people-search/faith-fletcher-72936
Ann Gallagher, PhD, is head of nursing and professor of care education, ethics, and research at University of Exeter in the U.K. and a visiting scholar at the National Center for Bioethics in Research and Health Care at Tuskegee University in Alabama. She is also a working group member of the Nuffield Council on Bioethics in-depth inquiry on the Future of Ageing. Her research areas include dignity in residential care, innovative approaches to ethics education in elder care, ethics and professional regulation, virtue ethics, love in professional life, professionalism in paramedic practice, and compassion in care. An international leader in ethics as applied to nursing and care, she is committed to professional education, innovation, scholarship, and research that enables individuals, families and communities to flourish. She is a Fellow of the Royal College of Nursing, a Fulbright Scholar, and editor-in-chief of the international journal Nursing Ethics. She is author of over 170 publications, including six books, most recently Slow Ethics and the Art of Care (Emerald Insight, 2020). https://medicine.exeter.ac.uk/people/profile/index.php?web_id=Ann_Gallagher
Anthony Ryan Hatch, PhD, associate professor and chair of the science and society program at Wesleyan University. He teaches, conducts research, and lectures widely on health systems, medical technology, and social inequalities. Dr. Hatch is the author of Silent Cells: The Secret Drugging of Captive America (Minnesota, 2019) and Blood Sugar: Racial Pharmacology and Food Justice in Black America (Minnesota, 2016). In addition to his research, scholarly writing, and teaching, Dr. Hatch is the founding director of Black Box Labs, a research and training laboratory that introduces students to qualitative research methods through faculty-student-community research collaborations. The goal of the lab is to use the methodological insights of science and technology studies to advance just science and social justice. He has been involved in supporting Wesleyan's Center for Prison Education and sustainability and environmental justice pedagogy. https://www.wesleyan.edu/academics/faculty/ahatch/profile.html
Anita Ho, PhD, MPH, is a bioethicist and health services researcher with training and experience in philosophy, clinical and organizational ethics, public health, and business. She is the Northern California regional director of ethics for Providence, a bioethics faculty member at both University of British Columbia and the University of California, San Francisco, and a scientist at the Centre for Health Evaluation and Outcomes Science in Canada. An international scholar with more than 70 publications, she is particularly interested in systemic and social justice issues arising in health care, domestically and internationally. Her current research focuses on ethical dimensions of utilizing innovative and artificial intelligence in health care, research and trial design ethics, supportive decision-making, and end-of-life decisions. She is completing a book for Oxford University Press on the ethics of using artificial intelligence in health monitoring. https://www.spph.ubc.ca/person/anita-ho/
Ping Jia, MA, is founder of the Health Governance Initiative, a nonprofit organization and civic think tank focusing on scientific public health, law, and policy. He is a senior research fellow at the Ren Bioethics Institute at Renmin University in Beijing; a senior research fellow at the Center for the Study of Bioethics at Huazhong University of Science and Technology in Wuhan; and vice president of Chinese Society for Bioethics. Since 2007, he has been Asia Society's Asia 21 fellow. Since 2009, he has been a Young Global Leader of the World Economic Forum. He has written extensively on law, public policy, and social affairs related to public health, biomedicine, and scientific research in China. He drafted the first nationwide compensation plan for blood transfusion-related HIV/AIDS in China in 2011. Since 2014, Jia has helped develop annual satellite sessions on ethics, law, and social problems for China's National Academy Plenary on HIV/AIDS and other infectious diseases sponsored by United Nations and the Chinese government. Jia provided numerous policy recommendations and reports on combating Covid-19, including health system reformation, disease tracking, access to vaccines, and an ethical framework for human challenge trials in China. Jia is an advisory committee member of the Ministry of Science and Technology on National Emergency Management and Scientific Ethics.
Alexander A. Kon, MD, HEC-C, FAAP, FCCM, is the medical director of the pediatric intensive care unit and inpatient pediatrics at Community Children's and chair of the Department of Women's and Children's Services at Community Medical Center in Missoula, Mont. He is also a voluntary clinical professor of pediatrics and affiliate faculty in the Department of Bioethics and Humanities at the University of Washington School of Medicine. He is a pediatric intensivist and bioethicist, and has held numerous national leadership positions, including serving as the president of the American Society for Bioethics and Humanities, chair of the ethics committee of the Society of Critical Care Medicine, and lead of the U.S. Military Health System Healthcare Ethics Working Group. Dr. Kon's work focuses primarily on decision-making for critically ill infants, children, and adults. Dr. Kon introduced the concept of informed nondissent, in which doctors bear the burden of making difficult choices when surrogates prefer to cede authority. This novel concept has been endorsed by major health care organizations and is being adopted nationally. Alexander Kon MD, FAAP, FCCM Community Children's (communitychildrens.org)
Douglas J. Opel, MD, MPH, is an associate professor in the Department of Pediatrics and an adjunct associate professor in the Department of Bioethics and Humanities at the University of Washington School of Medicine. He is also interim director of the Treuman Katz Center for Pediatric Bioethics at Seattle Children's Research Institute and past director of clinical ethics at Seattle Children's Hospital. Dr. Opel's research interests include provider-parent communication, medical decision-making, and public health ethics, with a primary goal of his research being to improve child health by optimizing pediatric decision-making and the provider-parent relationship.He has participated in several national interdisciplinary efforts to address vaccine confidence, public trust in vaccines, and vaccine mandates. Dr. Opel currently sits on the Vaccine Confidence Subcommittee of the Department of Health and Human Services' National Vaccine Advisory Committee, as well as the Lancet Commission on Vaccine Refusal Acceptance, and Demand in the United States. He also served on the American Academy of Pediatrics Committee on Bioethics from 2015 to 2021. Dr. Opel has published articles on pediatric and bioethical issues in major journals, and his work has been featured in major media outlets such as NPR, The New Yorker, and the Los Angeles Times. Douglas J. Opel MD MPH, UW Seattle Children's Profile
Quayshawn Spencer, PhD, is the Robert S. Blank Presidential Associate Professor of Philosophy at the University of Pennsylvania. He specializes in metaphysical issues in the philosophy of science, philosophy of biology, and philosophy of race. Some major issues he's published on are what is the nature of a biologically real object, what is the nature of a scientifically real object, what is the nature of U.S. race talk, and whether there exists any biological racial classification that is useful in medical research. His most recent book is What Is Race? Four Philosophical Views (Oxford University Press, 2019) https://philosophy.sas.upenn.edu/people/quayshawn-spencer
Daniel Fu-Chang Tsai, MD, PhD, is a professor in the Graduate Institute of Medical Education and Bioethics at National Taiwan University College of medicine, an attending physician in the Department of Medical Research at National Taiwan University Hospital, and director of the Center for Biomedical Ethics at National Taiwan University. He president of the Taiwan Association of Institutional Review Boards and a member of an expert advisory committee to the Taiwan government on the Covid-19 pandemic. He served as vice president of the International Association of Bioethics in 2016 to 2017 and is a member of the Merck Ethics Advisory Panel. He was awarded honorary membership by the UNESCO chair in bioethics in 2015 and Goldman-Berland Lectureship in Palliative Medicine in 2019. He has published 20 books and 15 book chapters on bioethics, informed consent, clinical ethics committee, case analysis in medical ethics, research ethics, big data research, and family medicine in Chinese and English. His research interests include cross-cultural bioethics, genetic ethics, transplantation ethics, clinical ethics and ethics consultation, research ethics, research integrity, and medical ethics education. https://scholars.lib.ntu.edu.tw/cris/rp/rp06186
Connie M. Ulrich, PhD, RN, FAAN, MSN, is the Lillian S. Brunner Endowed Chair in Medical and Surgical Nursing and a Professor of Medical Ethics and Health Policy and Nursing at the University of Pennsylvania School of Nursing with a secondary appointment in the Perelman School of Medicine, department of medical ethics and health policy. Dr. Ulrich's research focuses on advancing empirical bioethics in both clinical practice and research. Her current work includes a project that aims to better understand the role and responsibilities of clinical ethicists during Covid-19 and the ethical challenges they face in supporting clinicians, patients, and families. She is also the principal investigator of a bioethics educational grant that aims to develop and train nurse and physician bioethicists in Tanzania. Dr. Ulrich has served on several data safety and monitoring boards appointed by the National Institutes of Health and other organizations. In 2015, she provided testimony to the Presidential Commission for the Study of Bioethics on the value of nursing to the public and to public discourse on ethical issues; the ethical issues that nurses encounter that require bioethics education; and the role of bioethics education in preparing the next generation of nursing professionals. https://www.nursing.upenn.edu/details/profiles.php?id=58
Rebecca L. Walker, PhD, is a professor of social medicine and of philosophy at the University of North Carolina at Chapel Hill, where she is also faculty in the Center for Bioethics, a fellow in the Parr Center for Ethics, and adjunct professor of public policy. Her contributions to bioethics have been in animal research ethics, human healthy volunteer research, genomic advances, respect for autonomy, and health justice. Her work on the ethics of biomedical research using nonhuman animals departs from the typical focus on questions of rights or welfare, which tend to lead to polarizing perspectives. Instead, she promotes a virtue ethical approach, which allows for attention to the ethical issues arising internally to animal research practices. Among her publications, Dr. Walker has co-edited the third edition of the Social Medicine Reader (Duke University Press, 2019), Health Inequalities and Justice: New Conversations Across the Disciplines (UNC Press 2016) and Working Virtue: Virtue Ethics and Contemporary Moral Problems (Oxford University Press, 2007). https://rebeccawalker.web.unc.edu
Laura Wexler, PhD, is the Charles H. Farnam Professor of American studies and women's, gender, and sexuality studies at Yale. She writes about intersections of race, gender, sexuality, class, and nation within the photographic cultures of the United States, and yet views bioethics through a broad global lens. Among her many books and other publications, while simultaneously viewing challenges to human flourishing through a broad global lens. Among her many books and other publications, she is the author of the award-winning book, Tender Violence: Domestic Visions in an Age of U.S. Imperialism. She is the founding director of the Photographic Memory Workshop at Yale, a founding member of the steering committee of the Feminist Technology Network (FemTechNet), and a member of the Photogrammar Project, which has made a web-based interactive research system for visualizing the more than 170,00 photographs created by the Farm Security Administration and Office of War Information between 1935 and 1945. Her work introduces photography and visual culture as a medium through which powerful transdisciplinary conversations about race, gender, sexuality, class, and nation are generated, and new possibilities for analysis and expression emerge across the disciplines. https://wgss.yale.edu/people/laura-wexler
Douglas White, MD, MAS, is vice chair and a professor of critical care medicine, medicine, and clinical and translational science in the University of Pittsburgh School of Medicine. He holds the UPMC Endowed Chair for Ethics in Critical Care Medicine. He is the founding director of the University of Pittsburgh Program on Ethics and Decision-Making in Critical Illness, the nation's first research program focused on ethical issues in ICUs. His research team uses empirical methods and normative analysis to examine ethical challenges that arise in the care of critically ill patients. He has received numerous honors for his scholarly contributions to the field, including induction into the American Society for Clinical Investigation and the Greensick Award for Ethics from the Society of Critical Care Medicine. He chaired the National Institutes of Health's Societal and Ethical Issues in Research study section and co-chaired a National Academy of Medicine panel on improving collaboration between clinicians, patients, and family members. He has served as an external advisor to the World Health Organization, the California Department of Public Health, and the Commonwealth of Pennsylvania. https://www.ccm.pitt.edu/?q=content/white-douglas
Bryn Williams-Jones, PhD, is professor of bioethics and director of the Department of Social and Preventive Medicine in the School of Public Health at the University of Montreal. Dr. Williams-Jones is interested in the socio-ethical and policy implications of health innovations in diverse contexts. His work examines the conflicts that arise in academic research and professional practice with a view to developing ethical tools to manage these conflicts when they cannot be avoided. Current projects focus on issues in professional ethics, public health ethics, research integrity, and ethics education. Dr. Williams-Jones heads the Laboratory for Experimental Bioethics (LaBioethX); shares responsibility for the Ethics, Governance, and Democracy branch of the International Observatory on the Societal Impacts of AI and Digital Technology (OBVIA); and is editor-in-chief of the Canadian Journal of Bioethics. Bryn WILLIAMS-JONES La recherche Universite de Montreal (umontreal.ca)
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: BIOTECHNOLOGY & GENETIC SCIENCE (95%); ETHICS (94%); NURSES & NURSING (92%); GENOMICS (91%); HUMAN SUBJECTS (90%); INVESTIGATIONS (90%); BIOETHICS (89%); COLLEGE & UNIVERSITY PROFESSORS (89%); SCHOLARSHIPS & GRANTS (89%); LEGISLATIVE BODIES (79%); HEALTH CARE INFORMATION (78%); HEALTH CARE POLICY (78%); AIDS & HIV (77%); BIOMEDICINE (77%); COLLEGES & UNIVERSITIES (77%); DISEASES & DISORDERS (77%); HEALTH CARE REGULATION & POLICY (77%); HEALTH EQUITY (77%); LEVELS OF CARE (77%); MEDICAL ETHICS (77%); MEDICAL RESEARCH (77%); MEDICAL SCIENCE (77%); PUBLIC POLICY (77%); SOCIAL JUSTICE (77%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (77%); EXPERIMENTATION & RESEARCH (76%); HUMANITIES & SOCIAL SCIENCE (76%); MEDICAL RECORDS (73%); FOUNDATIONS (72%); ETHNIC & CULTURAL STUDIES (71%); MEDICAL EDUCATION (70%); ARTIFICIAL INTELLIGENCE (55%); SICKLE CELL DISEASE (50%)
Organization: HASTINGS CENTER (94%); AMERICAN NURSES ASSOCIATION (56%)
Industry: NURSES & NURSING (92%); COLLEGE & UNIVERSITY PROFESSORS (89%); HEALTH CARE INFORMATION (78%); HEALTH CARE POLICY (78%); NURSING & RESIDENTIAL CARE FACILITIES (78%); BIOMEDICINE (77%); COLLEGES & UNIVERSITIES (77%); HEALTH CARE REGULATION & POLICY (77%); HEALTH EQUITY (77%); MEDICAL RECORDS (73%); ARTIFICIAL INTELLIGENCE (55%)
Person: HILLARY RODHAM CLINTON (78%)
Geographic: UNITED STATES (93%); ASIA (79%)
Load-Date: December 1, 2021",positive,0.7753426432609558,balanced/neutral,"['safety', 'security', 'human rights', 'autonomy', 'consent', 'access', 'inequality']","['virtue ethics', 'justice', 'equity', 'autonomy', 'dignity', 'justice']","['regulation', 'policy', 'governance', 'oversight', 'framework', 'law']",['llm'],7,6,6,1
2021,Unknown Title,"Byline: Sohini Bagchi
Body
The growing sophistication and ubiquity of Artificial intelligence (AI) applications has raised a number of ethical concerns. These include issues of bias, safety, transparency, and accountability. These ethical issues concerning AI have caught the interest of many - individuals, businesses, and governments - in recent years. The COVID-19 pandemic has further raised specific concerns about how to apply digital or data ethics to decision-making processes when collecting, using and sharing data about employees. In fact, organizations are now expected to be even more watchful against AI dangers if they hope to leverage the many opportunities AI offers to the business.
Throwing light on some of the key ethical challenges in AI, Shubhangi Vashisth, Senior Principal Analyst on Artificial Intelligence at Gartner, says, ""While ethics should be a core part of AI and other digital programs, the problem is it's often treated as an afterthought.""
Recognizing the challenges in AI ethics
AI-systems often deliver biased results. For example, a search-engine technology also upholds biases of the real world, stereotyping gender roles. Or a hiring tool reinforces racial discrimination or entrenches these prejudices against certain communities. Oftentimes users and developers are not aware of the system's process to reach the output. This opacity increases the bias in datasets and decision systems, believes Vashisth.
""The other problem is that AI systems are black boxes that are based on the inability to fully understand why the algorithms behind the AI work the way they do. Lack of accountability, auditing, and engagement also reduce opportunities for human perception, further amplifying issues related to trust and transparency,"" she says.
If individuals and teams are cognizant of the existence of bias, then they have the necessary tools at the data, algorithm, and human levels to build a more responsible AI.
AI systems should be beneficial for businesses, society, and the environment on the whole, According to Vashisth, ""Ethical AI should follow principles such as fairness, reliability, safety, privacy, security, and inclusiveness. It should provide transparency and accountability. But it is often a challenge to attain these principles.
Fairness for example means that the AI systems should be inclusive, accessible and should not have unfair discrimination against individuals, communities, or groups. It should provide equitable access and treatment to everyone. Bringing more diversity into the team can however mitigate the bias often developed by algorithms, according to Vashisth.
AI systems should respect privacy rights and data protection, which in turn ensures the security of data. Ethical AI designed system provides proper data governance and model management systems. While designing systems, developers should be trained to build system keeping privacy and security as primary concerns. Besides, AI systems should reliably work in accordance with their intended purpose.
Vashisth, who presented her views on 'AI Governance and Responsible AI' at the Gartner Data & Analytics Summit for India that took place virtually on 4th & 5th August, this year, says, ""Creating explainability is among several important steps enterprises must embed in their AI operations in order to make responsible, ethical AI a part of doing business. A key to making it work is to ensure that ethics is included in every part of the organization's AI process and involves the stakeholders, including your employees, customers, shareholders and board of directors. The system provides opportunities for feedback and dialogues for making the process better.""
Mitigating AI's ethical dilemma
While it remains a dilemma on who is answerable in an organization on questions regarding ethical AI, Vashisth believes that it is important for companies to have AI ethicists to help them think through the ethics of AI development and deployment. An AI Ethicist advises on ethical AI practice and guards against bias, unintended consequences and ensures accountability within the organization.
""Also companies should have a formal code of ethics that lays out their principles, processes, and ways of handling ethical aspects of AI development. Those codes should be made public on the company's websites so that stakeholders and external parties understand the company's views on ethical AI,"" she says.
Vashisth believes that AI goes beyond the development of traditional product lines with narrow social implications. With its potential to distort basic human values, it is crucial to train people in how to think about AI.
Also, Ethical AI system should value human diversity, freedom, autonomy, and rights. According to her, ""Firms should have AI training programs that not only address the technical aspects of development, but the ethical, legal, or societal ramifications. That would help software developers understand that they are not merely acting on their own individual values, but are part of a broader society with a stake in AI development.""
Ethical AI underway
On a positive note, Vashisth observes some of the world's biggest corporations such as Google, Microsoft, Amazon, Facebook, Apple, and IBM among others, have joined in the discussions as well.
Google, for example, has published a document calling for the ""responsible development of AI."" It said AI should be socially beneficial, not reinforce unfair bias, should be tested for safety, should be accountable to people, should incorporate privacy design, should uphold high standards of scientific excellence, and should be available for uses that accord with those principles.
Microsoft meanwhile published an extensive report on ""the future computed."" It laid out the opportunities for AI, the need for ""principles, policies and laws for the responsible use of AI,"" and noted the possible ramifications for the future of jobs and work.
These groups seeks to develop industry best practices to guide AI development with the objective of promoting ""ethics, fairness and inclusivity; transparency, privacy, and interoperability; collaboration between people and AI systems; and the trustworthiness, reliability and robustness of the technology.""
Many firms also have AI audit trails explaining how particular algorithms were put together and its possible outcome. These audits provide some degree of transparency and explainability.
There is no doubt that responsible AI is expected to become a key focus area for many organizations in the coming years. As Vashisth believes, responsible AI practices can drive competitive advantage and business resilience and make the organization more attractive to talent.
""In the process, they must consider carefully and proactively the necessary governance mechanisms to be used to ensure ethical considerations in the deployment of AI tools,"" she says.
Building trust in technological solutions and tools is vital so that businesses and individuals can benefit from its use. Certainly, we don't want a Hiroshima moment in AI for the world to take notice and act.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (96%); ARTIFICIAL INTELLIGENCE ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); DISCRIMINATION (89%); NEGATIVE SOCIETAL NEWS (89%); SAFETY (89%); PRIVACY RIGHTS (84%); DATA ANALYTICS (78%); RACE & ETHNICITY (78%); SAFETY, ACCIDENTS & DISASTERS (78%); RACIAL DISCRIMINATION IN EMPLOYMENT (76%); ASSOCIATIONS & ORGANIZATIONS (75%); RACISM & XENOPHOBIA (73%); COVID CORONAVIRUS (72%); COVID-19 CORONAVIRUS (72%); INFECTIOUS DISEASE (72%); EPIDEMICS (57%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); DATA SECURITY (89%); INFORMATION SECURITY & PRIVACY (89%); DATA ANALYTICS (78%); DATA GOVERNANCE & STEWARDSHIP (76%); SEARCH ENGINES (67%)
Load-Date: July 18, 2023","The growing sophistication and ubiquity of Artificial intelligence (AI) applications has raised a number of ethical concerns. These include issues of bias, safety, transparency, and accountability. These ethical issues concerning AI have caught the interest of many - individuals, businesses, and governments - in recent years. The COVID-19 pandemic has further raised specific concerns about how to apply digital or data ethics to decision-making processes when collecting, using and sharing data about employees. In fact, organizations are now expected to be even more watchful against AI dangers if they hope to leverage the many opportunities AI offers to the business.
Throwing light on some of the key ethical challenges in AI, Shubhangi Vashisth, Senior Principal Analyst on Artificial Intelligence at Gartner, says, ""While ethics should be a core part of AI and other digital programs, the problem is it's often treated as an afterthought.""
Recognizing the challenges in AI ethics
AI-systems often deliver biased results. For example, a search-engine technology also upholds biases of the real world, stereotyping gender roles. Or a hiring tool reinforces racial discrimination or entrenches these prejudices against certain communities. Oftentimes users and developers are not aware of the system's process to reach the output. This opacity increases the bias in datasets and decision systems, believes Vashisth.
""The other problem is that AI systems are black boxes that are based on the inability to fully understand why the algorithms behind the AI work the way they do. Lack of accountability, auditing, and engagement also reduce opportunities for human perception, further amplifying issues related to trust and transparency,"" she says.
If individuals and teams are cognizant of the existence of bias, then they have the necessary tools at the data, algorithm, and human levels to build a more responsible AI.
AI systems should be beneficial for businesses, society, and the environment on the whole, According to Vashisth, ""Ethical AI should follow principles such as fairness, reliability, safety, privacy, security, and inclusiveness. It should provide transparency and accountability. But it is often a challenge to attain these principles.
Fairness for example means that the AI systems should be inclusive, accessible and should not have unfair discrimination against individuals, communities, or groups. It should provide equitable access and treatment to everyone. Bringing more diversity into the team can however mitigate the bias often developed by algorithms, according to Vashisth.
AI systems should respect privacy rights and data protection, which in turn ensures the security of data. Ethical AI designed system provides proper data governance and model management systems. While designing systems, developers should be trained to build system keeping privacy and security as primary concerns. Besides, AI systems should reliably work in accordance with their intended purpose.
Vashisth, who presented her views on 'AI Governance and Responsible AI' at the Gartner Data & Analytics Summit for India that took place virtually on 4th & 5th August, this year, says, ""Creating explainability is among several important steps enterprises must embed in their AI operations in order to make responsible, ethical AI a part of doing business. A key to making it work is to ensure that ethics is included in every part of the organization's AI process and involves the stakeholders, including your employees, customers, shareholders and board of directors. The system provides opportunities for feedback and dialogues for making the process better.""
Mitigating AI's ethical dilemma
While it remains a dilemma on who is answerable in an organization on questions regarding ethical AI, Vashisth believes that it is important for companies to have AI ethicists to help them think through the ethics of AI development and deployment. An AI Ethicist advises on ethical AI practice and guards against bias, unintended consequences and ensures accountability within the organization.
""Also companies should have a formal code of ethics that lays out their principles, processes, and ways of handling ethical aspects of AI development. Those codes should be made public on the company's websites so that stakeholders and external parties understand the company's views on ethical AI,"" she says.
Vashisth believes that AI goes beyond the development of traditional product lines with narrow social implications. With its potential to distort basic human values, it is crucial to train people in how to think about AI.
Also, Ethical AI system should value human diversity, freedom, autonomy, and rights. According to her, ""Firms should have AI training programs that not only address the technical aspects of development, but the ethical, legal, or societal ramifications. That would help software developers understand that they are not merely acting on their own individual values, but are part of a broader society with a stake in AI development.""
Ethical AI underway
On a positive note, Vashisth observes some of the world's biggest corporations such as Google, Microsoft, Amazon, Facebook, Apple, and IBM among others, have joined in the discussions as well.
Google, for example, has published a document calling for the ""responsible development of AI."" It said AI should be socially beneficial, not reinforce unfair bias, should be tested for safety, should be accountable to people, should incorporate privacy design, should uphold high standards of scientific excellence, and should be available for uses that accord with those principles.
Microsoft meanwhile published an extensive report on ""the future computed."" It laid out the opportunities for AI, the need for ""principles, policies and laws for the responsible use of AI,"" and noted the possible ramifications for the future of jobs and work.
These groups seeks to develop industry best practices to guide AI development with the objective of promoting ""ethics, fairness and inclusivity; transparency, privacy, and interoperability; collaboration between people and AI systems; and the trustworthiness, reliability and robustness of the technology.""
Many firms also have AI audit trails explaining how particular algorithms were put together and its possible outcome. These audits provide some degree of transparency and explainability.
There is no doubt that responsible AI is expected to become a key focus area for many organizations in the coming years. As Vashisth believes, responsible AI practices can drive competitive advantage and business resilience and make the organization more attractive to talent.
""In the process, they must consider carefully and proactively the necessary governance mechanisms to be used to ensure ethical considerations in the deployment of AI tools,"" she says.
Building trust in technological solutions and tools is vital so that businesses and individuals can benefit from its use. Certainly, we don't want a Hiroshima moment in AI for the world to take notice and act.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (96%); ARTIFICIAL INTELLIGENCE ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); DISCRIMINATION (89%); NEGATIVE SOCIETAL NEWS (89%); SAFETY (89%); PRIVACY RIGHTS (84%); DATA ANALYTICS (78%); RACE & ETHNICITY (78%); SAFETY, ACCIDENTS & DISASTERS (78%); RACIAL DISCRIMINATION IN EMPLOYMENT (76%); ASSOCIATIONS & ORGANIZATIONS (75%); RACISM & XENOPHOBIA (73%); COVID CORONAVIRUS (72%); COVID-19 CORONAVIRUS (72%); INFECTIOUS DISEASE (72%); EPIDEMICS (57%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); DATA SECURITY (89%); INFORMATION SECURITY & PRIVACY (89%); DATA ANALYTICS (78%); DATA GOVERNANCE & STEWARDSHIP (76%); SEARCH ENGINES (67%)
Load-Date: July 18, 2023",neutral,0.6691176891326904,balanced/neutral,"['privacy', 'bias', 'discrimination', 'fairness', 'transparency', 'explainability', 'accountability', 'safety', 'security', 'autonomy', 'inclusivity', 'access']","['fairness', 'autonomy']","['governance', 'standards', 'audit', 'should', 'must']",['algorithm'],12,2,5,1
2021,Unknown Title,"Body
Oct. 27 -- Sberbank issued the following news release:
In the framework of the international forum Ethics of Artificial Intelligence: The Beginning of Trust, Russia's largest tech companies adopted an AI code of ethics, which was drafted by the Artificial Intelligence Alliance. The document was signed by Sber, Yandex, MTS, VK, Gazprom Neft, and the Russian Direct Investment Fund (RDIF), as well as leading companies and research organizations. First Deputy Chief of Staff of the Presidential Executive Office Sergey Kiriyenko and Deputy Prime Minister Dmitry Chernyshenko were present at the signing.
The code of ethics enshrines a human-centric and humanistic approach to the development of AI technology, the principles of non-discrimination, data security, and information security, identification of AI in communication with humans, respect for human autonomy, and responsibility for the consequences of using AI. Commitment to the code of ethics will be a key social responsibility element for companies developing and implementing AI technology in Russia.
The AI Code of Ethics is to become a recommendation document on ethics for all AI market participants: government, business, Russian and foreign developers. The code establishes general ethical principles and behavioral standards, which the participants may choose to adhere to in the field of artificial intelligence.
As a tool for implementing the aforementioned principles and provisions, the code provides for the creation of a National AI Ethics Commission, the appointment of ethics commissioners, as well as the possibility of creating collegial industry ethics bodies. The Alliance will also develop methodological recommendations and a set of best and/or worst practices for dealing with emerging ethical issues in the AI life cycle.
Alexander Vedyakhin, first deputy chairman of the executive board, Sberbank:
""Many people consider AI technology to be controversial: on the one hand, it is a powerful driver of development for all of humanity; on the other hand, AI has a certain degree of autonomy in its decision-making, and it is not always obvious how these decisions are made. Today we have before us a great challenge - developing trust in AI technology. This is impossible without defining the rules for its ethical use. I am certain that the code we have adopted, in the drafting of which Sber has taken an active part, will be an important step in building trust in AI among our citizens and, as a result, in the development of the entire domestic artificial intelligence industry.""
Vyacheslav Nikolaev, president, MTS:
""In the near future, it is solutions based on AI technology that will provide a real breakthrough in terms of the digitalization of healthcare and education and the realization of everyday tasks. At the same time, we can confidently say that AI is the technology that causes the most alarm in society. Now is the time to define the principles that will help us use AI technology with maximum benefit to society. At MTS, we have always paid particular attention to ethical issues in the development of products, including those that use artificial intelligence, and we are happy to share this experience during the preparation of the Russian AI Code of Ethics.""
Anatoly Braverman, first deputy CEO, Russian Direct Investment Fund (RDIF):
""The shaping and development of the AI market requires effective interaction between its participants. The joint preparation, adoption, and future application of the AI Code of Ethics is an important stage in the formation of key infrastructure tools and the creation of success stories for developers, users, and investors in the field of artificial intelligence. Together with its co-investors and portfolio companies, RDIF has traditionally paid great attention to ethics and reputation issues, and it is ready to actively share its own best practices and experience and adopt those of others, too.""
The document was prepared with the support of the Presidential Executive Office of the Russian Federation, the Analytical Center for the Government of the Russian Federation, and the Ministry of Economic Development. It underwent expert and public discussion on platforms hosted by Digital Economy, the Federation Council, and the Civic Chamber.
Click here(https://a-ai.ru/code-of-ethics/) to learn more about the AI Code of Ethics.
Source: Sberbank
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (97%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (89%); BOARDS OF DIRECTORS (78%); BUSINESS ETHICS (78%); BUSINESS ETHICS & CORPORATE CITIZENSHIP (78%); FUNDS & INVESTMENT TRUSTS (78%); GOVERNMENT & PUBLIC ADMINISTRATION (75%); HEADS OF STATE & GOVERNMENT (75%); NEGATIVE SOCIETAL NEWS (74%); ESG FACTORS - SOCIAL (73%); APPOINTMENTS (71%); ASSOCIATIONS & ORGANIZATIONS (71%); PRIME MINISTERS (70%)
Company:  OAO GAZPROM NEFT (72%);  SBERBANK ROSSII OAO (58%);  YANDEX NV (58%)
Ticker: SIBN (RTS) (72%); GAZ (LSE) (72%); SBER (RTS) (58%); SBER (LSE) (58%); YNDX (NASDAQ) (58%)
Industry: NAICS211130 NATURAL GAS EXTRACTION (72%); NAICS211120 CRUDE PETROLEUM EXTRACTION (72%); SIC1311 CRUDE PETROLEUM & NATURAL GAS (72%); NAICS522293 INTERNATIONAL TRADE FINANCING (58%); NAICS522110 COMMERCIAL BANKING (58%); SIC6082 FOREIGN TRADE & INTERNATIONAL BANKING INSTITUTIONS (58%); SIC6021 NATIONAL COMMERCIAL BANKS (58%); NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (58%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); BIG TECH (90%); INFORMATION TECHNOLOGY INDUSTRY (90%); DIGITALIZATION & DIGITAL TRANSFORMATION (78%); FUNDS & INVESTMENT TRUSTS (78%); DATA SECURITY (74%); INFORMATION SECURITY & PRIVACY (69%)
Geographic: RUSSIAN FEDERATION (90%)
Load-Date: October 27, 2021","Oct. 27 -- Sberbank issued the following news release:
In the framework of the international forum Ethics of Artificial Intelligence: The Beginning of Trust, Russia's largest tech companies adopted an AI code of ethics, which was drafted by the Artificial Intelligence Alliance. The document was signed by Sber, Yandex, MTS, VK, Gazprom Neft, and the Russian Direct Investment Fund (RDIF), as well as leading companies and research organizations. First Deputy Chief of Staff of the Presidential Executive Office Sergey Kiriyenko and Deputy Prime Minister Dmitry Chernyshenko were present at the signing.
The code of ethics enshrines a human-centric and humanistic approach to the development of AI technology, the principles of non-discrimination, data security, and information security, identification of AI in communication with humans, respect for human autonomy, and responsibility for the consequences of using AI. Commitment to the code of ethics will be a key social responsibility element for companies developing and implementing AI technology in Russia.
The AI Code of Ethics is to become a recommendation document on ethics for all AI market participants: government, business, Russian and foreign developers. The code establishes general ethical principles and behavioral standards, which the participants may choose to adhere to in the field of artificial intelligence.
As a tool for implementing the aforementioned principles and provisions, the code provides for the creation of a National AI Ethics Commission, the appointment of ethics commissioners, as well as the possibility of creating collegial industry ethics bodies. The Alliance will also develop methodological recommendations and a set of best and/or worst practices for dealing with emerging ethical issues in the AI life cycle.
Alexander Vedyakhin, first deputy chairman of the executive board, Sberbank:
""Many people consider AI technology to be controversial: on the one hand, it is a powerful driver of development for all of humanity; on the other hand, AI has a certain degree of autonomy in its decision-making, and it is not always obvious how these decisions are made. Today we have before us a great challenge - developing trust in AI technology. This is impossible without defining the rules for its ethical use. I am certain that the code we have adopted, in the drafting of which Sber has taken an active part, will be an important step in building trust in AI among our citizens and, as a result, in the development of the entire domestic artificial intelligence industry.""
Vyacheslav Nikolaev, president, MTS:
""In the near future, it is solutions based on AI technology that will provide a real breakthrough in terms of the digitalization of healthcare and education and the realization of everyday tasks. At the same time, we can confidently say that AI is the technology that causes the most alarm in society. Now is the time to define the principles that will help us use AI technology with maximum benefit to society. At MTS, we have always paid particular attention to ethical issues in the development of products, including those that use artificial intelligence, and we are happy to share this experience during the preparation of the Russian AI Code of Ethics.""
Anatoly Braverman, first deputy CEO, Russian Direct Investment Fund (RDIF):
""The shaping and development of the AI market requires effective interaction between its participants. The joint preparation, adoption, and future application of the AI Code of Ethics is an important stage in the formation of key infrastructure tools and the creation of success stories for developers, users, and investors in the field of artificial intelligence. Together with its co-investors and portfolio companies, RDIF has traditionally paid great attention to ethics and reputation issues, and it is ready to actively share its own best practices and experience and adopt those of others, too.""
The document was prepared with the support of the Presidential Executive Office of the Russian Federation, the Analytical Center for the Government of the Russian Federation, and the Ministry of Economic Development. It underwent expert and public discussion on platforms hosted by Digital Economy, the Federation Council, and the Civic Chamber.
Click here(https://a-ai.ru/code-of-ethics/) to learn more about the AI Code of Ethics.
Source: Sberbank
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (97%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (89%); BOARDS OF DIRECTORS (78%); BUSINESS ETHICS (78%); BUSINESS ETHICS & CORPORATE CITIZENSHIP (78%); FUNDS & INVESTMENT TRUSTS (78%); GOVERNMENT & PUBLIC ADMINISTRATION (75%); HEADS OF STATE & GOVERNMENT (75%); NEGATIVE SOCIETAL NEWS (74%); ESG FACTORS - SOCIAL (73%); APPOINTMENTS (71%); ASSOCIATIONS & ORGANIZATIONS (71%); PRIME MINISTERS (70%)
Company:  OAO GAZPROM NEFT (72%);  SBERBANK ROSSII OAO (58%);  YANDEX NV (58%)
Ticker: SIBN (RTS) (72%); GAZ (LSE) (72%); SBER (RTS) (58%); SBER (LSE) (58%); YNDX (NASDAQ) (58%)
Industry: NAICS211130 NATURAL GAS EXTRACTION (72%); NAICS211120 CRUDE PETROLEUM EXTRACTION (72%); SIC1311 CRUDE PETROLEUM & NATURAL GAS (72%); NAICS522293 INTERNATIONAL TRADE FINANCING (58%); NAICS522110 COMMERCIAL BANKING (58%); SIC6082 FOREIGN TRADE & INTERNATIONAL BANKING INSTITUTIONS (58%); SIC6021 NATIONAL COMMERCIAL BANKS (58%); NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (58%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); BIG TECH (90%); INFORMATION TECHNOLOGY INDUSTRY (90%); DIGITALIZATION & DIGITAL TRANSFORMATION (78%); FUNDS & INVESTMENT TRUSTS (78%); DATA SECURITY (74%); INFORMATION SECURITY & PRIVACY (69%)
Geographic: RUSSIAN FEDERATION (90%)
Load-Date: October 27, 2021",neutral,0.7965646386146545,balanced/neutral,"['privacy', 'discrimination', 'security', 'autonomy']",['autonomy'],"['standards', 'framework']",[],4,1,2,0
2021,Unknown Title,"Byline: States News Service
Dateline: BOSTON, Mass. 
Body
The following information was released by Northeastern University:
Companies are increasingly expected to address issues such as justice and fairness with their artificial intelligence programs, but many don't know how, Northeastern professors found. Photo illustration by Matthew Modoono/Northeastern University
by Peter Ramjug September 23, 2021
Some of the world's biggest organizations, from the United Nations to Google to the U.S. Defense Department, proudly proclaim their bona fides when it comes to their ethical use of artificial intelligence.
But for many other organizations, talking the talk is the easy part. A new report by a pair of Northeastern researchers discusses how articulating values, ethical concepts, and principles is just the first step in addressing AI and data ethics challenges. The harder work is moving from vague, abstract promises to substantive commitments that are action-guiding and measurable.
""You see case after case where a company has these mission statements that they fail to live up to,"" says John Basl , an associate professor of philosophy and a co-author of the report. ""Their attempt to do ethics falls apart.""
Corporate pledges without proper execution amount to little more than platitudes and ""ethics washing,"" according to the report , published in conjunction with the Atlantic Council , a nonpartisan think tank in Washington. The findings will be discussed among the authors and other speakers at a virtual event on Sept. 23 at noon ET.
The report recommends greater transparency to help people understand how and why AI decisions are being made about them. ""If you deny me a loan and I can't figure out what caused that decision, then I don't know what to do for future loan applications,"" Basl explains. ""Transparency matters.""
The ethically problematic development and use of AI and big data will continue, and the industry will be seen by policy makers, employees, consumers, clients, and the public as failing to make good on its own stated commitments, the report adds.
Most companies are well-meaning and have done a good job of developing formal metrics for benchmarks such as fairness and bias, but they are not really able to pinpoint which of those is going to accomplish their aims, Basl says.
""One of the things this report is meant to do is force companies to reflect cohesively across their values instead of trying to pick and choose,"" he says. ""It doesn't tell them how to do any particular thing, but it provides them a process they have to go through if they want to sincerely realize their values.""
The deeper problem big businesses face, he adds, is in not having sufficient resources.
""When I want to say 'I'm unbiased and I'm being fair,' what does that actually mean? In different contexts that means different things,"" Basl adds.
He and co-researcher Ronald Sandler , director of Northeastern's Ethics Institute and head of the philosophy and religion department, note that as organizations are increasingly expected to address issues such as justice, fairness, privacy, autonomy, and accountability, they are often asked to do so without the guidance of regulations.
They suggest that a federal agency such as the Federal Communications Commission or the Consumer Financial Protection Bureau may step in to fill the void.
""Regulation is going to have to come,"" Basl predicts.
The National Institute of Standards and Technology , a non-regulatory agency within the Commerce Department, issued a proposal in June seeking comments on ways to identify and manage bias in AI.
""We want to engage the community in developing voluntary, consensus-based standards for managing AI bias and reducing the risk of harmful outcomes that it can cause,"" said NIST.
Basl points to software giant Microsoft as an example of a global company that appears to be doing ethical AI the right way.
""Microsoft, on paper, has a good approach,"" he says. ""They recognize the need to operationalize ethical principles, they have an ethics board to advise those efforts, and they have researchers dedicated to core ethical issues in AI. They seem to be putting in a good faith effort to try to get these things right.""
Microsoft is in the process of acquiring AI and healthcare speech technology firm Nuance Communications for $20 billion. ""AI is technology's most important priority, and healthcare is its most urgent application,"" Microsoft CEO Satya Nadella said when the deal was announced in April.
Even when companies take steps to bolster ethics with their technologies, it can sometimes backfire. Google pulled the plug on an internal AI ethics board in 2019 just one week after announcing it, following employee opposition to some of its members.
""Corporations are so big and have so many motivational threads that instilling this stuff in a clear, careful way that actually makes any progress is really hard,"" says Basl.
The ethics report is a follow-up to earlier research by Sandler and Basl that encourages companies to have properly resourced ethics committees to mitigate digital risks and maintain trust with the public. That 2019 report was driven by growing concerns over data use and security in light of the prevalence of AI technologies.
""The first report tells you how to build the oversight capacity to do ethics, and the second report outlines part of the job of that ethical capacity,"" Basl says.
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); COLLEGE & UNIVERSITY PROFESSORS (90%); RESEARCH INSTITUTES (90%); CONSUMERS (78%); PHILOSOPHY (78%); RESEARCH REPORTS (77%); ASSOCIATIONS & ORGANIZATIONS (76%); UNITED NATIONS (73%); WRITERS (72%); UNITED NATIONS INSTITUTIONS (71%); DEFENSE DEPARTMENTS (56%)
Company: GOOGLE LLC (57%)
Organization: NORTHEASTERN UNIVERSITY (94%); US DEPARTMENT OF DEFENSE (57%)
Industry: NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (57%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); COLLEGE & UNIVERSITY PROFESSORS (90%); WRITERS (72%); DEFENSE DEPARTMENTS (56%)
Geographic: BOSTON, MA, USA (79%); MASSACHUSETTS, USA (79%)
Load-Date: September 24, 2021","The following information was released by Northeastern University:
Companies are increasingly expected to address issues such as justice and fairness with their artificial intelligence programs, but many don't know how, Northeastern professors found. Photo illustration by Matthew Modoono/Northeastern University
by Peter Ramjug September 23, 2021
Some of the world's biggest organizations, from the United Nations to Google to the U.S. Defense Department, proudly proclaim their bona fides when it comes to their ethical use of artificial intelligence.
But for many other organizations, talking the talk is the easy part. A new report by a pair of Northeastern researchers discusses how articulating values, ethical concepts, and principles is just the first step in addressing AI and data ethics challenges. The harder work is moving from vague, abstract promises to substantive commitments that are action-guiding and measurable.
""You see case after case where a company has these mission statements that they fail to live up to,"" says John Basl , an associate professor of philosophy and a co-author of the report. ""Their attempt to do ethics falls apart.""
Corporate pledges without proper execution amount to little more than platitudes and ""ethics washing,"" according to the report , published in conjunction with the Atlantic Council , a nonpartisan think tank in Washington. The findings will be discussed among the authors and other speakers at a virtual event on Sept. 23 at noon ET.
The report recommends greater transparency to help people understand how and why AI decisions are being made about them. ""If you deny me a loan and I can't figure out what caused that decision, then I don't know what to do for future loan applications,"" Basl explains. ""Transparency matters.""
The ethically problematic development and use of AI and big data will continue, and the industry will be seen by policy makers, employees, consumers, clients, and the public as failing to make good on its own stated commitments, the report adds.
Most companies are well-meaning and have done a good job of developing formal metrics for benchmarks such as fairness and bias, but they are not really able to pinpoint which of those is going to accomplish their aims, Basl says.
""One of the things this report is meant to do is force companies to reflect cohesively across their values instead of trying to pick and choose,"" he says. ""It doesn't tell them how to do any particular thing, but it provides them a process they have to go through if they want to sincerely realize their values.""
The deeper problem big businesses face, he adds, is in not having sufficient resources.
""When I want to say 'I'm unbiased and I'm being fair,' what does that actually mean? In different contexts that means different things,"" Basl adds.
He and co-researcher Ronald Sandler , director of Northeastern's Ethics Institute and head of the philosophy and religion department, note that as organizations are increasingly expected to address issues such as justice, fairness, privacy, autonomy, and accountability, they are often asked to do so without the guidance of regulations.
They suggest that a federal agency such as the Federal Communications Commission or the Consumer Financial Protection Bureau may step in to fill the void.
""Regulation is going to have to come,"" Basl predicts.
The National Institute of Standards and Technology , a non-regulatory agency within the Commerce Department, issued a proposal in June seeking comments on ways to identify and manage bias in AI.
""We want to engage the community in developing voluntary, consensus-based standards for managing AI bias and reducing the risk of harmful outcomes that it can cause,"" said NIST.
Basl points to software giant Microsoft as an example of a global company that appears to be doing ethical AI the right way.
""Microsoft, on paper, has a good approach,"" he says. ""They recognize the need to operationalize ethical principles, they have an ethics board to advise those efforts, and they have researchers dedicated to core ethical issues in AI. They seem to be putting in a good faith effort to try to get these things right.""
Microsoft is in the process of acquiring AI and healthcare speech technology firm Nuance Communications for $20 billion. ""AI is technology's most important priority, and healthcare is its most urgent application,"" Microsoft CEO Satya Nadella said when the deal was announced in April.
Even when companies take steps to bolster ethics with their technologies, it can sometimes backfire. Google pulled the plug on an internal AI ethics board in 2019 just one week after announcing it, following employee opposition to some of its members.
""Corporations are so big and have so many motivational threads that instilling this stuff in a clear, careful way that actually makes any progress is really hard,"" says Basl.
The ethics report is a follow-up to earlier research by Sandler and Basl that encourages companies to have properly resourced ethics committees to mitigate digital risks and maintain trust with the public. That 2019 report was driven by growing concerns over data use and security in light of the prevalence of AI technologies.
""The first report tells you how to build the oversight capacity to do ethics, and the second report outlines part of the job of that ethical capacity,"" Basl says.
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); COLLEGE & UNIVERSITY PROFESSORS (90%); RESEARCH INSTITUTES (90%); CONSUMERS (78%); PHILOSOPHY (78%); RESEARCH REPORTS (77%); ASSOCIATIONS & ORGANIZATIONS (76%); UNITED NATIONS (73%); WRITERS (72%); UNITED NATIONS INSTITUTIONS (71%); DEFENSE DEPARTMENTS (56%)
Company: GOOGLE LLC (57%)
Organization: NORTHEASTERN UNIVERSITY (94%); US DEPARTMENT OF DEFENSE (57%)
Industry: NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (57%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); COLLEGE & UNIVERSITY PROFESSORS (90%); WRITERS (72%); DEFENSE DEPARTMENTS (56%)
Geographic: BOSTON, MA, USA (79%); MASSACHUSETTS, USA (79%)
Load-Date: September 24, 2021",neutral,0.8551474213600159,balanced/neutral,"['privacy', 'bias', 'fairness', 'transparency', 'accountability', 'security', 'autonomy', 'agency']","['justice', 'fairness', 'autonomy', 'justice']","['regulation', 'policy', 'oversight', 'standards', 'need to', 'suggest']",[],8,4,6,0
2021,Unknown Title,"Byline: Targeted News Service
Dateline: HELSINKI, Finland 
Body
The University of Helsinki issued the following news release:
The open and free online course Ethics of AI is designed to help us understand what it means to use AI ethically, and what it requires from society and individuals. The course uses examples of practical issues from its collaboration partners.
The Finnish and Swedish translations for the University of Helsinki's online course Ethics of AI will open on 23 November, in connection with a seminar on information policy at the Ministry of Finances. The course will help public administration, companies, and citizens understand what it means to use AI ethically and what that requires of both society and the individual. The course has been developed as a collaboration with the cities of Helsinki, Amsterdam, and London, as well as the Finnish Ministry of Finances.
Since AI is increasingly being used as a support for decisions concerning citizens, new kinds of questions emerge that the course is designed to sort out. What ethical viewpoints do the users and developers of different AI systems need to take into consideration? What are the ethical stumbling blocks when e.g. handling information on people's health? How is our information used? Who is responsible for decisions made by computers? How do we use face recognition ethically?
Lecturer Anna-Mari Rusanen, the person responsible for the contents of the course, wants to point out that the ethics of algorithms and intelligent technologies in general are still being formed, and the whole discussion on how to evaluate intelligent technologies socially is still ongoing.
New examples of situations requiring ethical evaluation appear every day. This is why it is vital to develop the skills to assess the principles for weighing the acceptability of the applications, says Rusanen.
Rusanen specialises in AI and cognition research. She studies the information processing of intelligent systems and the ethical and social consequences of AI development.
AI ethics is not just about the evaluation of the ethical acceptance of the technology; it has morphed into a question about politics, money and power. The more they become entangled into the objectives of AI development, the more we need a discourse on the goals of the development, Rusanen writes in the book Alykas huominen (the intelligent morrow) (Gaudeamus) that was published in autumn 2021.
Sample cases from real life
The online course consists of seven parts; the definition of AI ethics, the principles for benevolence and non-harm, responsibility, transparency, human rights, fairness, and AI ethics in practice. The sections include reading material and assignments.
The project partners have brought cases from real life to the course. The City of Helsinki, for example, has a case that focuses on the use of AI in social and health services, and in predicting risks to the health of its citizens. The case from the Ministry of Finances, for its part, considers the use of recommendation algorithms to offer improved public services.
The user does not need coding or special technological skills to take part of Ethics of AI. The university also has a free online course - Elements of AI - where you can learn the general principles of AI. In November, Elements of AI won the international German Design Award in the category Excellent Communications Design - Web.
The new MOOC centre at the University of Helsinki started operations at the beginning of 2021. The purpose of the centre is to bring continuity and method to the open online courses at the University of Helsinki. In Finland, the Ministry of Education and Culture has approached the threat of decline in competence with projects where the universities give complementary education in ICT through open online courses.
MSTRUCK-7676125 MSTRUCK
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ARTIFICIAL INTELLIGENCE ETHICS (92%); ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); DISTANCE LEARNING (90%); ALLIANCES & PARTNERSHIPS (77%); GOVERNMENT & PUBLIC ADMINISTRATION (70%); BEHAVIOR & COGNITION (69%)
Company:  AI SYSTEMS (55%)
Industry: SIC7372 PREPACKAGED SOFTWARE (55%); ARTIFICIAL INTELLIGENCE ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%)
Geographic: HELSINKI, FINLAND (95%); FINLAND (96%)
Load-Date: November 24, 2021","The University of Helsinki issued the following news release:
The open and free online course Ethics of AI is designed to help us understand what it means to use AI ethically, and what it requires from society and individuals. The course uses examples of practical issues from its collaboration partners.
The Finnish and Swedish translations for the University of Helsinki's online course Ethics of AI will open on 23 November, in connection with a seminar on information policy at the Ministry of Finances. The course will help public administration, companies, and citizens understand what it means to use AI ethically and what that requires of both society and the individual. The course has been developed as a collaboration with the cities of Helsinki, Amsterdam, and London, as well as the Finnish Ministry of Finances.
Since AI is increasingly being used as a support for decisions concerning citizens, new kinds of questions emerge that the course is designed to sort out. What ethical viewpoints do the users and developers of different AI systems need to take into consideration? What are the ethical stumbling blocks when e.g. handling information on people's health? How is our information used? Who is responsible for decisions made by computers? How do we use face recognition ethically?
Lecturer Anna-Mari Rusanen, the person responsible for the contents of the course, wants to point out that the ethics of algorithms and intelligent technologies in general are still being formed, and the whole discussion on how to evaluate intelligent technologies socially is still ongoing.
New examples of situations requiring ethical evaluation appear every day. This is why it is vital to develop the skills to assess the principles for weighing the acceptability of the applications, says Rusanen.
Rusanen specialises in AI and cognition research. She studies the information processing of intelligent systems and the ethical and social consequences of AI development.
AI ethics is not just about the evaluation of the ethical acceptance of the technology; it has morphed into a question about politics, money and power. The more they become entangled into the objectives of AI development, the more we need a discourse on the goals of the development, Rusanen writes in the book Alykas huominen (the intelligent morrow) (Gaudeamus) that was published in autumn 2021.
Sample cases from real life
The online course consists of seven parts; the definition of AI ethics, the principles for benevolence and non-harm, responsibility, transparency, human rights, fairness, and AI ethics in practice. The sections include reading material and assignments.
The project partners have brought cases from real life to the course. The City of Helsinki, for example, has a case that focuses on the use of AI in social and health services, and in predicting risks to the health of its citizens. The case from the Ministry of Finances, for its part, considers the use of recommendation algorithms to offer improved public services.
The user does not need coding or special technological skills to take part of Ethics of AI. The university also has a free online course - Elements of AI - where you can learn the general principles of AI. In November, Elements of AI won the international German Design Award in the category Excellent Communications Design - Web.
The new MOOC centre at the University of Helsinki started operations at the beginning of 2021. The purpose of the centre is to bring continuity and method to the open online courses at the University of Helsinki. In Finland, the Ministry of Education and Culture has approached the threat of decline in competence with projects where the universities give complementary education in ICT through open online courses.
MSTRUCK-7676125 MSTRUCK
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ARTIFICIAL INTELLIGENCE ETHICS (92%); ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); DISTANCE LEARNING (90%); ALLIANCES & PARTNERSHIPS (77%); GOVERNMENT & PUBLIC ADMINISTRATION (70%); BEHAVIOR & COGNITION (69%)
Company:  AI SYSTEMS (55%)
Industry: SIC7372 PREPACKAGED SOFTWARE (55%); ARTIFICIAL INTELLIGENCE ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%)
Geographic: HELSINKI, FINLAND (95%); FINLAND (96%)
Load-Date: November 24, 2021",neutral,0.856857180595398,balanced/neutral,"['fairness', 'transparency', 'human rights']",['fairness'],"['policy', 'need to']",[],3,1,2,0
2021,Unknown Title,"Body
The recent apartment building collapse in Miami, Florida, is a tragic reminder of the huge impacts engineering can have on our lives. Disasters such as this force engineers to reflect on their practice and perhaps fundamentally change their approach. Specifically, we should give much greater weight to ethics when training engineers. 
Engineers work in a vast range of fields that pose ethical concerns. These include artificial intelligence, data privacy, building construction, public health, and activity on shared environments (including Indigenous communities). The decisions engineers make, if not fully thought through, can have unintended consequences - including building failures and climate change. 
Engineers have ethical obligations (such as Engineers Australia's code of ethics) that they must follow. However, as identified at UNSW, the complexity of emerging social concerns creates a need for engineers' education to equip them with much deeper ethical skill sets. 
Engineering is seen as a trusted and ethical profession. In a 2019 Gallup poll, 66% rated the honesty and ethical standards of engineers as high/very high, on a par with medical doctors (65%). 
However, ethics as a body of knowledge is massive. There are nearly as many academic papers on ethics as mathematics, and clearly more than on artificial intelligence. 
Comparison of numbers of research papers by keyword (mathematics, ethics and AI).
With such a rich backdrop of knowledge, engineers must embrace ethics in a way that previous generations embraced mathematics. Complex societal problems make much greater demands on engineering thinking than in the past. We need to consider whole and complex systems, not just issues as individual challenges. 
Ethics and the construction industry
The construction industry provides a topical example of such complexity. Opal Tower in Sydney, Lacrosse building in Melbourne, Grenfell Tower in London and Torch Tower in Dubai became household names for all the wrong reasons. 
Importantly, these issues of poor quality and performance don't arise from new technology or know-how. They involve well-established technical domains of engineering: combustible cladding, fire safety, structural adequacy and so on. A fragmented design and delivery process with unclear responsibility and/or accountability has led to poor outcomes. 
These issues prompted the Australian Building Ministers' Forum to commission the Shergold Weir Report, followed by a task force to implement its recommendations across Australia. 
There are real shortcomings in the legal and contractual processes for allocating and ""commoditising"" risk in the industry. However, ethics should do the heavy lifting when legal frameworks are lacking. One key question is whether erosion of professional ethics has played a part in this state of affairs. The answer is a likely ""yes"". 
Engineers face ethical dilemmas such as: 
""Should I accept a narrow or inadequately framed design commission within a design and build delivery model when there is no certainty my design will be appropriately integrated with other parts of the project?""   
""How can I accept a commission when my client provides no budget for my oversight of the construction to ensure the technical integrity of my design is maintained when built?""   
""How do I play in a commercially competitive landscape with pressures to produce ""leaner"" designs to save cost without compromising safety and long-term performance of my design?""   
""Do I hide behind the contractual clauses (or minimum requirements of codes of practice) when I know the overall process is flawed and does not deliver quality and/or value for money for the end user?""
 Or worse: ""Do I resort to phoenixing to avoid any accountability?""The ethics of engineering involve much more than ensuring buildings don't collapse. Chad Davis/Flickr, CC BY-SAEngineering on Country The enduring connection of Aboriginal Australians to Country requires engineers to navigate ethical considerations in Indigenous communities. Engineers must reconcile the legal, technical and regulatory requirements of their projects with Indigenous cultural values and needs. They might not be properly equipped to navigate ethical scenarios when they encounter unfamiliar cultural connections, or regulations are insufficient.  Consider, for example, the sacred sites of the McArthur River Mine. Traditional owners have raised concerns that current mining activities do not adequately protect sacred and cultural heritage sites. Evidence given by community leaders provides insight into the intimate and diverse relationship that traditional owners have with the land.  In considering such evidence, engineers must be able to evaluate both physical site risks (such as acidification of mine tailings and contamination of water bodies) and cultural risks (such as failing to identify all locations of cultural value).  How might we tackle such complicated projects? By properly engaging with traditional communities and by having diverse teams with multiple worldviews and experiences, along with strong technical skills. The broad field of ethical knowledge provides the skill sets to attempt to reconcile the diverse considerations. Cornell Dam on the Croton River near Croton-on-Hudson, New York, was the tallest dam in the world when completed in 1906. The dam was built with beauty and the environment in mind, but protests and disputes still impacted its construction. Malinda Rathnayake/Flickr, CC BYWhat should the curriculum look like? Engineering students' ethical development requires a holistic approach. One assessment suggested: 
""[...] that institutions integrate ethics instruction throughout the formal curriculum, support use of varied approaches that foster high‐quality experiences, and leverage both influences of co‐curricular experiences and students' desires to engage in positive ethical behaviours.""
The curriculum should include: 
skills/expertise - the underlying intellectual basis for discerning what is ethical and what is not, which is much more than codes of conduct or a prescriptive, formulaic approach   
practice - practical know-how in terms of ethical solutions that engineers can apply   
mindset - having an individual and group culture of acting ethically. The engineers' problem-solving mindset must be supplemented by constant reflection on the decisions made and their ethical consequences.
 Ethics is not an ""add-on"" subject. It must permeate all aspects of tertiary education - teaching, research and professional behaviour.  While the arguments for acting now are strong, market realities will also drive the process. The upcoming generation will likely displace those who are slow or reluctant to adapt.  For instance, engineering firms are under pressure from their own staff on the issue of climate change. More than 1,900 Australian engineers and nearly 180 engineering organisations have signed a declaration committing them to evaluate all new projects against the need to mitigate climate change.  Future engineers must transcend any remaining single-solution mindsets from the past. They'll need to embrace a much more complex and socially minded ethics. And that begins with their university education.     This article is republished from The Conversation under a Creative Commons license. Read the original article.    READ MORE:Reforms give construction regulator power to reject projects, push for national adoption
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (96%); ARTIFICIAL INTELLIGENCE (90%); ENGINEERING (90%); PROFESSIONAL WORKERS (90%); SOCIETAL ISSUES (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (90%); TECHNICIANS & TECHNOLOGICAL WORKERS (90%); CONSTRUCTION SECTOR PERFORMANCE (89%); NEGATIVE NEWS (89%); 2017 GRENFELL TOWER FIRE (78%); ARTIFICIAL INTELLIGENCE ETHICS (78%); BUILDING COLLAPSES (78%); NEGATIVE SOCIETAL NEWS (78%); RESEARCH REPORTS (78%); SAFETY (78%); SAFETY, ACCIDENTS & DISASTERS (73%); PHYSICIANS & SURGEONS (72%); POLLS & SURVEYS (72%); PUBLIC HEALTH (72%); INDIGENOUS PEOPLES (71%); FIRE PREVENTION & SAFETY (69%)
Industry: ARTIFICIAL INTELLIGENCE (90%); CONSTRUCTION (90%); ENGINEERING (90%); CONSTRUCTION SECTOR PERFORMANCE (89%); ARTIFICIAL INTELLIGENCE ETHICS (78%); KNOWLEDGE MANAGEMENT (76%); PHYSICIANS & SURGEONS (72%); INFORMATION SECURITY & PRIVACY (56%)
Geographic: MELBOURNE, AUSTRALIA (71%); SYDNEY, AUSTRALIA (71%); MIAMI, FL, USA (58%); LONDON, ENGLAND (53%); DUBAI, UNITED ARAB EMIRATES (79%); FLORIDA, USA (79%); AUSTRALIA (91%); UNITED ARAB EMIRATES (73%)
Load-Date: July 21, 2021","The recent apartment building collapse in Miami, Florida, is a tragic reminder of the huge impacts engineering can have on our lives. Disasters such as this force engineers to reflect on their practice and perhaps fundamentally change their approach. Specifically, we should give much greater weight to ethics when training engineers. 
Engineers work in a vast range of fields that pose ethical concerns. These include artificial intelligence, data privacy, building construction, public health, and activity on shared environments (including Indigenous communities). The decisions engineers make, if not fully thought through, can have unintended consequences - including building failures and climate change. 
Engineers have ethical obligations (such as Engineers Australia's code of ethics) that they must follow. However, as identified at UNSW, the complexity of emerging social concerns creates a need for engineers' education to equip them with much deeper ethical skill sets. 
Engineering is seen as a trusted and ethical profession. In a 2019 Gallup poll, 66% rated the honesty and ethical standards of engineers as high/very high, on a par with medical doctors (65%). 
However, ethics as a body of knowledge is massive. There are nearly as many academic papers on ethics as mathematics, and clearly more than on artificial intelligence. 
Comparison of numbers of research papers by keyword (mathematics, ethics and AI).
With such a rich backdrop of knowledge, engineers must embrace ethics in a way that previous generations embraced mathematics. Complex societal problems make much greater demands on engineering thinking than in the past. We need to consider whole and complex systems, not just issues as individual challenges. 
Ethics and the construction industry
The construction industry provides a topical example of such complexity. Opal Tower in Sydney, Lacrosse building in Melbourne, Grenfell Tower in London and Torch Tower in Dubai became household names for all the wrong reasons. 
Importantly, these issues of poor quality and performance don't arise from new technology or know-how. They involve well-established technical domains of engineering: combustible cladding, fire safety, structural adequacy and so on. A fragmented design and delivery process with unclear responsibility and/or accountability has led to poor outcomes. 
These issues prompted the Australian Building Ministers' Forum to commission the Shergold Weir Report, followed by a task force to implement its recommendations across Australia. 
There are real shortcomings in the legal and contractual processes for allocating and ""commoditising"" risk in the industry. However, ethics should do the heavy lifting when legal frameworks are lacking. One key question is whether erosion of professional ethics has played a part in this state of affairs. The answer is a likely ""yes"". 
Engineers face ethical dilemmas such as: 
""Should I accept a narrow or inadequately framed design commission within a design and build delivery model when there is no certainty my design will be appropriately integrated with other parts of the project?""   
""How can I accept a commission when my client provides no budget for my oversight of the construction to ensure the technical integrity of my design is maintained when built?""   
""How do I play in a commercially competitive landscape with pressures to produce ""leaner"" designs to save cost without compromising safety and long-term performance of my design?""   
""Do I hide behind the contractual clauses (or minimum requirements of codes of practice) when I know the overall process is flawed and does not deliver quality and/or value for money for the end user?""
 Or worse: ""Do I resort to phoenixing to avoid any accountability?""The ethics of engineering involve much more than ensuring buildings don't collapse. Chad Davis/Flickr, CC BY-SAEngineering on Country The enduring connection of Aboriginal Australians to Country requires engineers to navigate ethical considerations in Indigenous communities. Engineers must reconcile the legal, technical and regulatory requirements of their projects with Indigenous cultural values and needs. They might not be properly equipped to navigate ethical scenarios when they encounter unfamiliar cultural connections, or regulations are insufficient.  Consider, for example, the sacred sites of the McArthur River Mine. Traditional owners have raised concerns that current mining activities do not adequately protect sacred and cultural heritage sites. Evidence given by community leaders provides insight into the intimate and diverse relationship that traditional owners have with the land.  In considering such evidence, engineers must be able to evaluate both physical site risks (such as acidification of mine tailings and contamination of water bodies) and cultural risks (such as failing to identify all locations of cultural value).  How might we tackle such complicated projects? By properly engaging with traditional communities and by having diverse teams with multiple worldviews and experiences, along with strong technical skills. The broad field of ethical knowledge provides the skill sets to attempt to reconcile the diverse considerations. Cornell Dam on the Croton River near Croton-on-Hudson, New York, was the tallest dam in the world when completed in 1906. The dam was built with beauty and the environment in mind, but protests and disputes still impacted its construction. Malinda Rathnayake/Flickr, CC BYWhat should the curriculum look like? Engineering students' ethical development requires a holistic approach. One assessment suggested: 
""[...] that institutions integrate ethics instruction throughout the formal curriculum, support use of varied approaches that foster high‐quality experiences, and leverage both influences of co‐curricular experiences and students' desires to engage in positive ethical behaviours.""
The curriculum should include: 
skills/expertise - the underlying intellectual basis for discerning what is ethical and what is not, which is much more than codes of conduct or a prescriptive, formulaic approach   
practice - practical know-how in terms of ethical solutions that engineers can apply   
mindset - having an individual and group culture of acting ethically. The engineers' problem-solving mindset must be supplemented by constant reflection on the decisions made and their ethical consequences.
 Ethics is not an ""add-on"" subject. It must permeate all aspects of tertiary education - teaching, research and professional behaviour.  While the arguments for acting now are strong, market realities will also drive the process. The upcoming generation will likely displace those who are slow or reluctant to adapt.  For instance, engineering firms are under pressure from their own staff on the issue of climate change. More than 1,900 Australian engineers and nearly 180 engineering organisations have signed a declaration committing them to evaluate all new projects against the need to mitigate climate change.  Future engineers must transcend any remaining single-solution mindsets from the past. They'll need to embrace a much more complex and socially minded ethics. And that begins with their university education.     This article is republished from The Conversation under a Creative Commons license. Read the original article.    READ MORE:Reforms give construction regulator power to reject projects, push for national adoption
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (96%); ARTIFICIAL INTELLIGENCE (90%); ENGINEERING (90%); PROFESSIONAL WORKERS (90%); SOCIETAL ISSUES (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (90%); TECHNICIANS & TECHNOLOGICAL WORKERS (90%); CONSTRUCTION SECTOR PERFORMANCE (89%); NEGATIVE NEWS (89%); 2017 GRENFELL TOWER FIRE (78%); ARTIFICIAL INTELLIGENCE ETHICS (78%); BUILDING COLLAPSES (78%); NEGATIVE SOCIETAL NEWS (78%); RESEARCH REPORTS (78%); SAFETY (78%); SAFETY, ACCIDENTS & DISASTERS (73%); PHYSICIANS & SURGEONS (72%); POLLS & SURVEYS (72%); PUBLIC HEALTH (72%); INDIGENOUS PEOPLES (71%); FIRE PREVENTION & SAFETY (69%)
Industry: ARTIFICIAL INTELLIGENCE (90%); CONSTRUCTION (90%); ENGINEERING (90%); CONSTRUCTION SECTOR PERFORMANCE (89%); ARTIFICIAL INTELLIGENCE ETHICS (78%); KNOWLEDGE MANAGEMENT (76%); PHYSICIANS & SURGEONS (72%); INFORMATION SECURITY & PRIVACY (56%)
Geographic: MELBOURNE, AUSTRALIA (71%); SYDNEY, AUSTRALIA (71%); MIAMI, FL, USA (58%); LONDON, ENGLAND (53%); DUBAI, UNITED ARAB EMIRATES (79%); FLORIDA, USA (79%); AUSTRALIA (91%); UNITED ARAB EMIRATES (73%)
Load-Date: July 21, 2021",negative,0.6865031719207764,balanced/neutral,"['privacy', 'accountability', 'safety', 'security']",[],"['oversight', 'standards', 'should', 'must', 'need to']",[],4,0,5,0
2021,Unknown Title,"Byline: States News Service
Dateline: LOS ANGELES, Calif. 
Body
The following information was released by the USC Marshall School of Business:
USC Neely Center's fifth annual symposium takes a cross-disciplinary look at the ethical implications of artificial intelligence
October 26, 2021
Share this page:
Great strides are being made in artificial intelligence across many disciplines, from self-driving cars to smart phones to virtual assistants to robots. AI makes headlines. But should we be worried?
The Neely Center for Ethical Leadership and Decision Making, in its fifth annual symposium, will take a deeper dive into that question, with scholars from the fields of business, ethics, public policy and technology, Thursday, Nov. 4, 2021 from 8:30 a.m.- 1:30 p.m. The event is virtual.
The Neely Center is a collaboration of the USC Marshall School of Business, the USC Viterbi School of Engineering, and the USC Sol Price School of Public Policy. Now in its sixth year, the Center aims to foster discussions of applied ethics in commerce, public policy, and engineering among the university community and its surrounding neighborhoods, including students, staff, faculty, and community members.
Endowed by Jerry Neely, a University Life Trustee and 1958 business school graduate and his wife, Nancy Neely, the Center seeks to draw on the experience and wisdom of leaders in every field to focus attention on the great need for ethical consciousness among company leaders and professionals of all stripes.
This year's symposium is titled, ""The Ethical Implications of Artificial Intelligence,"" and will explore just some of the many questions surrounding the topic. What is the appropriate application of AI in the professions, and what are the attending ethical considerations? What promise does it hold for elevating ethical consciousness where it is applied? On the other hand, must its presence be safeguarded with a special vigilance? Can technology ever approach the consciousness of humans? If it does, should the status of personhoodwith both its rights and responsibilitiesbe considered for such technology?
With scholars from Marshall, Viterbi, Price, and Dornsife, expect a vibrant discussion of these and other questions. Also speaking is Marian Croak, a Viterbi alumna and vice president of engineering at Google, who is credited as a developer of voice-over IP. She was recently one of two black scientists inducted into the National Inventor's Hall of Fame this year.
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); BUSINESS EDUCATION (90%); BUSINESS ETHICS (90%); CONFERENCES & CONVENTIONS (90%); PUBLIC POLICY (89%); BUSINESS ETHICS & CORPORATE CITIZENSHIP (78%); STUDENTS & STUDENT LIFE (78%); PROFESSIONAL WORKERS (73%); COLLEGES & UNIVERSITIES (72%); GRADUATE & PROFESSIONAL SCHOOLS (72%); VIRTUAL EVENTS (69%)
Company: GOOGLE LLC (56%)
Industry: NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (56%); ARTIFICIAL INTELLIGENCE (90%); COLLEGES & UNIVERSITIES (72%); GRADUATE & PROFESSIONAL SCHOOLS (72%); AUTONOMOUS MOTOR VEHICLES (57%); MOBILE & CELLULAR TELEPHONES (56%)
Geographic: LOS ANGELES, CA, USA (79%); CALIFORNIA, USA (79%)
Load-Date: October 26, 2021","The following information was released by the USC Marshall School of Business:
USC Neely Center's fifth annual symposium takes a cross-disciplinary look at the ethical implications of artificial intelligence
October 26, 2021
Share this page:
Great strides are being made in artificial intelligence across many disciplines, from self-driving cars to smart phones to virtual assistants to robots. AI makes headlines. But should we be worried?
The Neely Center for Ethical Leadership and Decision Making, in its fifth annual symposium, will take a deeper dive into that question, with scholars from the fields of business, ethics, public policy and technology, Thursday, Nov. 4, 2021 from 8:30 a.m.- 1:30 p.m. The event is virtual.
The Neely Center is a collaboration of the USC Marshall School of Business, the USC Viterbi School of Engineering, and the USC Sol Price School of Public Policy. Now in its sixth year, the Center aims to foster discussions of applied ethics in commerce, public policy, and engineering among the university community and its surrounding neighborhoods, including students, staff, faculty, and community members.
Endowed by Jerry Neely, a University Life Trustee and 1958 business school graduate and his wife, Nancy Neely, the Center seeks to draw on the experience and wisdom of leaders in every field to focus attention on the great need for ethical consciousness among company leaders and professionals of all stripes.
This year's symposium is titled, ""The Ethical Implications of Artificial Intelligence,"" and will explore just some of the many questions surrounding the topic. What is the appropriate application of AI in the professions, and what are the attending ethical considerations? What promise does it hold for elevating ethical consciousness where it is applied? On the other hand, must its presence be safeguarded with a special vigilance? Can technology ever approach the consciousness of humans? If it does, should the status of personhoodwith both its rights and responsibilitiesbe considered for such technology?
With scholars from Marshall, Viterbi, Price, and Dornsife, expect a vibrant discussion of these and other questions. Also speaking is Marian Croak, a Viterbi alumna and vice president of engineering at Google, who is credited as a developer of voice-over IP. She was recently one of two black scientists inducted into the National Inventor's Hall of Fame this year.
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); BUSINESS EDUCATION (90%); BUSINESS ETHICS (90%); CONFERENCES & CONVENTIONS (90%); PUBLIC POLICY (89%); BUSINESS ETHICS & CORPORATE CITIZENSHIP (78%); STUDENTS & STUDENT LIFE (78%); PROFESSIONAL WORKERS (73%); COLLEGES & UNIVERSITIES (72%); GRADUATE & PROFESSIONAL SCHOOLS (72%); VIRTUAL EVENTS (69%)
Company: GOOGLE LLC (56%)
Industry: NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (56%); ARTIFICIAL INTELLIGENCE (90%); COLLEGES & UNIVERSITIES (72%); GRADUATE & PROFESSIONAL SCHOOLS (72%); AUTONOMOUS MOTOR VEHICLES (57%); MOBILE & CELLULAR TELEPHONES (56%)
Geographic: LOS ANGELES, CA, USA (79%); CALIFORNIA, USA (79%)
Load-Date: October 26, 2021",neutral,0.740863561630249,balanced/neutral,[],[],"['policy', 'should', 'must']",[],0,0,3,0
2021,Unknown Title,"Body
HOBOKEN, N.J : NICE unveiled a Robo Ethical Framework promoting responsibility and transparency in the design, creation and deployment of AI-powered robots. NICE’s ethical guidelines set the standard for designing, building and deploying robots, and form the basis for solid and ethically sound robot and human collaboration. Comprising a set of five guiding principles, NICE's Robo-Ethical Framework underlies every interaction with process robots – from planning to implementation - and drives ethically sound human-robot partnerships in the workplace. The launch of this framework reiterates the company's dedication to these standards and invites industry wide adoption.
Sarah Burnett, partner at Emergence Partners said, “NICE RPA scored well in our Ethics in Technology Assessment (ETA) framework. It is commendable to see NICE taking a strong stand by establishing a Robo-Ethical framework. The upsurge in adoption of AI necessitates commitment to doing what is ethical, and respectful to customers. NICE’s move in this direction is admirable and I advise other organizations to follow suit. ”
By introducing the industry's first set of standards to self-govern the creation of responsible AI-driven robotics, NICE commits to ensuring transparent design, development and implementation of process automations as is already inherent to its RPA platform. Deeply rooted in its product capabilities, NICE’s ethical framework is shared with every customer along with their robotic license. While the ultimate determination of what is beneficial to humanity is subjective and contextually rooted, NICE aims to keep the importance of ensuring a positive impact in RPA top of mind in the industry.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: IFN
Subject: ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); BUSINESS NEWS (90%); ROBOTICS (90%)
Industry: ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ROBOTICS (90%)
Geographic: NEW JERSEY, USA (59%)
Load-Date: July 1, 2021","HOBOKEN, N.J : NICE unveiled a Robo Ethical Framework promoting responsibility and transparency in the design, creation and deployment of AI-powered robots. NICE’s ethical guidelines set the standard for designing, building and deploying robots, and form the basis for solid and ethically sound robot and human collaboration. Comprising a set of five guiding principles, NICE's Robo-Ethical Framework underlies every interaction with process robots – from planning to implementation - and drives ethically sound human-robot partnerships in the workplace. The launch of this framework reiterates the company's dedication to these standards and invites industry wide adoption.
Sarah Burnett, partner at Emergence Partners said, “NICE RPA scored well in our Ethics in Technology Assessment (ETA) framework. It is commendable to see NICE taking a strong stand by establishing a Robo-Ethical framework. The upsurge in adoption of AI necessitates commitment to doing what is ethical, and respectful to customers. NICE’s move in this direction is admirable and I advise other organizations to follow suit. ”
By introducing the industry's first set of standards to self-govern the creation of responsible AI-driven robotics, NICE commits to ensuring transparent design, development and implementation of process automations as is already inherent to its RPA platform. Deeply rooted in its product capabilities, NICE’s ethical framework is shared with every customer along with their robotic license. While the ultimate determination of what is beneficial to humanity is subjective and contextually rooted, NICE aims to keep the importance of ensuring a positive impact in RPA top of mind in the industry.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: IFN
Subject: ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); BUSINESS NEWS (90%); ROBOTICS (90%)
Industry: ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ROBOTICS (90%)
Geographic: NEW JERSEY, USA (59%)
Load-Date: July 1, 2021",positive,0.5529493093490601,balanced/neutral,['transparency'],[],"['standards', 'guidelines', 'framework']","['robot', 'robotics']",1,0,3,2
2021,Unknown Title,"Dateline: LOS ANGELES 
Body
Scientific discoveries deepen our understanding of nature and ourselves, with the potential to transform our everyday lives, yet can raise ethical concerns or risks for society.
This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20211209005349/en/
Brooke Smith, Director of Public Engagement, The Kavli Foundation (Photo: Business Wire)
Cutting-edge neuroscience, genetics, and artificial intelligence are a few examples that are driving the need to discuss: Who bears responsibility for broad ethical considerations of scientific discoveries? When is it optimal to consider implications and risks? How can the public be empowered to participate in these discussions?
Two Kavli Centers for Ethics, Science, and the Public - at the University of California, Berkeley, and the University of Cambridge - are launching to engage the public in identifying and exploring ethical considerations and impacts born from scientific discovery.
The Kavli Foundation's vision for the centers is a paradigm shift to meet an as-yet unmet need within science: a proactive and sustained effort that is intentional in connecting the public, scientists, ethicists, social scientists, and science communicators early in the process of scientific discoveries to identify and discuss potential impacts on society.
""We're embarking on a democratization of the way we think, collaborate, and communicate about scientific discoveries and their ethical aspects - and ensuring the public is included,"" said The Kavli Foundation President Cynthia Friend . ""It's long past due for this to happen.""
Until now, there hasn't been a sustained and proactive venture to address ethical implications born from scientific discovery that involves the public early and intentionally in the scientific process. And while there is increasing recognition within the scientific community that the public should be involved, mechanisms and infrastructure to do this are lacking. The public is too often left out of these important discussions, or they are brought in too late.
""With the Kavli Centers for Ethics, Science, and the Public, we are taking necessary action to create the infrastructure that enables early and intentional public engagement in the ethical considerations born from scientific discoveries,"" remarked The Kavli Foundation Director of Public Engagement Brooke Smith .
Two centers were selected for this new venture based on their vision, approach, and experience. While both are multi-faceted and complementary in their approaches working across disciplines in the sciences and humanities, each will have an initial focus that is unique.
The Kavli Center for Ethics, Science, and the Public at UC Berkeley will reimagine how scientists are trained, beginning in the fields of neuroscience, genetics, and artificial intelligence. Leading the center is AI expert Stuart Russell , along with Nobel-Prize Laureate Saul Perlmutter , who provided some of the first evidence that the expansion of the universe is accelerating; Nobel and Kavli Prize Laureate Jennifer Doudna , known for her discovery of the gene-editing tool CRISPR; theoretical and moral philosopher Jay Wallace ; bioethicist Jodi Halpern ; neuroscientist Jack Gallant ; and historian and writer Elena Conis .
""The impetus from The Kavli Foundation has helped to mobilize Berkeley's unparalleled resources in the humanities, social sciences, natural sciences, and engineering to collaborate on addressing one of humanity's most pressing problems: how to ensure that our rapidly advancing scientific and technological capabilities are directed towards the interests of humanity,"" said Stuart Russell, who serves as the inaugural Director of the Kavli Center for Ethics, Science, and the Public at UC Berkeley.
In a unique collaboration with Wellcome Connecting Science, the Kavli Center for Ethics, Science, and the Public at the University of Cambridge will be led by internationally recognized social scientist and genetic counsellor Anna Middleton ; supported by sociologist and bioethicist Richard Milne ; and journalist and broadcaster Catherine Galloway ; with creative industry expertise from broadcaster Vivienne Parry , OBE; sociology of education expertise from Susan Robertson ; and genomics and public engagement expertise from Julian Rayner . Drawing on a network of experts in ethics and public engagement from the UK, China, Russia, India, and Japan, the new center will explore how ethical implications raised by science are tackled in different cultural contexts within the domains of genomics, big data, health research, and emerging technologies.
""From the discovery of DNA's structure to sequencing 20% of the world's COVID virus and the development of the first artificial intelligence, Cambridge has been at the cutting edge of science for centuries,"" remarked Anna Middleton, director for the Kavli Center for Ethics, Science, and the Public at the University of Cambridge. ""Through collaboration with experts in popular culture we will find the evidence base to communicate complex ideas around the ethical issues raised by science so that all of us can share in decision making around the implications of science for society.""
The idea for the Centers was sparked by The Kavli Foundation's work and observations in science and society, including research at the 20 Kavli Institutes globally, where inspiring and transformative science is being done-ranging from decoding brain activity to fabricating artificial cells.
""This is a long-overdue beginning of an important journey for the scientific community, and we look forward to the impact the Kavli Centers for Ethics, Science, and the Public will have on the future role of science within society,"" said Friend.
The Kavli Foundation is dedicated to advancing science for the benefit of humanity. The foundation's mission is implemented through Kavli research institutes globally and programs that support basic science in the fields of astrophysics, nanoscience, neuroscience, and theoretical physics; initiatives that strengthen the relationship between science and society; and prizes and awards including the international Kavli Prizes and the AAAS Kavli Science Journalism Awards. Learn more at kavlifoundation.org and follow @kavlifoundation.
View source version on businesswire.com: https://www.businesswire.com/news/home/20211209005349/en/
CONTACT: Stacey Bailey
sbailey@kavlifoundation.org
http://www.businesswire.com
Graphic
Brooke Smith, Director of Public Engagement, The Kavli Foundation (Photo: Business Wire)
Cynthia M. Friend, President, The Kavli Foundation (Photo: Business Wire)
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ETHICS (97%); SCIENCE & TECHNOLOGY (95%); PRESS RELEASES (91%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (90%); AWARDS & PRIZES (85%); HUMANITIES & SOCIAL SCIENCE (79%); NEUROSCIENCE (79%); GENES & CHROMOSOMES (78%); NOBEL PRIZES (78%); ARTIFICIAL INTELLIGENCE (76%); Photo/Multimedia (%); Product/Service (%)
Company: CA-KAVLI-FOUNDATION
Organization: UNIVERSITY OF CALIFORNIA BERKELEY (56%)
Industry: ARTIFICIAL INTELLIGENCE (76%); White House/Federal Government (%); Other Education (%); Technology (%); University (%); Education (%); General Health (%); Philanthropy (%); Public Policy/Government (%); Genetics (%); Foundation (%); Other Technology (%); Science (%); Other Science (%); Health (%); Research (%)
Geographic: SAN FRANCISCO BAY AREA, CA, USA (93%); LOS ANGELES, CA, USA (79%); CALIFORNIA, USA (90%); EUROPE (79%); NORTH AMERICA (79%); SOUTH AMERICA (79%); UNITED KINGDOM (79%); UNITED STATES (79%); California; United States; United Kingdom; Africa; South America; North America; Asia Pacific; Europe
Load-Date: December 9, 2021","Scientific discoveries deepen our understanding of nature and ourselves, with the potential to transform our everyday lives, yet can raise ethical concerns or risks for society.
This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20211209005349/en/
Brooke Smith, Director of Public Engagement, The Kavli Foundation (Photo: Business Wire)
Cutting-edge neuroscience, genetics, and artificial intelligence are a few examples that are driving the need to discuss: Who bears responsibility for broad ethical considerations of scientific discoveries? When is it optimal to consider implications and risks? How can the public be empowered to participate in these discussions?
Two Kavli Centers for Ethics, Science, and the Public - at the University of California, Berkeley, and the University of Cambridge - are launching to engage the public in identifying and exploring ethical considerations and impacts born from scientific discovery.
The Kavli Foundation's vision for the centers is a paradigm shift to meet an as-yet unmet need within science: a proactive and sustained effort that is intentional in connecting the public, scientists, ethicists, social scientists, and science communicators early in the process of scientific discoveries to identify and discuss potential impacts on society.
""We're embarking on a democratization of the way we think, collaborate, and communicate about scientific discoveries and their ethical aspects - and ensuring the public is included,"" said The Kavli Foundation President Cynthia Friend . ""It's long past due for this to happen.""
Until now, there hasn't been a sustained and proactive venture to address ethical implications born from scientific discovery that involves the public early and intentionally in the scientific process. And while there is increasing recognition within the scientific community that the public should be involved, mechanisms and infrastructure to do this are lacking. The public is too often left out of these important discussions, or they are brought in too late.
""With the Kavli Centers for Ethics, Science, and the Public, we are taking necessary action to create the infrastructure that enables early and intentional public engagement in the ethical considerations born from scientific discoveries,"" remarked The Kavli Foundation Director of Public Engagement Brooke Smith .
Two centers were selected for this new venture based on their vision, approach, and experience. While both are multi-faceted and complementary in their approaches working across disciplines in the sciences and humanities, each will have an initial focus that is unique.
The Kavli Center for Ethics, Science, and the Public at UC Berkeley will reimagine how scientists are trained, beginning in the fields of neuroscience, genetics, and artificial intelligence. Leading the center is AI expert Stuart Russell , along with Nobel-Prize Laureate Saul Perlmutter , who provided some of the first evidence that the expansion of the universe is accelerating; Nobel and Kavli Prize Laureate Jennifer Doudna , known for her discovery of the gene-editing tool CRISPR; theoretical and moral philosopher Jay Wallace ; bioethicist Jodi Halpern ; neuroscientist Jack Gallant ; and historian and writer Elena Conis .
""The impetus from The Kavli Foundation has helped to mobilize Berkeley's unparalleled resources in the humanities, social sciences, natural sciences, and engineering to collaborate on addressing one of humanity's most pressing problems: how to ensure that our rapidly advancing scientific and technological capabilities are directed towards the interests of humanity,"" said Stuart Russell, who serves as the inaugural Director of the Kavli Center for Ethics, Science, and the Public at UC Berkeley.
In a unique collaboration with Wellcome Connecting Science, the Kavli Center for Ethics, Science, and the Public at the University of Cambridge will be led by internationally recognized social scientist and genetic counsellor Anna Middleton ; supported by sociologist and bioethicist Richard Milne ; and journalist and broadcaster Catherine Galloway ; with creative industry expertise from broadcaster Vivienne Parry , OBE; sociology of education expertise from Susan Robertson ; and genomics and public engagement expertise from Julian Rayner . Drawing on a network of experts in ethics and public engagement from the UK, China, Russia, India, and Japan, the new center will explore how ethical implications raised by science are tackled in different cultural contexts within the domains of genomics, big data, health research, and emerging technologies.
""From the discovery of DNA's structure to sequencing 20% of the world's COVID virus and the development of the first artificial intelligence, Cambridge has been at the cutting edge of science for centuries,"" remarked Anna Middleton, director for the Kavli Center for Ethics, Science, and the Public at the University of Cambridge. ""Through collaboration with experts in popular culture we will find the evidence base to communicate complex ideas around the ethical issues raised by science so that all of us can share in decision making around the implications of science for society.""
The idea for the Centers was sparked by The Kavli Foundation's work and observations in science and society, including research at the 20 Kavli Institutes globally, where inspiring and transformative science is being done-ranging from decoding brain activity to fabricating artificial cells.
""This is a long-overdue beginning of an important journey for the scientific community, and we look forward to the impact the Kavli Centers for Ethics, Science, and the Public will have on the future role of science within society,"" said Friend.
The Kavli Foundation is dedicated to advancing science for the benefit of humanity. The foundation's mission is implemented through Kavli research institutes globally and programs that support basic science in the fields of astrophysics, nanoscience, neuroscience, and theoretical physics; initiatives that strengthen the relationship between science and society; and prizes and awards including the international Kavli Prizes and the AAAS Kavli Science Journalism Awards. Learn more at kavlifoundation.org and follow @kavlifoundation.
View source version on businesswire.com: https://www.businesswire.com/news/home/20211209005349/en/
CONTACT: Stacey Bailey
sbailey@kavlifoundation.org
http://www.businesswire.com
Graphic
Brooke Smith, Director of Public Engagement, The Kavli Foundation (Photo: Business Wire)
Cynthia M. Friend, President, The Kavli Foundation (Photo: Business Wire)
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ETHICS (97%); SCIENCE & TECHNOLOGY (95%); PRESS RELEASES (91%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (90%); AWARDS & PRIZES (85%); HUMANITIES & SOCIAL SCIENCE (79%); NEUROSCIENCE (79%); GENES & CHROMOSOMES (78%); NOBEL PRIZES (78%); ARTIFICIAL INTELLIGENCE (76%); Photo/Multimedia (%); Product/Service (%)
Company: CA-KAVLI-FOUNDATION
Organization: UNIVERSITY OF CALIFORNIA BERKELEY (56%)
Industry: ARTIFICIAL INTELLIGENCE (76%); White House/Federal Government (%); Other Education (%); Technology (%); University (%); Education (%); General Health (%); Philanthropy (%); Public Policy/Government (%); Genetics (%); Foundation (%); Other Technology (%); Science (%); Other Science (%); Health (%); Research (%)
Geographic: SAN FRANCISCO BAY AREA, CA, USA (93%); LOS ANGELES, CA, USA (79%); CALIFORNIA, USA (90%); EUROPE (79%); NORTH AMERICA (79%); SOUTH AMERICA (79%); UNITED KINGDOM (79%); UNITED STATES (79%); California; United States; United Kingdom; Africa; South America; North America; Asia Pacific; Europe
Load-Date: December 9, 2021",neutral,0.6434764862060547,balanced/neutral,[],[],"['policy', 'should', 'need to']",[],0,0,3,0
2021,Unknown Title,"Body
The AI Alliance and several other organizations have signed a code of ethics of artificial intelligence (AI). The signing took place at TASS as part of the first international forum ""Ethics of Artificial Intelligence: The Beginning of Trust"", which takes place on October 26 in Moscow.
The Code will become part of the Artificial Intelligence federal project and the Strategy for the Development of the Information Society for 2017-2030.
It establishes general ethical principles and standards of conduct to guide those involved in activities using artificial intelligence.
The Code applies to relations involving ethical aspects of the creation (design, construction, piloting), implementation and use of AI technologies at all stages of the life cycle, which are currently not regulated by Russian law or other regulatory acts.
The document is divided into two sections, which include topics such as stimulating the development of AI, raising awareness of the ethics of using AI, identifying AI in communication with a person, and information security.
According to the deputy head of the Analytical Center under the Russian government, Sergey Nakvasin, currently, about 20 countries are dealing with the ethics of artificial intelligence.
The Russian code was authored by the AI Alliance jointly with the Analytical Center under the Russian government and the Economic Development Ministry. Joining the code is voluntary.
The document was signed by the alliance members Sberbank, Gazprom Neft, Yandex VK, MTS and the Russian Direct Investment Fund, as well as representatives of Skolkovo, Rostelecom, Rosatom, InfoWatch and real estate platform Cian.
For the first time, when signing agreements, a Gosklyuch mobile signature is used. Gosklyuch is a service, which generates a user's electronic signature free of charge. The signature is stored within a protected infrastructure of e-government.
In December 2020, Russian President Vladimir Putin called for the development of an internal moral and ethical code in the field of artificial intelligence. The head of state stressed it was important to ensure the participation of the professional community and business in the formation of ethical principles for the use of AI.
Source: Russian News Agency
Classification
Language: ENGLISH
Publication-Type: Wire
Subject: ETHICS (97%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); ASSOCIATIONS & ORGANIZATIONS (89%); AGREEMENTS (78%); BUSINESS NEWS (76%); ECONOMIC DEVELOPMENT (76%); ELECTRONIC GOVERNMENT (76%); GOVERNMENT & PUBLIC ADMINISTRATION (74%); FUNDS & INVESTMENT TRUSTS (72%); HEADS OF STATE & GOVERNMENT (72%)
Company:  YANDEX NV (90%);  OAO GAZPROM NEFT (65%);  ROSTELECOM OAO (64%);  SBERBANK ROSSII OAO (53%)
Ticker: YNDX (NASDAQ) (90%); SIBN (RTS) (65%); GAZ (LSE) (65%); RTKM (RTS) (64%); RKMD (LSE) (64%); SBER (RTS) (53%); SBER (LSE) (53%)
Industry: NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (90%); NAICS211130 NATURAL GAS EXTRACTION (65%); NAICS211120 CRUDE PETROLEUM EXTRACTION (65%); SIC1311 CRUDE PETROLEUM & NATURAL GAS (65%); NAICS238210 ELECTRICAL CONTRACTORS & OTHER WIRING INSTALLATION CONTRACTORS (64%); SIC1731 ELECTRICAL WORK (64%); NAICS522293 INTERNATIONAL TRADE FINANCING (53%); NAICS522110 COMMERCIAL BANKING (53%); SIC6082 FOREIGN TRADE & INTERNATIONAL BANKING INSTITUTIONS (53%); SIC6021 NATIONAL COMMERCIAL BANKS (53%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); NEWS SYNDICATION (78%); PRESS AGENCY RELEASES (78%); INFORMATION SECURITY & PRIVACY (76%); DIGITAL SIGNATURES (73%); FUNDS & INVESTMENT TRUSTS (72%)
Person: VLADIMIR PUTIN (57%)
Geographic: MOSCOW, RUSSIAN FEDERATION (58%); RUSSIAN FEDERATION (95%)
Load-Date: October 27, 2021","The AI Alliance and several other organizations have signed a code of ethics of artificial intelligence (AI). The signing took place at TASS as part of the first international forum ""Ethics of Artificial Intelligence: The Beginning of Trust"", which takes place on October 26 in Moscow.
The Code will become part of the Artificial Intelligence federal project and the Strategy for the Development of the Information Society for 2017-2030.
It establishes general ethical principles and standards of conduct to guide those involved in activities using artificial intelligence.
The Code applies to relations involving ethical aspects of the creation (design, construction, piloting), implementation and use of AI technologies at all stages of the life cycle, which are currently not regulated by Russian law or other regulatory acts.
The document is divided into two sections, which include topics such as stimulating the development of AI, raising awareness of the ethics of using AI, identifying AI in communication with a person, and information security.
According to the deputy head of the Analytical Center under the Russian government, Sergey Nakvasin, currently, about 20 countries are dealing with the ethics of artificial intelligence.
The Russian code was authored by the AI Alliance jointly with the Analytical Center under the Russian government and the Economic Development Ministry. Joining the code is voluntary.
The document was signed by the alliance members Sberbank, Gazprom Neft, Yandex VK, MTS and the Russian Direct Investment Fund, as well as representatives of Skolkovo, Rostelecom, Rosatom, InfoWatch and real estate platform Cian.
For the first time, when signing agreements, a Gosklyuch mobile signature is used. Gosklyuch is a service, which generates a user's electronic signature free of charge. The signature is stored within a protected infrastructure of e-government.
In December 2020, Russian President Vladimir Putin called for the development of an internal moral and ethical code in the field of artificial intelligence. The head of state stressed it was important to ensure the participation of the professional community and business in the formation of ethical principles for the use of AI.
Source: Russian News Agency
Classification
Language: ENGLISH
Publication-Type: Wire
Subject: ETHICS (97%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); ASSOCIATIONS & ORGANIZATIONS (89%); AGREEMENTS (78%); BUSINESS NEWS (76%); ECONOMIC DEVELOPMENT (76%); ELECTRONIC GOVERNMENT (76%); GOVERNMENT & PUBLIC ADMINISTRATION (74%); FUNDS & INVESTMENT TRUSTS (72%); HEADS OF STATE & GOVERNMENT (72%)
Company:  YANDEX NV (90%);  OAO GAZPROM NEFT (65%);  ROSTELECOM OAO (64%);  SBERBANK ROSSII OAO (53%)
Ticker: YNDX (NASDAQ) (90%); SIBN (RTS) (65%); GAZ (LSE) (65%); RTKM (RTS) (64%); RKMD (LSE) (64%); SBER (RTS) (53%); SBER (LSE) (53%)
Industry: NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (90%); NAICS211130 NATURAL GAS EXTRACTION (65%); NAICS211120 CRUDE PETROLEUM EXTRACTION (65%); SIC1311 CRUDE PETROLEUM & NATURAL GAS (65%); NAICS238210 ELECTRICAL CONTRACTORS & OTHER WIRING INSTALLATION CONTRACTORS (64%); SIC1731 ELECTRICAL WORK (64%); NAICS522293 INTERNATIONAL TRADE FINANCING (53%); NAICS522110 COMMERCIAL BANKING (53%); SIC6082 FOREIGN TRADE & INTERNATIONAL BANKING INSTITUTIONS (53%); SIC6021 NATIONAL COMMERCIAL BANKS (53%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); NEWS SYNDICATION (78%); PRESS AGENCY RELEASES (78%); INFORMATION SECURITY & PRIVACY (76%); DIGITAL SIGNATURES (73%); FUNDS & INVESTMENT TRUSTS (72%)
Person: VLADIMIR PUTIN (57%)
Geographic: MOSCOW, RUSSIAN FEDERATION (58%); RUSSIAN FEDERATION (95%)
Load-Date: October 27, 2021",neutral,0.7906988859176636,balanced/neutral,"['privacy', 'security', 'agency']",[],"['standards', 'law']",[],3,0,2,0
2021,Unknown Title,"Body
Tribune News Network
Doha
The Hamad Bin Khalifa University (HBKU) College of Science and Engineering (CSE) was one of the organisers of the 2nd International Workshop on Ethics in Software Engineering Research and Practice (SEthics 2021), which took place virtually on June 4.
The workshop series, held in conjunction with the 43rd International Conference on Software Engineering (ICSE 2021), provides a forum for discussion of the intersection of ethics, human values, and software engineering activities. Dr. Raian Ali, professor of Information and Computing Technology (ICT), CSE at HBKU and a chair on the SEthics organising committee, helped to ensure diversity of content that shed light on current practices and research results while identifying emerging trends and research directions in software engineering.
Following an opening keynote on the ""Ethics of AI and Emerging Digital Technologies,"" workshop sessions included presentations of research papers, working groups, and panel discussions between academics and practitioners from the United States, Germany, Sweden, Qatar, and the United Kingdom. Their talks covered a wide range of topics, including how ethical lapses lead to complicated and problematic software; filling gaps in ethical research and practice toward ethical data-driven software; and trustworthy artificial intelligence.
During a panel titled Software for Behavioural Mining and Change: The Ethical Dilemma, chaired by Dr. Ali, CSE's Dr. Dena Al-Thani, assistant professor in ICT, presented her user-centered approach to design, disability, and ethics, as well as the major projects she leads in Qatar in this area.
Prof. Bernd Stahl of De Montfort University's Centre for Computing and Social Responsibility and Prof. Gopal Ramchurn of the University of Southampton were among the panel's UK-based experts. The panel and workshop discussed the work of the CSE Technology and Behavioral Research Group (i-Solouk), as well as ethical issues that arise when technology influences people's behavior and decisions.
Speaking after the workshop, Dr. Mounir Hamdi, Founding Dean, CSE, said: ""As the ethical landscape surrounding the use of emerging software applications such as the Internet of Things and AI becomes more complex, it is even more fundamental to answer the ethical concerns and questions raised about the role of human values in software engineering practice and research. It is an area in which CSE's high-impact research allows us to have an impact through informed contributions. This workshop during ICSE 2021 is one of the prime venues for high-quality research and conclusions drawn to inform a variety of stakeholders in this specialized area, and CSE's participation was exceptionally productive.""
Classification
Language: ENGLISH
Publication-Type: Newspaper
Journal Code: 25
Subject: ETHICS (95%); ENGINEERING (92%); COMPUTER ENGINEERING (90%); EXPERIMENTATION & RESEARCH (90%); COLLEGE & UNIVERSITY PROFESSORS (89%); ARTIFICIAL INTELLIGENCE (88%); BEHAVIOR & COGNITION (78%); CONFERENCES & CONVENTIONS (78%); EMERGING TECHNOLOGY (78%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (78%); ARTIFICIAL INTELLIGENCE ETHICS (77%); TRENDS & EVENTS (76%); TRENDS (74%)
Industry: COMPUTER SOFTWARE (96%); SOFTWARE DEVELOPMENT & ENGINEERING (94%); ENGINEERING (92%); COMPUTER ENGINEERING (90%); SOFTWARE SERVICES & APPLICATIONS (90%); COLLEGE & UNIVERSITY PROFESSORS (89%); ARTIFICIAL INTELLIGENCE (88%); COMPUTER NETWORKS (78%); COMPUTING & INFORMATION TECHNOLOGY (78%); ARTIFICIAL INTELLIGENCE ETHICS (77%); INTERNET OF THINGS (73%)
Geographic: DOHA, QATAR (59%); QATAR (90%); UNITED KINGDOM (90%); UNITED STATES (79%); GERMANY (55%)
Load-Date: June 9, 2021","Tribune News Network
Doha
The Hamad Bin Khalifa University (HBKU) College of Science and Engineering (CSE) was one of the organisers of the 2nd International Workshop on Ethics in Software Engineering Research and Practice (SEthics 2021), which took place virtually on June 4.
The workshop series, held in conjunction with the 43rd International Conference on Software Engineering (ICSE 2021), provides a forum for discussion of the intersection of ethics, human values, and software engineering activities. Dr. Raian Ali, professor of Information and Computing Technology (ICT), CSE at HBKU and a chair on the SEthics organising committee, helped to ensure diversity of content that shed light on current practices and research results while identifying emerging trends and research directions in software engineering.
Following an opening keynote on the ""Ethics of AI and Emerging Digital Technologies,"" workshop sessions included presentations of research papers, working groups, and panel discussions between academics and practitioners from the United States, Germany, Sweden, Qatar, and the United Kingdom. Their talks covered a wide range of topics, including how ethical lapses lead to complicated and problematic software; filling gaps in ethical research and practice toward ethical data-driven software; and trustworthy artificial intelligence.
During a panel titled Software for Behavioural Mining and Change: The Ethical Dilemma, chaired by Dr. Ali, CSE's Dr. Dena Al-Thani, assistant professor in ICT, presented her user-centered approach to design, disability, and ethics, as well as the major projects she leads in Qatar in this area.
Prof. Bernd Stahl of De Montfort University's Centre for Computing and Social Responsibility and Prof. Gopal Ramchurn of the University of Southampton were among the panel's UK-based experts. The panel and workshop discussed the work of the CSE Technology and Behavioral Research Group (i-Solouk), as well as ethical issues that arise when technology influences people's behavior and decisions.
Speaking after the workshop, Dr. Mounir Hamdi, Founding Dean, CSE, said: ""As the ethical landscape surrounding the use of emerging software applications such as the Internet of Things and AI becomes more complex, it is even more fundamental to answer the ethical concerns and questions raised about the role of human values in software engineering practice and research. It is an area in which CSE's high-impact research allows us to have an impact through informed contributions. This workshop during ICSE 2021 is one of the prime venues for high-quality research and conclusions drawn to inform a variety of stakeholders in this specialized area, and CSE's participation was exceptionally productive.""
Classification
Language: ENGLISH
Publication-Type: Newspaper
Journal Code: 25
Subject: ETHICS (95%); ENGINEERING (92%); COMPUTER ENGINEERING (90%); EXPERIMENTATION & RESEARCH (90%); COLLEGE & UNIVERSITY PROFESSORS (89%); ARTIFICIAL INTELLIGENCE (88%); BEHAVIOR & COGNITION (78%); CONFERENCES & CONVENTIONS (78%); EMERGING TECHNOLOGY (78%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (78%); ARTIFICIAL INTELLIGENCE ETHICS (77%); TRENDS & EVENTS (76%); TRENDS (74%)
Industry: COMPUTER SOFTWARE (96%); SOFTWARE DEVELOPMENT & ENGINEERING (94%); ENGINEERING (92%); COMPUTER ENGINEERING (90%); SOFTWARE SERVICES & APPLICATIONS (90%); COLLEGE & UNIVERSITY PROFESSORS (89%); ARTIFICIAL INTELLIGENCE (88%); COMPUTER NETWORKS (78%); COMPUTING & INFORMATION TECHNOLOGY (78%); ARTIFICIAL INTELLIGENCE ETHICS (77%); INTERNET OF THINGS (73%)
Geographic: DOHA, QATAR (59%); QATAR (90%); UNITED KINGDOM (90%); UNITED STATES (79%); GERMANY (55%)
Load-Date: June 9, 2021",neutral,0.908286988735199,balanced/neutral,[],[],[],[],0,0,0,0
2021,Unknown Title,"Body
Data scientist and ethical AI advocate Saishruthi Swaminathan is using her experience and expertise to promote responsible behavior in machine learning creations through her lectures, speeches, and writing
Meet Saishruthi, a professional expertise on Ethical AI, open source technology, and machine learning. The young data scientist from India also advocates for women's empowerment and participation in technology, especially in AI and ML.
Over the past years, Saishruthi has participated in several activities, events, and projects that established her as an expert and advocate in Ethical AI. From the most remote part of India to Silicon Valley, she has inspired other women with her intelligence, creativity, and passion.
She has also participated in diversity and inclusivity events by delivering speech on discrimination mitigation in Artificial Intelligence. Her participation as chief guest at GUVI - a Guinness world record event is phenomenal. She used the platform to share her knowledge of Ethical AI and machine learning with over 600,000 individuals who came to learn to code.
Saishruthi Swaminathan is a young data scientist from India. She has B.E in electrical and electronic engineering from Sri Sairam Engineering College, Tamil Nadu, India, and MS Electrical Master's Degree in engineering from San Jose State University, USA.
She is a Technical Lead at CODAIT Machine Learning, a team within IBM. Her team builds open-source software that can be integrated and used on enterprise platforms. Her work and leadership skills have led to the creation of interesting programs that add so much value to human life, businesses, and organizations.
Machine Learning algorithms are beginning to directly impact human life by making important decisions that affect people every day. Saishruthi believes it's important to ensure these algorithms are built to respond to ethical situations ethically.
Saishruthi Swaminathan is very passionate about Ethical AI. Her deep knowledge of the subject and understanding of the concept makes her one of the top women in the world who has contributed to ethical and trusted AI. So far, she has taken part in more than 50 talks in Artificial Intelligence, enlightening professionals, students, enthusiasts, and experts.
She's a women empowerment advocate working tirelessly to help girls and women in rural areas of India access education. She writes articles and posts them on medium.com in her spare time, and many of her posts have become very popular with readers. Saishruthi also teaches AI courses on COURSERA, an online learning platform that works with authors who're experts in their fields.
For more information, please visit here.
About Saishruthi Swaminathan
Saishruthi Swaminathan is a data scientist and Ethical AI advocate. She has a master's degree in electrical engineering and leads a team of developers in IBM that create open-source programs. She's a women empowerment advocate, author, teacher, and speaker of repute.
Media Contact
Company Name: Saishruthi Swaminathan
Contact Person: Media Relations
Email: milesritzy32@gmail.com
Country: India
Website: https://www.linkedin.com/in/saishruthi-swaminathan/
Source: www.abnewswire.com
(Distributed by M2 Communications (www.m2.com)) 
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: INVESTREND
Subject: ARTIFICIAL INTELLIGENCE ETHICS (94%); MACHINE LEARNING (94%); ARTIFICIAL INTELLIGENCE (90%); DATA SCIENCE (90%); ENGINEERING (90%); ETHICS (90%); FEMINISM & WOMEN'S RIGHTS (89%); CERTIFICATES, DEGREES & DIPLOMAS (87%); ELECTRICAL ENGINEERING (87%); WOMEN (78%); DISCRIMINATION (77%); PROFESSIONAL WORKERS (77%); WRITERS (77%); ELECTRONICS ENGINEERING (73%); DIVERSITY & INCLUSION (72%); STUDENTS & STUDENT LIFE (71%); TRENDS & EVENTS (71%); RURAL COMMUNITIES (70%)
Organization: SAN JOSE STATE UNIVERSITY (55%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (94%); MACHINE LEARNING (94%); OPEN SOURCE SOFTWARE (91%); ARTIFICIAL INTELLIGENCE (90%); DATA SCIENCE (90%); ENGINEERING (90%); ELECTRICAL ENGINEERING (87%); WRITERS (77%); ELECTRONICS ENGINEERING (73%); COMPUTER SOFTWARE (50%)
Geographic: SILICON VALLEY, CA, USA (79%); SAN FRANCISCO BAY AREA, CA, USA (77%); SAN JOSE, CA, USA (57%); TAMIL NADU, INDIA (73%); INDIA (92%)
Load-Date: May 19, 2021","Data scientist and ethical AI advocate Saishruthi Swaminathan is using her experience and expertise to promote responsible behavior in machine learning creations through her lectures, speeches, and writing
Meet Saishruthi, a professional expertise on Ethical AI, open source technology, and machine learning. The young data scientist from India also advocates for women's empowerment and participation in technology, especially in AI and ML.
Over the past years, Saishruthi has participated in several activities, events, and projects that established her as an expert and advocate in Ethical AI. From the most remote part of India to Silicon Valley, she has inspired other women with her intelligence, creativity, and passion.
She has also participated in diversity and inclusivity events by delivering speech on discrimination mitigation in Artificial Intelligence. Her participation as chief guest at GUVI - a Guinness world record event is phenomenal. She used the platform to share her knowledge of Ethical AI and machine learning with over 600,000 individuals who came to learn to code.
Saishruthi Swaminathan is a young data scientist from India. She has B.E in electrical and electronic engineering from Sri Sairam Engineering College, Tamil Nadu, India, and MS Electrical Master's Degree in engineering from San Jose State University, USA.
She is a Technical Lead at CODAIT Machine Learning, a team within IBM. Her team builds open-source software that can be integrated and used on enterprise platforms. Her work and leadership skills have led to the creation of interesting programs that add so much value to human life, businesses, and organizations.
Machine Learning algorithms are beginning to directly impact human life by making important decisions that affect people every day. Saishruthi believes it's important to ensure these algorithms are built to respond to ethical situations ethically.
Saishruthi Swaminathan is very passionate about Ethical AI. Her deep knowledge of the subject and understanding of the concept makes her one of the top women in the world who has contributed to ethical and trusted AI. So far, she has taken part in more than 50 talks in Artificial Intelligence, enlightening professionals, students, enthusiasts, and experts.
She's a women empowerment advocate working tirelessly to help girls and women in rural areas of India access education. She writes articles and posts them on medium.com in her spare time, and many of her posts have become very popular with readers. Saishruthi also teaches AI courses on COURSERA, an online learning platform that works with authors who're experts in their fields.
For more information, please visit here.
About Saishruthi Swaminathan
Saishruthi Swaminathan is a data scientist and Ethical AI advocate. She has a master's degree in electrical engineering and leads a team of developers in IBM that create open-source programs. She's a women empowerment advocate, author, teacher, and speaker of repute.
Media Contact
Company Name: Saishruthi Swaminathan
Contact Person: Media Relations
Email: milesritzy32@gmail.com
Country: India
Website: https://www.linkedin.com/in/saishruthi-swaminathan/
Source: www.abnewswire.com
(Distributed by M2 Communications (www.m2.com)) 
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: INVESTREND
Subject: ARTIFICIAL INTELLIGENCE ETHICS (94%); MACHINE LEARNING (94%); ARTIFICIAL INTELLIGENCE (90%); DATA SCIENCE (90%); ENGINEERING (90%); ETHICS (90%); FEMINISM & WOMEN'S RIGHTS (89%); CERTIFICATES, DEGREES & DIPLOMAS (87%); ELECTRICAL ENGINEERING (87%); WOMEN (78%); DISCRIMINATION (77%); PROFESSIONAL WORKERS (77%); WRITERS (77%); ELECTRONICS ENGINEERING (73%); DIVERSITY & INCLUSION (72%); STUDENTS & STUDENT LIFE (71%); TRENDS & EVENTS (71%); RURAL COMMUNITIES (70%)
Organization: SAN JOSE STATE UNIVERSITY (55%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (94%); MACHINE LEARNING (94%); OPEN SOURCE SOFTWARE (91%); ARTIFICIAL INTELLIGENCE (90%); DATA SCIENCE (90%); ENGINEERING (90%); ELECTRICAL ENGINEERING (87%); WRITERS (77%); ELECTRONICS ENGINEERING (73%); COMPUTER SOFTWARE (50%)
Geographic: SILICON VALLEY, CA, USA (79%); SAN FRANCISCO BAY AREA, CA, USA (77%); SAN JOSE, CA, USA (57%); TAMIL NADU, INDIA (73%); INDIA (92%)
Load-Date: May 19, 2021",positive,0.7629821300506592,balanced/neutral,"['discrimination', 'inclusivity', 'access']",[],['advocate'],['machine learning'],3,0,1,1
2021,Unknown Title,"Dateline: ATLANTA, June 17, 2021 
Body
PR Newswire
 Convercent by OneTrust today announced the availability ofThird-Party Risk Management for Ethics and Compliance professionals. The new solution enables ethics and compliance teams to fully understand and manage third-party risk through automated screening, due diligence, risk management, and ongoing monitoring of key risks that arise from the scope, activities, location, and persons present in that third party relationship, to name a few. Using the power fromleading TPRM solution OneTrust Vendorpedia, Third-Party Risk Management for Ethics and Compliance leverages the collective power of Convercent by OneTrust and theOneTrust privacy, security, and data governance platform. 
Register for the webinar: Third-Party Risk Management and Due Diligence for Ethics and Compliance on Tuesday, June 29 at 9 am MDT | 11 am EDT | 4 pm BST
Increased reliance on third parties, growing ethical concerns, and new regulations create complex requirements for ethics and compliance professionals. Some of those obligations include understanding the potential risk present in your third parties. Accordingly, ethics and compliance professionals must conduct third-party screening, due diligence, risk management and mitigation, and ongoing monitoring throughout the relationship with third parties. 
Third-Party Risk Management for Ethics and Compliance helps address these challenges with integration and automation, ultimately reducing the ethical and compliance risks associated with those third parties. Rich integrations with screening and compliance monitoring providers adds even more visibility into the risk in third-party relationships. 
The solution enables automation of the end-to-end third-party due diligence process through key capabilities including: 
Screening: Run compliance checks for your third parties against sanctions lists, adverse media and other sources, such as PEP lists, Anti-Slavery, Bribery and Corruption, and more. Inherent Risk-Based Tiering: Automatically calculate inherent risk scores to prioritize further diligence and assessment to the third parties that present the most risk to your organization. Risk Assessment: Leverage dozens of out-of-the-box ethics & compliance third-party assessment templates and automated risk assessment workflows with intelligent risk flagging. Risk Management: Reduce risk with automated treatment workflows, leveraging out-of-the-box mitigation recommendations and control frameworks as well as allow for cross-departmental collaboration and management. Compliance Reporting: Understand the state of your third-party risk management program with interactive dashboards, executive-ready PDF reports, and recordkeeping to demonstrate compliance. Ongoing Monitoring: Monitor your third parties over time, running routine adverse media and other compliance checks that trigger pre-determined actions as additional risks.
Register for the webinar: Third-Party Risk Management and Due Diligence for Ethics and Compliance on Tuesday, June 29 at 9 am MDT | 11 am EDT | 4 pm BST
""Ethics and compliance teams evaluate numerous types of third parties including suppliers, agents, venture partners, investors, customers, and many others. As a result of this outsourcing, third parties introduce more compliance risk than ever before. The Third-Party Risk Management for Ethics and Compliance solution will help automate screening, due diligence, risk management, and ongoing monitoring, providing greater visibility into third parties and the risks they pose from an ethics and compliance standpoint,"" said Asha Palmer, Chief Ethics and Compliance Officer at Convercent by OneTrust. 
Patrick Quinlan, Chief Executive Officer at Convercent by OneTrust added, ""This solution combines the ethics and compliance expertise of the Convercent by OneTrust team with the world-class OneTrust privacy, security, and governance platform, furthering our shared vision to help organizations around the world be more trusted.""
To learn more about Convercent by OneTrust's Third-Party Risk Management for Ethics and Compliance,register for the webinar. For information or to request a demo, visit Convercent.com. 
Availability:
Convercent by OneTrust Third-Party Risk Management for Ethics and Compliance is available today. To purchase, reach out to your Convercent by OneTrust account executive or request a demo.
Resources:                 
OneTrust News: OneTrust Acquires Ethics and Compliance Leader Convercent Webinar: Third-Party Risk Management and Due Diligence for Ethics and Compliance Demo: Third-Party Risk Management and Due Diligence for Ethics and Compliance
OneTrust and Convercent by OneTrust are registered trademarks or trademarks of OneTrust LLC or its subsidiaries in the United States and other jurisdictions.
About Convercent by OneTrustConvercent by OneTrust is the leading global provider of ethics and compliance software, empowering the world's largest and most admired companies to understand organizational risk, protect their brand, and engage employees with their ethics and compliance program. The Convercent by OneTrust Ethics Cloud Platform, which includes the Ethics and Compliance Portal; Helpline and Case Manager; Policy and Learning; and Disclosures and Conflict of Interest, leverages a global dataset to deliver business leaders the insights required to make proactive, informed decisions about their company's ethical health through real-time dashboards and analytics.
Convercent by OneTrust is a part of the #1 most widely used privacy, security and governance platform used by more than 10,000 customers and powered by 150 awarded patents. OneTrust is powered by the OneTrust Athena™ AI and robotic automation engine, and integrates seamlessly with the full OneTrust platform, including OneTrust Privacy Management Software, OneTrust DataDiscovery™, OneTrust DataGovernance™, OneTrust GRC, OneTrust Ethics, OneTrust PreferenceChoice™, OneTrust ESG, and OneTrust DataGuidance™.
Media Contact
Katie Thompson
704-776-8127
media@onetrust.com
 View original content to download multimedia:http://www.prnewswire.com/news-releases/convercent-by-onetrust-announces-third-party-risk-management-solution-for-ethics--compliance-to-automate-third-party-screening-due-diligence-and-ongoing-monitoring-301314515.html
SOURCE OneTrust
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: RISK MANAGEMENT (96%); ETHICS (92%); REGULATORY COMPLIANCE (91%); BUSINESS ETHICS (90%); PRESS RELEASES (90%); HUMAN RIGHTS VIOLATIONS (78%); OUTSOURCING (78%); POLITICALLY EXPOSED PERSONS (78%); VENTURE CAPITAL (78%); CORRUPTION (73%); NEGATIVE NEWS (73%); SLAVERY (73%); BRIBERY (50%); BUSINESS REPORTS & FORECASTS (50%); ONETRUST-solution (%); PDT New Products and Services (%); CCA Conference Call Announcements (%)
Company: OneTrust
Industry: RISK MANAGEMENT (96%); TELECONFERENCING (89%); VENTURE CAPITAL (78%); CPR Computer; Electronics Products (%); STW Computer Software (%); HTS Makers and developers of computer and network security products and services, Internet firewalls, intrusion detection, encryption software, virus protection (%)
Geographic: ATLANTA, GA, USA (59%); GEORGIA, USA (59%); Georgia
Load-Date: June 17, 2021","PR Newswire
 Convercent by OneTrust today announced the availability ofThird-Party Risk Management for Ethics and Compliance professionals. The new solution enables ethics and compliance teams to fully understand and manage third-party risk through automated screening, due diligence, risk management, and ongoing monitoring of key risks that arise from the scope, activities, location, and persons present in that third party relationship, to name a few. Using the power fromleading TPRM solution OneTrust Vendorpedia, Third-Party Risk Management for Ethics and Compliance leverages the collective power of Convercent by OneTrust and theOneTrust privacy, security, and data governance platform. 
Register for the webinar: Third-Party Risk Management and Due Diligence for Ethics and Compliance on Tuesday, June 29 at 9 am MDT | 11 am EDT | 4 pm BST
Increased reliance on third parties, growing ethical concerns, and new regulations create complex requirements for ethics and compliance professionals. Some of those obligations include understanding the potential risk present in your third parties. Accordingly, ethics and compliance professionals must conduct third-party screening, due diligence, risk management and mitigation, and ongoing monitoring throughout the relationship with third parties. 
Third-Party Risk Management for Ethics and Compliance helps address these challenges with integration and automation, ultimately reducing the ethical and compliance risks associated with those third parties. Rich integrations with screening and compliance monitoring providers adds even more visibility into the risk in third-party relationships. 
The solution enables automation of the end-to-end third-party due diligence process through key capabilities including: 
Screening: Run compliance checks for your third parties against sanctions lists, adverse media and other sources, such as PEP lists, Anti-Slavery, Bribery and Corruption, and more. Inherent Risk-Based Tiering: Automatically calculate inherent risk scores to prioritize further diligence and assessment to the third parties that present the most risk to your organization. Risk Assessment: Leverage dozens of out-of-the-box ethics & compliance third-party assessment templates and automated risk assessment workflows with intelligent risk flagging. Risk Management: Reduce risk with automated treatment workflows, leveraging out-of-the-box mitigation recommendations and control frameworks as well as allow for cross-departmental collaboration and management. Compliance Reporting: Understand the state of your third-party risk management program with interactive dashboards, executive-ready PDF reports, and recordkeeping to demonstrate compliance. Ongoing Monitoring: Monitor your third parties over time, running routine adverse media and other compliance checks that trigger pre-determined actions as additional risks.
Register for the webinar: Third-Party Risk Management and Due Diligence for Ethics and Compliance on Tuesday, June 29 at 9 am MDT | 11 am EDT | 4 pm BST
""Ethics and compliance teams evaluate numerous types of third parties including suppliers, agents, venture partners, investors, customers, and many others. As a result of this outsourcing, third parties introduce more compliance risk than ever before. The Third-Party Risk Management for Ethics and Compliance solution will help automate screening, due diligence, risk management, and ongoing monitoring, providing greater visibility into third parties and the risks they pose from an ethics and compliance standpoint,"" said Asha Palmer, Chief Ethics and Compliance Officer at Convercent by OneTrust. 
Patrick Quinlan, Chief Executive Officer at Convercent by OneTrust added, ""This solution combines the ethics and compliance expertise of the Convercent by OneTrust team with the world-class OneTrust privacy, security, and governance platform, furthering our shared vision to help organizations around the world be more trusted.""
To learn more about Convercent by OneTrust's Third-Party Risk Management for Ethics and Compliance,register for the webinar. For information or to request a demo, visit Convercent.com. 
Availability:
Convercent by OneTrust Third-Party Risk Management for Ethics and Compliance is available today. To purchase, reach out to your Convercent by OneTrust account executive or request a demo.
Resources:                 
OneTrust News: OneTrust Acquires Ethics and Compliance Leader Convercent Webinar: Third-Party Risk Management and Due Diligence for Ethics and Compliance Demo: Third-Party Risk Management and Due Diligence for Ethics and Compliance
OneTrust and Convercent by OneTrust are registered trademarks or trademarks of OneTrust LLC or its subsidiaries in the United States and other jurisdictions.
About Convercent by OneTrustConvercent by OneTrust is the leading global provider of ethics and compliance software, empowering the world's largest and most admired companies to understand organizational risk, protect their brand, and engage employees with their ethics and compliance program. The Convercent by OneTrust Ethics Cloud Platform, which includes the Ethics and Compliance Portal; Helpline and Case Manager; Policy and Learning; and Disclosures and Conflict of Interest, leverages a global dataset to deliver business leaders the insights required to make proactive, informed decisions about their company's ethical health through real-time dashboards and analytics.
Convercent by OneTrust is a part of the #1 most widely used privacy, security and governance platform used by more than 10,000 customers and powered by 150 awarded patents. OneTrust is powered by the OneTrust Athena™ AI and robotic automation engine, and integrates seamlessly with the full OneTrust platform, including OneTrust Privacy Management Software, OneTrust DataDiscovery™, OneTrust DataGovernance™, OneTrust GRC, OneTrust Ethics, OneTrust PreferenceChoice™, OneTrust ESG, and OneTrust DataGuidance™.
Media Contact
Katie Thompson
704-776-8127
media@onetrust.com
 View original content to download multimedia:http://www.prnewswire.com/news-releases/convercent-by-onetrust-announces-third-party-risk-management-solution-for-ethics--compliance-to-automate-third-party-screening-due-diligence-and-ongoing-monitoring-301314515.html
SOURCE OneTrust
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: RISK MANAGEMENT (96%); ETHICS (92%); REGULATORY COMPLIANCE (91%); BUSINESS ETHICS (90%); PRESS RELEASES (90%); HUMAN RIGHTS VIOLATIONS (78%); OUTSOURCING (78%); POLITICALLY EXPOSED PERSONS (78%); VENTURE CAPITAL (78%); CORRUPTION (73%); NEGATIVE NEWS (73%); SLAVERY (73%); BRIBERY (50%); BUSINESS REPORTS & FORECASTS (50%); ONETRUST-solution (%); PDT New Products and Services (%); CCA Conference Call Announcements (%)
Company: OneTrust
Industry: RISK MANAGEMENT (96%); TELECONFERENCING (89%); VENTURE CAPITAL (78%); CPR Computer; Electronics Products (%); STW Computer Software (%); HTS Makers and developers of computer and network security products and services, Internet firewalls, intrusion detection, encryption software, virus protection (%)
Geographic: ATLANTA, GA, USA (59%); GEORGIA, USA (59%); Georgia
Load-Date: June 17, 2021",neutral,0.7828718423843384,balanced/neutral,"['privacy', 'security', 'human rights']",[],"['policy', 'governance', 'compliance', 'must']",[],3,0,4,0
2021,Unknown Title,"Body
In the framework of the international forum Ethics of Artificial Intelligence: The Beginning of Trust, Russias largest tech companies adopted an AI code of ethics, which was drafted by the Artificial Intelligence Alliance. The document was signed by Sber, Yandex, MTS, VK, Gazprom Neft, and the Russian Direct Investment Fund (RDIF), as well as leading companies and research organizations. First Deputy Chief of Staff of the Presidential Executive Office Sergey Kiriyenko and Deputy Prime Minister Dmitry Chernyshenko were present at the signing.
The code of ethics enshrines a human-centric and humanistic approach to the development of AI technology, the principles of non-discrimination, data security, and information security, identification of AI in communication with humans, respect for human autonomy, and responsibility for the consequences of using AI. Commitment to the code of ethics will be a key social responsibility element for companies developing and implementing AI technology in Russia.
The AI Code of Ethics is to become a recommendation document on ethics for all AI market participants: government, business, Russian and foreign developers. The code establishes general ethical principles and behavioral standards, which the participants may choose to adhere to in the field of artificial intelligence.
As a tool for implementing the aforementioned principles and provisions, the code provides for the creation of a National AI Ethics Commission, the appointment of ethics commissioners, as well as the possibility of creating collegial industry ethics bodies. The Alliance will also develop methodological recommendations and a set of best and/or worst practices for dealing with emerging ethical issues in the AI life cycle.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Journal Code: 193
Subject: ETHICS (97%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); BUSINESS ETHICS (78%); BUSINESS ETHICS & CORPORATE CITIZENSHIP (78%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (78%); HEADS OF STATE & GOVERNMENT (76%); NEGATIVE SOCIETAL NEWS (74%); ESG FACTORS - SOCIAL (73%); FUNDS & INVESTMENT TRUSTS (73%); APPOINTMENTS (71%); GOVERNMENT & PUBLIC ADMINISTRATION (71%); PRIME MINISTERS (71%); ASSOCIATIONS & ORGANIZATIONS (56%)
Company:  OAO GAZPROM NEFT (72%);  YANDEX NV (58%)
Ticker: SIBN (RTS) (72%); GAZ (LSE) (72%); YNDX (NASDAQ) (58%)
Industry: NAICS211130 NATURAL GAS EXTRACTION (72%); NAICS211120 CRUDE PETROLEUM EXTRACTION (72%); SIC1311 CRUDE PETROLEUM & NATURAL GAS (72%); NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (58%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); BIG TECH (90%); INFORMATION TECHNOLOGY INDUSTRY (90%); PRESS AGENCY RELEASES (90%); DATA SECURITY (74%); FUNDS & INVESTMENT TRUSTS (73%); INFORMATION SECURITY & PRIVACY (69%)
Geographic: RUSSIAN FEDERATION (91%)
Load-Date: October 27, 2021","In the framework of the international forum Ethics of Artificial Intelligence: The Beginning of Trust, Russias largest tech companies adopted an AI code of ethics, which was drafted by the Artificial Intelligence Alliance. The document was signed by Sber, Yandex, MTS, VK, Gazprom Neft, and the Russian Direct Investment Fund (RDIF), as well as leading companies and research organizations. First Deputy Chief of Staff of the Presidential Executive Office Sergey Kiriyenko and Deputy Prime Minister Dmitry Chernyshenko were present at the signing.
The code of ethics enshrines a human-centric and humanistic approach to the development of AI technology, the principles of non-discrimination, data security, and information security, identification of AI in communication with humans, respect for human autonomy, and responsibility for the consequences of using AI. Commitment to the code of ethics will be a key social responsibility element for companies developing and implementing AI technology in Russia.
The AI Code of Ethics is to become a recommendation document on ethics for all AI market participants: government, business, Russian and foreign developers. The code establishes general ethical principles and behavioral standards, which the participants may choose to adhere to in the field of artificial intelligence.
As a tool for implementing the aforementioned principles and provisions, the code provides for the creation of a National AI Ethics Commission, the appointment of ethics commissioners, as well as the possibility of creating collegial industry ethics bodies. The Alliance will also develop methodological recommendations and a set of best and/or worst practices for dealing with emerging ethical issues in the AI life cycle.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Journal Code: 193
Subject: ETHICS (97%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); BUSINESS ETHICS (78%); BUSINESS ETHICS & CORPORATE CITIZENSHIP (78%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (78%); HEADS OF STATE & GOVERNMENT (76%); NEGATIVE SOCIETAL NEWS (74%); ESG FACTORS - SOCIAL (73%); FUNDS & INVESTMENT TRUSTS (73%); APPOINTMENTS (71%); GOVERNMENT & PUBLIC ADMINISTRATION (71%); PRIME MINISTERS (71%); ASSOCIATIONS & ORGANIZATIONS (56%)
Company:  OAO GAZPROM NEFT (72%);  YANDEX NV (58%)
Ticker: SIBN (RTS) (72%); GAZ (LSE) (72%); YNDX (NASDAQ) (58%)
Industry: NAICS211130 NATURAL GAS EXTRACTION (72%); NAICS211120 CRUDE PETROLEUM EXTRACTION (72%); SIC1311 CRUDE PETROLEUM & NATURAL GAS (72%); NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (58%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); BIG TECH (90%); INFORMATION TECHNOLOGY INDUSTRY (90%); PRESS AGENCY RELEASES (90%); DATA SECURITY (74%); FUNDS & INVESTMENT TRUSTS (73%); INFORMATION SECURITY & PRIVACY (69%)
Geographic: RUSSIAN FEDERATION (91%)
Load-Date: October 27, 2021",neutral,0.7323125600814819,balanced/neutral,"['privacy', 'discrimination', 'security', 'autonomy', 'agency']",['autonomy'],"['standards', 'framework']",[],5,1,2,0
2021,Unknown Title,"Byline: Jumana Abu-Khalaf, Research Fellow in Computing and Security, Edith Cowan University
Highlight: Between driverless cars, autonomous weapons and AI-powered medical diagnostic tools, it seems there will be no shortage of ethically-complex situations involving AI in the future.
Body
Artificial intelligence (AI) is already making decisions in the fields of business, health care and manufacturing. But AI algorithms generally still get help from people applying checks and making the final call. 
What would happen if AI systems had to make independent decisions, and ones that could mean life or death for humans? 
Pop culture has long portrayed our general distrust of AI. In the 2004 sci-fi movie I, Robot, detective Del Spooner (played by Will Smith) is suspicious of robots after being rescued by one from a car crash, while a 12-year-old girl was left to drown. He says:
I was the logical choice. It calculated that I had a 45% chance of survival. Sarah only had an 11% chance. That was somebody's baby - 11% is more than enough. A human being would've known that.
Unlike humans, robots lack a moral conscience and follow the ""ethics"" programmed into them. At the same time, human morality is highly variable. The ""right"" thing to do in any situation will depend on who you ask.
For machines to help us to their full potential, we need to make sure they behave ethically. So the question becomes: how do the ethics of AI developers and engineers influence the decisions made by AI? 
Read more: After 75 years, Isaac Asimov's Three Laws of Robotics need updating
The self-driving future
Imagine a future with self-driving cars that are fully autonomous. If everything works as intended, the morning commute will be an opportunity to prepare for the day's meetings, catch up on news, or sit back and relax. 
But what if things go wrong? The car approaches a traffic light, but suddenly the brakes fail and the computer has to make a split-second decision. It can swerve into a nearby pole and kill the passenger, or keep going and kill the pedestrian ahead. 
The computer controlling the car will only have access to limited information collected through car sensors, and will have to make a decision based on this. As dramatic as this may seem, we're only a few years away from potentially facing such dilemmas. 
Autonomous cars will generally provide safer driving, but accidents will be inevitable - especially in the foreseeable future, when these cars will be sharing the roads with human drivers and other road users.  
Tesla does not yet produce fully autonomous cars, although it plans to. In collision situations, Tesla cars don't automatically operate or deactivate the Automatic Emergency Braking (AEB) system if a human driver is in control. 
In other words, the driver's actions are not disrupted - even if they themselves are causing the collision. Instead, if the car detects a potential collision, it sends alerts to the driver to take action. 
In ""autopilot"" mode, however, the car should automatically brake for pedestrians. Some argue if the car can prevent a collision, then there is a moral obligation for it to override the driver's actions in every scenario. But would we want an autonomous car to make this decision?
What's a life worth?
What if a car's computer could evaluate the relative ""value"" of the passenger in its car and of the pedestrian? If its decision considered this value, technically it would just be making a cost-benefit analysis. 
This may sound alarming, but there are already technologies being developed that could allow for this to happen. For instance, the recently re-branded Meta (formerly Facebook) has highly evolved facial recognition that can easily identify individuals in a scene.
Read more: Facebook will drop its facial recognition system - but here's why we should be sceptical
If these data were incorporated into an autonomous vehicle's AI system, the algorithm could place a dollar value on each life. This possibility is depicted in an extensive 2018 study conducted by experts at the Massachusetts Institute of Technology and colleagues. 
Through the Moral Machine experiment, researchers posed various self-driving car scenarios that compelled participants to decide whether to kill a homeless pedestrian or an executive pedestrian. 
Results revealed participants' choices depended on the level of economic inequality in their country, wherein more economic inequality meant they were more likely to sacrifice the homeless man. 
While not quite as evolved, such data aggregation is already in use with China's social credit system, which decides what social entitlements people have. 
The health-care industry is another area where we will see AI making decisions that could save or harm humans. Experts are increasingly developing AI to spot anomalies in medical imaging, and to help physicians in prioritising medical care.
For now, doctors have the final say, but as these technologies become increasingly advanced, what will happen when a doctor and AI algorithm don't make the same diagnosis? 
Another example is an automated medicine reminder system. How should the system react if a patient refuses to take their medication? And how does that affect the patient's autonomy, and the overall accountability of the system? 
AI-powered drones and weaponry are also ethically concerning, as they can make the decision to kill. There are conflicting views on whether such technologies should be completely banned or regulated. For example, the use of autonomous drones can be limited to surveillance. 
Some have called for military robots to be programmed with ethics. But this raises issues about the programmer's accountability in the case where a drone kills civilians by mistake.
Read more: Gun-toting robo-dogs look like a dystopian nightmare. That's why they offer a powerful moral lesson
Philosophical dilemmas
There have been many philosophical debates regarding the ethical decisions AI will have to make. The classic example of this is the trolley problem.
People often struggle to make decisions that could have a life-changing outcome. When evaluating how we react to such situations, one study reported choices can vary depending on a range of factors including the respondant's age, gender and culture.
When it comes to AI systems, the algorithms training processes are critical to how they will work in the real world. A system developed in one country can be influenced by the views, politics, ethics and morals of that country, making it unsuitable for use in another place and time.
If the system was controlling aircraft, or guiding a missile, you'd want a high level of confidence it was trained with data that's representative of the environment it's being used in. 
Examples of failures and bias in technology implementation have included racist soap dispenser and inappropriate automatic image labelling. 
AI is not ""good"" or ""evil"". The effects it has on people will depend on the ethics of its developers. So to make the most of it, we'll need to reach a consensus on what we consider ""ethical"".
While private companies, public organisations and research institutions have their own guidelines for ethical AI, the United Nations has recommended developing what they call ""a comprehensive global standard-setting instrument"" to provide a global ethical AI framework - and ensure human rights are protected.
The authors do not work for, consult, own shares in or receive funding from any company or organisation that would benefit from this article, and have disclosed no relevant affiliations beyond their academic appointment.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); NEGATIVE NEWS (90%); ROBOTICS (90%); ARTIFICIAL INTELLIGENCE ETHICS (79%); MEDICAL DIAGNOSTICS, SCREENING & TESTING (78%); TRAFFIC ACCIDENTS (78%); ACCIDENTS & DISASTERS (77%); COMMUTING (72%); SCIENCE FICTION & FANTASY FILMS (56%)
Industry: ARTIFICIAL INTELLIGENCE (90%); AUTONOMOUS MOTOR VEHICLES (90%); MANUFACTURING (90%); ROBOTICS (90%); ARTIFICIAL INTELLIGENCE ETHICS (79%); ADVANCED DRIVER ASSISTANCE SYSTEMS (78%); INDUSTRIAL AUTOMATION (78%); MEDIA & TELECOMMUNICATIONS (78%); TRAFFIC ACCIDENTS (78%); AUTOMOTIVE ELECTRONICS (73%); SCIENCE FICTION & FANTASY FILMS (56%)
Person: WILL SMITH (76%)
Load-Date: November 23, 2021","Artificial intelligence (AI) is already making decisions in the fields of business, health care and manufacturing. But AI algorithms generally still get help from people applying checks and making the final call. 
What would happen if AI systems had to make independent decisions, and ones that could mean life or death for humans? 
Pop culture has long portrayed our general distrust of AI. In the 2004 sci-fi movie I, Robot, detective Del Spooner (played by Will Smith) is suspicious of robots after being rescued by one from a car crash, while a 12-year-old girl was left to drown. He says:
I was the logical choice. It calculated that I had a 45% chance of survival. Sarah only had an 11% chance. That was somebody's baby - 11% is more than enough. A human being would've known that.
Unlike humans, robots lack a moral conscience and follow the ""ethics"" programmed into them. At the same time, human morality is highly variable. The ""right"" thing to do in any situation will depend on who you ask.
For machines to help us to their full potential, we need to make sure they behave ethically. So the question becomes: how do the ethics of AI developers and engineers influence the decisions made by AI? 
Read more: After 75 years, Isaac Asimov's Three Laws of Robotics need updating
The self-driving future
Imagine a future with self-driving cars that are fully autonomous. If everything works as intended, the morning commute will be an opportunity to prepare for the day's meetings, catch up on news, or sit back and relax. 
But what if things go wrong? The car approaches a traffic light, but suddenly the brakes fail and the computer has to make a split-second decision. It can swerve into a nearby pole and kill the passenger, or keep going and kill the pedestrian ahead. 
The computer controlling the car will only have access to limited information collected through car sensors, and will have to make a decision based on this. As dramatic as this may seem, we're only a few years away from potentially facing such dilemmas. 
Autonomous cars will generally provide safer driving, but accidents will be inevitable - especially in the foreseeable future, when these cars will be sharing the roads with human drivers and other road users.  
Tesla does not yet produce fully autonomous cars, although it plans to. In collision situations, Tesla cars don't automatically operate or deactivate the Automatic Emergency Braking (AEB) system if a human driver is in control. 
In other words, the driver's actions are not disrupted - even if they themselves are causing the collision. Instead, if the car detects a potential collision, it sends alerts to the driver to take action. 
In ""autopilot"" mode, however, the car should automatically brake for pedestrians. Some argue if the car can prevent a collision, then there is a moral obligation for it to override the driver's actions in every scenario. But would we want an autonomous car to make this decision?
What's a life worth?
What if a car's computer could evaluate the relative ""value"" of the passenger in its car and of the pedestrian? If its decision considered this value, technically it would just be making a cost-benefit analysis. 
This may sound alarming, but there are already technologies being developed that could allow for this to happen. For instance, the recently re-branded Meta (formerly Facebook) has highly evolved facial recognition that can easily identify individuals in a scene.
Read more: Facebook will drop its facial recognition system - but here's why we should be sceptical
If these data were incorporated into an autonomous vehicle's AI system, the algorithm could place a dollar value on each life. This possibility is depicted in an extensive 2018 study conducted by experts at the Massachusetts Institute of Technology and colleagues. 
Through the Moral Machine experiment, researchers posed various self-driving car scenarios that compelled participants to decide whether to kill a homeless pedestrian or an executive pedestrian. 
Results revealed participants' choices depended on the level of economic inequality in their country, wherein more economic inequality meant they were more likely to sacrifice the homeless man. 
While not quite as evolved, such data aggregation is already in use with China's social credit system, which decides what social entitlements people have. 
The health-care industry is another area where we will see AI making decisions that could save or harm humans. Experts are increasingly developing AI to spot anomalies in medical imaging, and to help physicians in prioritising medical care.
For now, doctors have the final say, but as these technologies become increasingly advanced, what will happen when a doctor and AI algorithm don't make the same diagnosis? 
Another example is an automated medicine reminder system. How should the system react if a patient refuses to take their medication? And how does that affect the patient's autonomy, and the overall accountability of the system? 
AI-powered drones and weaponry are also ethically concerning, as they can make the decision to kill. There are conflicting views on whether such technologies should be completely banned or regulated. For example, the use of autonomous drones can be limited to surveillance. 
Some have called for military robots to be programmed with ethics. But this raises issues about the programmer's accountability in the case where a drone kills civilians by mistake.
Read more: Gun-toting robo-dogs look like a dystopian nightmare. That's why they offer a powerful moral lesson
Philosophical dilemmas
There have been many philosophical debates regarding the ethical decisions AI will have to make. The classic example of this is the trolley problem.
People often struggle to make decisions that could have a life-changing outcome. When evaluating how we react to such situations, one study reported choices can vary depending on a range of factors including the respondant's age, gender and culture.
When it comes to AI systems, the algorithms training processes are critical to how they will work in the real world. A system developed in one country can be influenced by the views, politics, ethics and morals of that country, making it unsuitable for use in another place and time.
If the system was controlling aircraft, or guiding a missile, you'd want a high level of confidence it was trained with data that's representative of the environment it's being used in. 
Examples of failures and bias in technology implementation have included racist soap dispenser and inappropriate automatic image labelling. 
AI is not ""good"" or ""evil"". The effects it has on people will depend on the ethics of its developers. So to make the most of it, we'll need to reach a consensus on what we consider ""ethical"".
While private companies, public organisations and research institutions have their own guidelines for ethical AI, the United Nations has recommended developing what they call ""a comprehensive global standard-setting instrument"" to provide a global ethical AI framework - and ensure human rights are protected.
The authors do not work for, consult, own shares in or receive funding from any company or organisation that would benefit from this article, and have disclosed no relevant affiliations beyond their academic appointment.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); NEGATIVE NEWS (90%); ROBOTICS (90%); ARTIFICIAL INTELLIGENCE ETHICS (79%); MEDICAL DIAGNOSTICS, SCREENING & TESTING (78%); TRAFFIC ACCIDENTS (78%); ACCIDENTS & DISASTERS (77%); COMMUTING (72%); SCIENCE FICTION & FANTASY FILMS (56%)
Industry: ARTIFICIAL INTELLIGENCE (90%); AUTONOMOUS MOTOR VEHICLES (90%); MANUFACTURING (90%); ROBOTICS (90%); ARTIFICIAL INTELLIGENCE ETHICS (79%); ADVANCED DRIVER ASSISTANCE SYSTEMS (78%); INDUSTRIAL AUTOMATION (78%); MEDIA & TELECOMMUNICATIONS (78%); TRAFFIC ACCIDENTS (78%); AUTOMOTIVE ELECTRONICS (73%); SCIENCE FICTION & FANTASY FILMS (56%)
Person: WILL SMITH (76%)
Load-Date: November 23, 2021",neutral,0.7541792392730713,balanced/neutral,"['surveillance', 'bias', 'accountability', 'human rights', 'autonomy', 'access', 'inequality']",['autonomy'],"['guidelines', 'framework', 'should', 'need to']","['facial recognition', 'autonomous vehicle', 'self-driving car', 'drone', 'robot', 'robotics', 'algorithm']",7,1,4,7
2021,Unknown Title,"Byline: Sohini Bagchi
Body
Disruptive technologies offer tremendous opportunity for businesses to become smarter, more flexible, and more responsive. But often people fail to understand the ramifications of their usage, running into bigger risks. It is then that the question of digital ethics comes into the picture.
Let's say, a private bank decides to use machine learning to filter out who is eligible for a loan. The machine learning will need training on a data set, which could be historical data or user created data. If, historically, the bank has denied loans to a certain category of people, the same bias would carry forward to the machine. So essentially, they have transferred this bias to the system and now the system would deny loans to that certain category of people.
The above example is a classic case of digital ethics - a topic of immediate concern and deliberation not only for private organizations, but also for the government, regulatory bodies and end users, explains Vishal Jain, Partner at Deloitte India in a freewheeling chat with CXOToday.
The growing concerns of Digital Ethics
While digital ethics has been a topic of discussion ever since businesses started moving online, the pandemic has made it a greater priory. With more businesses embracing digital technologies like AI/ML, big data, cloud, IoT and more in a big way. The need of the hour is to take a relook at the business operations layered on digital touch points, thinking around ethical uses of technology.
Given the growing concerns around information and content available over digital platforms, Deloitte Touche Tohmatsu India LLP conducted a study jointly with Bangalore Chamber of Commerce (BCIC) in April to emphasize on the immediate need for India Inc. to introduce and adopt a ""Digital Ethics framework"" that would ensure a holistic view of ethics, and govern every digital intervention in the transformation journey of a business.
With the EU recently releasing standards and considerations for ethics use of AI and several global organizations are also adopting standards for Open AI, Jain clearly sees a movement towards making Digital 'safe to use' and ensuring that treatment of data is within the acceptable limits from data subject's perspective.
However, with organizations often face challenges related to privacy, algorithmic bias, and a range of other technology-related ethical issues that leads to reputational and even financial risks.
Problem is, leaders seldom develop an overall approach to the ethical impacts of technology use and more so, companies fail to consider technology to be their core business, even as they increasingly rely on advanced digital and physical technologies to run their day-to-day operations.
""All these concerns have made it crucial to have an ethical framework that would ensure effective governance and risk mitigation aspects are in place,"" says Jain.
Jain states that businesses must address Digital Ethics at three levels: 1. Digital Ethics for the consumer 2. Digital Ethics as it applies to the provider of digital services and 3. Ethical treatment of the data generated by this interaction.
We will have to manage the ethics at all three levels. That's why Digital Ethics has to be managed at multiple levels, and it is prevalent across the organisations with integration with various functions.
Making Digital Ethics a Priority
The research suggests that companies that are more advanced digitally tend to be more focused on technology-related ethics than companies still early in their digital journey. But technological maturity alone doesn't drive ethical tech. These companies are also typically supported by leaders who understand the impacts of technology disruptors from a diverse and inclusive set of stakeholders, and foster an organizational culture of continuous learning, debate, transparency, and open dialogue.
For organizations to successfully introduce digital ethics, Jain recommends the following:
* Communicating the importance of digital ethics is the key, where the view must flow from top down and this is a culture driven by leadership, so as to lead by example
* Creation of a digital ethics committee or council to ensure that all digital projects are reviewed and thus they are enabled towards a long lasting impact on digital program is important
* Creating a policy that will help the organisation to imbibe the above practices into the processes and technical development is a significant step
* Increasing awareness on the subject and training delivered to stakeholders involved in fulfilling the value chain will bring the necessary adoption
The Future of Digital Ethics
Needless to say then digital ethics depends on leaders making it a priority, fostering the culture of ethics in everything they do and developing ethical decision-making processes.
Looking into the future of digital ethics, Jain believes while at present only a few companies are early movers to capture data across different dimensions and sources, we will see more organisations building their own digital ethic frameworks involving key functions like data privacy, ethics, human resources, technology, business, and risks.
""We will also see establishment of ethics committees and them playing an active role in the development of digital programs. More countries will establish standards for digital ethics which will be monitored both at the societal and organization level over the next few years,"" he mentions.
By embracing an ethical technology mindset, organizations can anticipate and respond to ethical challenges that emerge over time.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ARTIFICIAL INTELLIGENCE (90%); ETHICS (90%); MACHINE LEARNING (90%); ASSOCIATIONS & ORGANIZATIONS (89%); DISRUPTIVE INNOVATION (89%); EMERGING TECHNOLOGY (79%); BUSINESS OPERATIONS (78%); COMPANY ACTIVITIES & MANAGEMENT (78%); CORPORATE GOVERNANCE (78%); RISK MANAGEMENT (77%); HISTORY (76%); BUSINESS & PROFESSIONAL ASSOCIATIONS (73%); CHAMBERS OF COMMERCE (73%); GOVERNMENT & PUBLIC ADMINISTRATION (73%)
Company:  DELOITTE TOUCHE TOHMATSU (66%);  DELOITTE LLP (56%)
Industry: NAICS541211 OFFICES OF CERTIFIED PUBLIC ACCOUNTANTS (66%); SIC8721 ACCOUNTING, AUDITING, & BOOKKEEPING SERVICES (66%); ARTIFICIAL INTELLIGENCE (90%); MACHINE LEARNING (90%); BANKING & FINANCE (78%); INTERNET OF THINGS (78%); RISK MANAGEMENT (77%); BIG DATA (76%)
Geographic: BANGALORE, KARNATAKA, INDIA (56%); INDIA (90%)
Load-Date: July 18, 2023","Disruptive technologies offer tremendous opportunity for businesses to become smarter, more flexible, and more responsive. But often people fail to understand the ramifications of their usage, running into bigger risks. It is then that the question of digital ethics comes into the picture.
Let's say, a private bank decides to use machine learning to filter out who is eligible for a loan. The machine learning will need training on a data set, which could be historical data or user created data. If, historically, the bank has denied loans to a certain category of people, the same bias would carry forward to the machine. So essentially, they have transferred this bias to the system and now the system would deny loans to that certain category of people.
The above example is a classic case of digital ethics - a topic of immediate concern and deliberation not only for private organizations, but also for the government, regulatory bodies and end users, explains Vishal Jain, Partner at Deloitte India in a freewheeling chat with CXOToday.
The growing concerns of Digital Ethics
While digital ethics has been a topic of discussion ever since businesses started moving online, the pandemic has made it a greater priory. With more businesses embracing digital technologies like AI/ML, big data, cloud, IoT and more in a big way. The need of the hour is to take a relook at the business operations layered on digital touch points, thinking around ethical uses of technology.
Given the growing concerns around information and content available over digital platforms, Deloitte Touche Tohmatsu India LLP conducted a study jointly with Bangalore Chamber of Commerce (BCIC) in April to emphasize on the immediate need for India Inc. to introduce and adopt a ""Digital Ethics framework"" that would ensure a holistic view of ethics, and govern every digital intervention in the transformation journey of a business.
With the EU recently releasing standards and considerations for ethics use of AI and several global organizations are also adopting standards for Open AI, Jain clearly sees a movement towards making Digital 'safe to use' and ensuring that treatment of data is within the acceptable limits from data subject's perspective.
However, with organizations often face challenges related to privacy, algorithmic bias, and a range of other technology-related ethical issues that leads to reputational and even financial risks.
Problem is, leaders seldom develop an overall approach to the ethical impacts of technology use and more so, companies fail to consider technology to be their core business, even as they increasingly rely on advanced digital and physical technologies to run their day-to-day operations.
""All these concerns have made it crucial to have an ethical framework that would ensure effective governance and risk mitigation aspects are in place,"" says Jain.
Jain states that businesses must address Digital Ethics at three levels: 1. Digital Ethics for the consumer 2. Digital Ethics as it applies to the provider of digital services and 3. Ethical treatment of the data generated by this interaction.
We will have to manage the ethics at all three levels. That's why Digital Ethics has to be managed at multiple levels, and it is prevalent across the organisations with integration with various functions.
Making Digital Ethics a Priority
The research suggests that companies that are more advanced digitally tend to be more focused on technology-related ethics than companies still early in their digital journey. But technological maturity alone doesn't drive ethical tech. These companies are also typically supported by leaders who understand the impacts of technology disruptors from a diverse and inclusive set of stakeholders, and foster an organizational culture of continuous learning, debate, transparency, and open dialogue.
For organizations to successfully introduce digital ethics, Jain recommends the following:
* Communicating the importance of digital ethics is the key, where the view must flow from top down and this is a culture driven by leadership, so as to lead by example
* Creation of a digital ethics committee or council to ensure that all digital projects are reviewed and thus they are enabled towards a long lasting impact on digital program is important
* Creating a policy that will help the organisation to imbibe the above practices into the processes and technical development is a significant step
* Increasing awareness on the subject and training delivered to stakeholders involved in fulfilling the value chain will bring the necessary adoption
The Future of Digital Ethics
Needless to say then digital ethics depends on leaders making it a priority, fostering the culture of ethics in everything they do and developing ethical decision-making processes.
Looking into the future of digital ethics, Jain believes while at present only a few companies are early movers to capture data across different dimensions and sources, we will see more organisations building their own digital ethic frameworks involving key functions like data privacy, ethics, human resources, technology, business, and risks.
""We will also see establishment of ethics committees and them playing an active role in the development of digital programs. More countries will establish standards for digital ethics which will be monitored both at the societal and organization level over the next few years,"" he mentions.
By embracing an ethical technology mindset, organizations can anticipate and respond to ethical challenges that emerge over time.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ARTIFICIAL INTELLIGENCE (90%); ETHICS (90%); MACHINE LEARNING (90%); ASSOCIATIONS & ORGANIZATIONS (89%); DISRUPTIVE INNOVATION (89%); EMERGING TECHNOLOGY (79%); BUSINESS OPERATIONS (78%); COMPANY ACTIVITIES & MANAGEMENT (78%); CORPORATE GOVERNANCE (78%); RISK MANAGEMENT (77%); HISTORY (76%); BUSINESS & PROFESSIONAL ASSOCIATIONS (73%); CHAMBERS OF COMMERCE (73%); GOVERNMENT & PUBLIC ADMINISTRATION (73%)
Company:  DELOITTE TOUCHE TOHMATSU (66%);  DELOITTE LLP (56%)
Industry: NAICS541211 OFFICES OF CERTIFIED PUBLIC ACCOUNTANTS (66%); SIC8721 ACCOUNTING, AUDITING, & BOOKKEEPING SERVICES (66%); ARTIFICIAL INTELLIGENCE (90%); MACHINE LEARNING (90%); BANKING & FINANCE (78%); INTERNET OF THINGS (78%); RISK MANAGEMENT (77%); BIG DATA (76%)
Geographic: BANGALORE, KARNATAKA, INDIA (56%); INDIA (90%)
Load-Date: July 18, 2023",neutral,0.6777977347373962,balanced/neutral,"['privacy', 'bias', 'transparency']",[],"['policy', 'governance', 'standards', 'framework', 'must']",['machine learning'],3,0,5,1
2021,Unknown Title,"Body
Link to Image
Link to Story
Artificial intelligence (AI) was once the stuff of science fiction. But it's becoming widespread. It is used in mobile phone technology and motor vehicles . It powers tools for agriculture and healthcare .
But concerns have emerged about the accountability of AI and related technologies like machine learning. In December 2020 a computer scientist, Timnit Gebru, was fired from Google's Ethical AI team. She had previously raised the alarm about the social effects of bias in AI technologies. For instance, in a 2018 paper Gebru and another researcher, Joy Buolamwini, had showed how facial recognition software was less accurate in identifying women and people of colour than white men. Biases in training data can have far-reaching and unintended effects.
There is already a substantial body of research about ethics in AI. This highlights the importance of principles to ensure technologies do not simply worsen biases or even introduce new social harms. As the UNESCO draft recommendation on the ethics of AI states:
In recent years, many frameworks and guidelines have been created that identify objectives and priorities for ethical AI.
This is certainly a step in the right direction. But it's also critical to look beyond technical solutions when addressing issues of bias or inclusivity. Biases can enter at the level of who frames the objectives and balances the priorities.
In a recent paper , we argue that inclusivity and diversity also need to be at the level of identifying values and defining frameworks of what counts as ethical AI in the first place. This is especially pertinent when considering the growth of AI research and machine learning across the African continent.
ContextResearch and development of AI and machine learning technologies is growing in African countries. Programmes such as Data Science Africa , Data Science Nigeria , and the Deep Learning Indaba with its satellite IndabaX events , which have so far been held in 27 different African countries, illustrate the interest and human investment in the fields.
The potential of AI and related technologies to promote opportunities for growth, development and democratisation in Africa is a key driver of this research.
Yet very few African voices have so far been involved in the international ethical frameworks that aim to guide the research. This might not be a problem if the principles and values in those frameworks have universal application. But it's not clear that they do.
For instance, the European AI4People framework offers a synthesis of six other ethical frameworks. It identifies respect for autonomy as one of its key principles. This principle has been criticised within the applied ethical field of bioethics. It is seen as failing to do justice to the communitarian values common across Africa. These focus less on the individual and more on community, even requiring that exceptions are made to upholding such a principle to allow for effective interventions.
Challenges like these - or even acknowledgement that there could be such challenges - are largely absent from the discussions and frameworks for ethical AI.
Just like training data can entrench existing inequalities and injustices, so can failing to recognise the possibility of diverse sets of values that can vary across social, cultural and political contexts.
Unusable resultsIn addition, failing to take into account social, cultural and political contexts can mean that even a seemingly perfect ethical technical solution can be ineffective or misguided once implemented .
For machine learning to be effective at making useful predictions, any learning system needs access to training data. This involves samples of the data of interest: inputs in the form of multiple features or measurements, and outputs which are the labels scientists want to predict. In most cases, both these features and labels require human knowledge of the problem. But a failure to correctly account for the local context could result in underperforming systems.
For example, mobile phone call records have been used to estimate population sizes before and after disasters. However, vulnerable populations are less likely to have access to mobile devices. So, this kind of approach could yield results that aren't useful .
Similarly, computer vision technologies for identifying different kinds of structures in an area will likely underperform where different construction materials are used. In both of these cases, as we and other colleagues discuss in another recent paper , not accounting for regional differences may have profound effects on anything from the delivery of disaster aid, to the performance of autonomous systems.
Going forwardAI technologies must not simply worsen or incorporate the problematic aspects of current human societies.
Being sensitive to and inclusive of different contexts is vital for designing effective technical solutions. It is equally important not to assume that values are universal. Those developing AI need to start including people of different backgrounds: not just in the technical aspects of designing data sets and the like but also in defining the values that can be called upon to frame and set objectives and priorities.
MENAFN23112021000199003603ID1103236806
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ARTIFICIAL INTELLIGENCE ETHICS (91%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); DATA SCIENCE (89%); DEEP LEARNING (89%); MACHINE LEARNING (89%); COMPUTER SCIENCE (78%); IDENTIFICATION TECHNOLOGIES (78%); RACE & ETHNICITY (77%); NEGATIVE PERSONAL NEWS (76%); DIVERSITY & INCLUSION (75%); BIOETHICS (72%); BIOMETRICS (70%)
Company: GOOGLE LLC (84%)
Industry: NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (84%); ARTIFICIAL INTELLIGENCE ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); DATA SCIENCE (89%); DEEP LEARNING (89%); MACHINE LEARNING (89%); COMPUTER SCIENCE (78%); COMPUTER SOFTWARE (76%); PATTERN RECOGNITION (73%); MOBILE & CELLULAR TELEPHONES (72%)
Geographic: AFRICA (94%); NIGERIA (78%)
Load-Date: November 23, 2021","Link to Image
Link to Story
Artificial intelligence (AI) was once the stuff of science fiction. But it's becoming widespread. It is used in mobile phone technology and motor vehicles . It powers tools for agriculture and healthcare .
But concerns have emerged about the accountability of AI and related technologies like machine learning. In December 2020 a computer scientist, Timnit Gebru, was fired from Google's Ethical AI team. She had previously raised the alarm about the social effects of bias in AI technologies. For instance, in a 2018 paper Gebru and another researcher, Joy Buolamwini, had showed how facial recognition software was less accurate in identifying women and people of colour than white men. Biases in training data can have far-reaching and unintended effects.
There is already a substantial body of research about ethics in AI. This highlights the importance of principles to ensure technologies do not simply worsen biases or even introduce new social harms. As the UNESCO draft recommendation on the ethics of AI states:
In recent years, many frameworks and guidelines have been created that identify objectives and priorities for ethical AI.
This is certainly a step in the right direction. But it's also critical to look beyond technical solutions when addressing issues of bias or inclusivity. Biases can enter at the level of who frames the objectives and balances the priorities.
In a recent paper , we argue that inclusivity and diversity also need to be at the level of identifying values and defining frameworks of what counts as ethical AI in the first place. This is especially pertinent when considering the growth of AI research and machine learning across the African continent.
ContextResearch and development of AI and machine learning technologies is growing in African countries. Programmes such as Data Science Africa , Data Science Nigeria , and the Deep Learning Indaba with its satellite IndabaX events , which have so far been held in 27 different African countries, illustrate the interest and human investment in the fields.
The potential of AI and related technologies to promote opportunities for growth, development and democratisation in Africa is a key driver of this research.
Yet very few African voices have so far been involved in the international ethical frameworks that aim to guide the research. This might not be a problem if the principles and values in those frameworks have universal application. But it's not clear that they do.
For instance, the European AI4People framework offers a synthesis of six other ethical frameworks. It identifies respect for autonomy as one of its key principles. This principle has been criticised within the applied ethical field of bioethics. It is seen as failing to do justice to the communitarian values common across Africa. These focus less on the individual and more on community, even requiring that exceptions are made to upholding such a principle to allow for effective interventions.
Challenges like these - or even acknowledgement that there could be such challenges - are largely absent from the discussions and frameworks for ethical AI.
Just like training data can entrench existing inequalities and injustices, so can failing to recognise the possibility of diverse sets of values that can vary across social, cultural and political contexts.
Unusable resultsIn addition, failing to take into account social, cultural and political contexts can mean that even a seemingly perfect ethical technical solution can be ineffective or misguided once implemented .
For machine learning to be effective at making useful predictions, any learning system needs access to training data. This involves samples of the data of interest: inputs in the form of multiple features or measurements, and outputs which are the labels scientists want to predict. In most cases, both these features and labels require human knowledge of the problem. But a failure to correctly account for the local context could result in underperforming systems.
For example, mobile phone call records have been used to estimate population sizes before and after disasters. However, vulnerable populations are less likely to have access to mobile devices. So, this kind of approach could yield results that aren't useful .
Similarly, computer vision technologies for identifying different kinds of structures in an area will likely underperform where different construction materials are used. In both of these cases, as we and other colleagues discuss in another recent paper , not accounting for regional differences may have profound effects on anything from the delivery of disaster aid, to the performance of autonomous systems.
Going forwardAI technologies must not simply worsen or incorporate the problematic aspects of current human societies.
Being sensitive to and inclusive of different contexts is vital for designing effective technical solutions. It is equally important not to assume that values are universal. Those developing AI need to start including people of different backgrounds: not just in the technical aspects of designing data sets and the like but also in defining the values that can be called upon to frame and set objectives and priorities.
MENAFN23112021000199003603ID1103236806
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ARTIFICIAL INTELLIGENCE ETHICS (91%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); DATA SCIENCE (89%); DEEP LEARNING (89%); MACHINE LEARNING (89%); COMPUTER SCIENCE (78%); IDENTIFICATION TECHNOLOGIES (78%); RACE & ETHNICITY (77%); NEGATIVE PERSONAL NEWS (76%); DIVERSITY & INCLUSION (75%); BIOETHICS (72%); BIOMETRICS (70%)
Company: GOOGLE LLC (84%)
Industry: NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (84%); ARTIFICIAL INTELLIGENCE ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); DATA SCIENCE (89%); DEEP LEARNING (89%); MACHINE LEARNING (89%); COMPUTER SCIENCE (78%); COMPUTER SOFTWARE (76%); PATTERN RECOGNITION (73%); MOBILE & CELLULAR TELEPHONES (72%)
Geographic: AFRICA (94%); NIGERIA (78%)
Load-Date: November 23, 2021",neutral,0.7703796029090881,balanced/neutral,"['bias', 'accountability', 'autonomy', 'inclusivity', 'access']","['justice', 'autonomy', 'justice']","['guidelines', 'framework', 'must', 'need to']","['machine learning', 'deep learning', 'computer vision', 'facial recognition']",5,3,4,4
2021,Unknown Title,"Byline: Kyle Wiggers
Body
Aug 13, 2021( VentureBeat: http://venturebeat.com/ Delivered by Newstex) ; 
 Secrets to driving subscriber growth The most efficient channels, pricing and packaging, even strategies for cancellations -- find out how the pros do it. 
 Register now[1]  
All the sessions from Transform 2021 are available on-demand now. Watch now.[2] 
 As new principles emerge to guide the development ethical, safe, and inclusive AI[3], the industry faces self-inflicted challenges. Increasingly, there are many sets of guidelines — the Organization for Economic Cooperation and Development's AI repository alone hosts more than 100 documents — that are vague and high-level. And while a number of tools are available, most come without actionable guidance on how to use, customize, and troubleshoot them. 
 This is cause for alarm, because as the coauthors of a recent paper[4] write, AI's impacts are hard to assess — especially when they have second- and third-order effects. Ethics discussions tend to focus on futuristic scenarios that may not come to pass and unrealistic generalizations that make the conversations untenable. In particular, companies run the risk of engaging in 'ethics shopping,' 'ethics washing,' or 'ethics shirking,' in which they ameliorate their position with customers to build trust while minimizing accountability. 
 The points are salient in light of efforts by European Commission's High-level Expert Group on AI (HLEG) and the U.S. National Institute of Standards and Technology, among others, to create standards for building 'trustworthy AI.' In a paper, digital ethics researcher Mark Ryan argues that AI isn't the type of thing that has the capacity to be trustworthy because the category of 'trust' simply doesn't apply to AI. In fact, AI can't have the capacity to be trusted as long as it can't be held responsible for its actions, he argues. 
 'Trust is separate from risk analysis that is solely based on predictions based on past behavior,' he explains[5]. 'While reliability and past experience may be used to develop, confer, or reject trust placed in the trustee, it is not the sole or defining characteristic of trust. Though we may trust people that we rely on, it is not presupposed that we do.' 
Responsible adoption 
Productizing AI responsibly means different things to different companies. For some, 'responsible' implies adopting AI in a manner that's ethical, transparent, and accountable. For others, it means ensuring that their use of AI remains consistent with laws, regulations, norms, customer expectations, and organizational values. In any case, 'responsible AI' promises to guard against the use of biased data or algorithms, providing an assurance that automated decisions are justified and explainable — at least in theory. 
 Recognizing this, organizations must overcome a misalignment of incentives, disciplinary divides, distributions of responsibilities, and other blockers in responsibly adopting AI. It requires an impact assessment framework that's not only broad, flexible, iterative, possible to operationalize, and guided, but highly participatory as well, according to the paper's coauthors. They emphasize the need to shy away from anticipating impacts that are assumed to be important and become more deliberate in deployment choices. As a way of normalizing the practice, the coauthors advocate for including these ideas in documentation the same way that topics like privacy and bias are currently covered. 
 Another paper[6] — this from researchers at the Data &#38; Society Research Institute and Princeton — posits 'algorithmic impact assessments' as a tool to help AI designers analyze the benefits and potential pitfalls of algorithmic systems. Impact assessments can address the issues of transparency, fairness, and accountability by providing guardrails and accountability forums that can compel developers to make changes to AI systems. 
 This is easier said than done, of course. Algorithmic impact assessments focus on the effects of AI decision-making, which doesn't necessarily measure harms and may even obscure them — real harms can be difficult to quantify. But if the assessments are implemented with accountability measures, they can perhaps foster technology that respects — rather than erodes — dignity. 
 As Montreal AI ethics researcher Abhishek Gupta recently wrote in a column[7]: 'Design decisions for AI systems involve value judgements and optimization choices. Some relate to technical considerations like latency and accuracy, others relate to business metrics. But each require careful consideration as they have consequences in the final outcome from the system. To be clear, not everything has to translate into a tradeoff. There are often smart reformulations of a problem so that you can meet the needs of your users and customers while also satisfying internal business considerations.' 
 For AI coverage, send news tips toKyle Wiggers[8] — and be sure to subscribe to the AI Weekly newsletter[9]and bookmark our AI channel,The Machine[10]. 
 Thanks for reading, 
 Kyle Wiggers 
 AI Staff Writer VentureBeat VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative technology and transact. 
Our site delivers essential information on data technologies and strategies to guide you as you lead your organizations. We invite you to become a member of our community, to access: 
 up-to-date information on the subjects of interest to you our newsletters gated thought-leader content and discounted access to our prized events, such as Transform 2021: Learn More[11] networking features, and more 
Become a member[12] 
 [ 1]: https://www.brighttalk.com/webcast/12339/498837?utm_source=vb&#38;utm_medium=aug-11-sidebar&#38;utm_content=text-link&#38;utm_campaign=aug-24-recurly-webinar [ 2]: https://venturebeat.com/event/transform-2021/on-demand/#primary [ 3]: https://venturebeat.com/2021/07/30/how-to-approach-ai-more-responsibly-according-to-a-top-ai-ethicist/ [ 4]: https://ieeexplore.ieee.org/document/9445790 [ 5]: https://montrealethics.ai/in-ai-we-trust-ethics-artificial-intelligence-and-reliability/ [ 6]: https://dl.acm.org/doi/pdf/10.1145/3442188.3445935 [ 7]: https://arxiv.org/pdf/2108.03929.pdf [ 8]: kyle.wiggers@venturebeat.com  [ 9]: https://venturebeat.com/newsletters/ [ 10]: https://venturebeat.com/category/ai/ [ 11]: https://events.venturebeat.com/transform2021/ [ 12]: https://venturebeat.com/venturebeat-membership-plans/ 
Classification
Language: English
Publication-Type: Web Blog
Journal Code: VNTR-6695
Subject: ETHICS (92%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ARTIFICIAL INTELLIGENCE (89%); PRICES (89%); ECONOMIC DEVELOPMENT (76%); ASSOCIATIONS & ORGANIZATIONS (75%); WRITERS (73%); INTERNATIONAL ECONOMIC ORGANIZATIONS (72%); ECONOMIC POLICY (70%); TRUST ARRANGEMENTS (70%); STANDARDS & MEASUREMENTS (64%); AI ethics (%); newsletter (%); Big Data (%); AI (%); Dev (%); machine learning (%); trustworthy AI (%); AI Weekly (%); Enterprise (%); responsible AI (%); ethics (%); category-/People & Society (%); VB Home Page (%); artificial intelligence (%)
Organization: ORGANISATION FOR ECONOMIC CO-OPERATION & DEVELOPMENT (57%); EUROPEAN COMMISSION (54%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (90%); ARTIFICIAL INTELLIGENCE (89%); WRITERS (73%)
Geographic: EUROPE (72%)
Load-Date: August 13, 2021","Aug 13, 2021( VentureBeat: http://venturebeat.com/ Delivered by Newstex) ; 
 Secrets to driving subscriber growth The most efficient channels, pricing and packaging, even strategies for cancellations -- find out how the pros do it. 
 Register now[1]  
All the sessions from Transform 2021 are available on-demand now. Watch now.[2] 
 As new principles emerge to guide the development ethical, safe, and inclusive AI[3], the industry faces self-inflicted challenges. Increasingly, there are many sets of guidelines — the Organization for Economic Cooperation and Development's AI repository alone hosts more than 100 documents — that are vague and high-level. And while a number of tools are available, most come without actionable guidance on how to use, customize, and troubleshoot them. 
 This is cause for alarm, because as the coauthors of a recent paper[4] write, AI's impacts are hard to assess — especially when they have second- and third-order effects. Ethics discussions tend to focus on futuristic scenarios that may not come to pass and unrealistic generalizations that make the conversations untenable. In particular, companies run the risk of engaging in 'ethics shopping,' 'ethics washing,' or 'ethics shirking,' in which they ameliorate their position with customers to build trust while minimizing accountability. 
 The points are salient in light of efforts by European Commission's High-level Expert Group on AI (HLEG) and the U.S. National Institute of Standards and Technology, among others, to create standards for building 'trustworthy AI.' In a paper, digital ethics researcher Mark Ryan argues that AI isn't the type of thing that has the capacity to be trustworthy because the category of 'trust' simply doesn't apply to AI. In fact, AI can't have the capacity to be trusted as long as it can't be held responsible for its actions, he argues. 
 'Trust is separate from risk analysis that is solely based on predictions based on past behavior,' he explains[5]. 'While reliability and past experience may be used to develop, confer, or reject trust placed in the trustee, it is not the sole or defining characteristic of trust. Though we may trust people that we rely on, it is not presupposed that we do.' 
Responsible adoption 
Productizing AI responsibly means different things to different companies. For some, 'responsible' implies adopting AI in a manner that's ethical, transparent, and accountable. For others, it means ensuring that their use of AI remains consistent with laws, regulations, norms, customer expectations, and organizational values. In any case, 'responsible AI' promises to guard against the use of biased data or algorithms, providing an assurance that automated decisions are justified and explainable — at least in theory. 
 Recognizing this, organizations must overcome a misalignment of incentives, disciplinary divides, distributions of responsibilities, and other blockers in responsibly adopting AI. It requires an impact assessment framework that's not only broad, flexible, iterative, possible to operationalize, and guided, but highly participatory as well, according to the paper's coauthors. They emphasize the need to shy away from anticipating impacts that are assumed to be important and become more deliberate in deployment choices. As a way of normalizing the practice, the coauthors advocate for including these ideas in documentation the same way that topics like privacy and bias are currently covered. 
 Another paper[6] — this from researchers at the Data &#38; Society Research Institute and Princeton — posits 'algorithmic impact assessments' as a tool to help AI designers analyze the benefits and potential pitfalls of algorithmic systems. Impact assessments can address the issues of transparency, fairness, and accountability by providing guardrails and accountability forums that can compel developers to make changes to AI systems. 
 This is easier said than done, of course. Algorithmic impact assessments focus on the effects of AI decision-making, which doesn't necessarily measure harms and may even obscure them — real harms can be difficult to quantify. But if the assessments are implemented with accountability measures, they can perhaps foster technology that respects — rather than erodes — dignity. 
 As Montreal AI ethics researcher Abhishek Gupta recently wrote in a column[7]: 'Design decisions for AI systems involve value judgements and optimization choices. Some relate to technical considerations like latency and accuracy, others relate to business metrics. But each require careful consideration as they have consequences in the final outcome from the system. To be clear, not everything has to translate into a tradeoff. There are often smart reformulations of a problem so that you can meet the needs of your users and customers while also satisfying internal business considerations.' 
 For AI coverage, send news tips toKyle Wiggers[8] — and be sure to subscribe to the AI Weekly newsletter[9]and bookmark our AI channel,The Machine[10]. 
 Thanks for reading, 
 Kyle Wiggers 
 AI Staff Writer VentureBeat VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative technology and transact. 
Our site delivers essential information on data technologies and strategies to guide you as you lead your organizations. We invite you to become a member of our community, to access: 
 up-to-date information on the subjects of interest to you our newsletters gated thought-leader content and discounted access to our prized events, such as Transform 2021: Learn More[11] networking features, and more 
Become a member[12] 
 [ 1]: https://www.brighttalk.com/webcast/12339/498837?utm_source=vb&#38;utm_medium=aug-11-sidebar&#38;utm_content=text-link&#38;utm_campaign=aug-24-recurly-webinar [ 2]: https://venturebeat.com/event/transform-2021/on-demand/#primary [ 3]: https://venturebeat.com/2021/07/30/how-to-approach-ai-more-responsibly-according-to-a-top-ai-ethicist/ [ 4]: https://ieeexplore.ieee.org/document/9445790 [ 5]: https://montrealethics.ai/in-ai-we-trust-ethics-artificial-intelligence-and-reliability/ [ 6]: https://dl.acm.org/doi/pdf/10.1145/3442188.3445935 [ 7]: https://arxiv.org/pdf/2108.03929.pdf [ 8]: kyle.wiggers@venturebeat.com  [ 9]: https://venturebeat.com/newsletters/ [ 10]: https://venturebeat.com/category/ai/ [ 11]: https://events.venturebeat.com/transform2021/ [ 12]: https://venturebeat.com/venturebeat-membership-plans/ 
Classification
Language: English
Publication-Type: Web Blog
Journal Code: VNTR-6695
Subject: ETHICS (92%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ARTIFICIAL INTELLIGENCE (89%); PRICES (89%); ECONOMIC DEVELOPMENT (76%); ASSOCIATIONS & ORGANIZATIONS (75%); WRITERS (73%); INTERNATIONAL ECONOMIC ORGANIZATIONS (72%); ECONOMIC POLICY (70%); TRUST ARRANGEMENTS (70%); STANDARDS & MEASUREMENTS (64%); AI ethics (%); newsletter (%); Big Data (%); AI (%); Dev (%); machine learning (%); trustworthy AI (%); AI Weekly (%); Enterprise (%); responsible AI (%); ethics (%); category-/People & Society (%); VB Home Page (%); artificial intelligence (%)
Organization: ORGANISATION FOR ECONOMIC CO-OPERATION & DEVELOPMENT (57%); EUROPEAN COMMISSION (54%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (90%); ARTIFICIAL INTELLIGENCE (89%); WRITERS (73%)
Geographic: EUROPE (72%)
Load-Date: August 13, 2021",neutral,0.8326565027236938,balanced/neutral,"['privacy', 'bias', 'fairness', 'transparency', 'accountability', 'access']","['fairness', 'dignity']","['policy', 'standards', 'guidelines', 'framework', 'must', 'need to', 'advocate']",['machine learning'],6,2,7,1
2021,Unknown Title,"Byline: Will Griffin, Hypergiant Industries
Body
Aug 28, 2021( VentureBeat: http://venturebeat.com/ Delivered by Newstex)  
 How open API standards will transform financial services Open standards will have a huge impact on driving innovation in banking. Learn the status in the U.S. - and the bold new opportunities open standards are set to usher in. 
 Register here[1]  
The Transform Technology Summits start October 13th with Low-Code/No Code: Enabling Enterprise Agility. Register now![2] 
 Earlier this summer, the National Artificial Intelligence Research Resource (NAIRR) Task Force released a request[3] for information (RFI) on how to build an implementation roadmap for a shared AI research infrastructure. Along with requests for ideas on how to own and implement this agenda, it requested guidance on how to best ensure that privacy, civil liberties, and civil rights are protected going forward. To accomplish this objective, values-based ethical reasoning education and training resources must be at the core of the Task Force's strategy. 
What's at stake 
Congress's passage of the ​​National Defense Authorization Act for Fiscal 2021 directing the Biden White House to create the NAIRR Task Force could be as consequential to America's democratic ideals as numerous wars, policy, and civil rights movements in our past. 
 While the early public announcements of the NAIRR Task Force do not refer explicitly to foreign governments, make no mistake that geopolitical competition with China, Russia, and other nation-states looms large in the urgency of its mission. 
 Not since the Manhattan Project and the race to develop the atomic bomb has a technology been as important in its potential to reshape the balance of power between Western democracy and what Stanford's Institute for Human-Centered Artificial Intelligence calls 'digital authoritarianism[4].' Similar to the nuclear arms race, the path the United States takes in developing and deploying this technology will determine the ambit of freedom and quality of life for billions of people on Earth. The stakes are that high. 
The precedents are clear 
 While the stated delivery date for the NAIRR Task Force's report and roadmap is not until November 2022, it is important to keep in mind that ensuring ethics in AI[5] that uphold America's values is a long process, and one extremely core to the American identity. Yet, the precedent for an ethical, inclusive roadmap is written in our history, and we can look to the military, medical, and legal professions for examples of how to do so successfully. 
 The military. On July 26, 1948, President Harry Truman issued Executive Order 9981 to initiate the desegregation of the military. This led to the establishment of the President's Committee on Equality of Treatment and Opportunity in the Armed Services [6]and one of the most consequential ethics and values reports in United States history. But it's worth noting that it wasn't until January 2, 2021, that retired 4-Star General Lloyd Austin III was appointed to be the first Black Secretary of Defense. Embedding ethics and American values in the AI-related disciplines will require the same sustained and unrelenting effort. 
 The medical field. The Code of Medical Ethics of the American Medical Association (AMA) is considered the gold standard in ethics and values in a professional discipline, dating all the way back to fifth century BCE and the ideals of Greek physician Hippocrates to 'relieve suffering and promote well-being in a relationship of fidelity with the patient.' Despite this deep and rich history of ethics at the core of medicine, it took until 1977 for Johns Hopkins to become the first medical school in the nation to implement a required course on medical ethics in its core curriculum. 
 The law. Bar associations began to introduce ethical codes for attorneys and judges in the United States in the early 1800s, but it was not until the early twentieth century and the widespread adoption of the Harvard Method[7] in law schools that legal ethics were tied to professional responsibility and a clear set of moral duties to society were embedded into legal education and the profession. 
The road ahead 
The AI-related disciplines (computer science, engineering, and design) are far behind other professions in ethics requirements, education, and training. There are, however, dozens of promising tech ethics-driven organizations and initiatives working to promote and unify ethical reasoning education and training in AI. 
 Higher education. Embedding ethics and values training into the core curriculum of every college-trained engineer, designer, and computer scientist must be a core goal of any national AI strategy. 
 To this end, The Markkula Center for Applied Ethics[8] at the University of Santa Clara is one of the most prolific producers of actionable technology ethics curricula, case-studies, and decision-making training for students and practitioners. Likewise, MIT has started to develop an AI-specific ethics curriculum[9] for this purpose and should also be consulted during the implementation planning process. Moreover, ethics in AI institutes[10] are being created all over the world and represent fertile ground for resources to be added to the NAIRR Task Force. 
 While most of these efforts focus on higher-education and current professionals, the Task Force also has an opportunity to begin sharing values and ethics resources to the large STEM-focused high school programs emerging across the country. The Committee on STEM Education of the National Science and Technology Council has highlighted[11] the need for more ethics education at all levels of STEM education, and the NAIRR Task Force has the opportunity to distribute and unify those resources. 
 Public-private partnerships and consortiums. Leading public-private and professional organizations are building best-in-class offerings that train AI professionals on methods to build ethically sound AI. Consulting these outside groups will be essential as the NAIRR Task Force forges ahead with its national AI strategy. 
 For example, The World Economic Forum (WEF)'s Shaping The Future of Technology Governance: Artificial Intelligence and Machine Learning Platform[12] is having a significant impact on governments and corporations around the world through consulting, publicly available research, white papers, ethical toolkits, and case studies. These products can help accelerate the benefits and mitigate the risks of artificial intelligence and machine learning. 
 Similarly, the Responsible AI Institute[13] (RAI) has created the first independent, accredited certification program for Responsible AI. In fact, RAI has already been tapped by the United States Department of Defense Joint Artificial Intelligence Command (JAIC) to embed ethical and values-driven responsible AI guardrails [14]into procurement practices. 
 Looking ahead, it will take us years to embed ethics and values into AI-related professional disciplines, but it is possible. As the NAIRR Task Force institutes its roadmap, the team must reference our history, provide resources for ethical training in university settings, scale that training to high school STEM programs, and work with professional organizations to instill best in class materials to upskill those currently in the industry. If we are going to win the AI innovation race while preserving our democratic principles, we must start here and we must start now. 
 Will Griffin is Chief Ethics Officer of Hypergiant[15], an enterprise AI company based in Austin, Texas. He received the 2020 IEEE Award for Distinguished Ethical Practices and created Hypergiant's Top of Mind Ethics (TOME) framework, which won the Communitas Award for Excellence in AI Ethics. 
VentureBeat VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative technology and transact. 
Our site delivers essential information on data technologies and strategies to guide you as you lead your organizations. We invite you to become a member of our community, to access: 
 up-to-date information on the subjects of interest to you our newsletters gated thought-leader content and discounted access to our prized events, such as Transform 2021: Learn More[16] networking features, and more 
Become a member[17] 
 [ 1]: https://www.brighttalk.com/webcast/12339/501350?utm_source=vb&#38;utm_medium=aug-16-sidebar&#38;utm_content=text-link&#38;utm_campaign=aug-31-yodlee-webinar [ 2]: https://venturebeat.com/event/the-low-code-no-code-summit/register/ [ 3]: https://www.federalregister.gov/documents/2021/07/23/2021-15660/request-for-information-rfi-on-an-implementation-plan-for-a-national-artificial-intelligence [ 4]: https://hai.stanford.edu/news/ai-report-competition-grows-between-china-and-us [ 5]: https://venturebeat.com/2021/01/22/center-for-applied-data-ethics-suggests-treating-ai-like-a-bureaucracy/#:~:text=And%20VentureBeat%20has%20written%20extensively%20about%20the%20fact,framework%20to%20close%20AI%20accountability%20gaps%20within%20organizations. [ 6]: https://www.trumanlibrary.gov/library/freedom-to-serve [ 7]: https://scholarship.law.wm.edu/cgi/viewcontent.cgi?article=2041&#38;context=facpubs [ 8]: https://www.scu.edu/ethics/focus-areas/technology-ethics/ [ 9]: https://www.media.mit.edu/projects/ai-ethics-for-middle-school/overview/ [ 10]: https://thegoodai.co/2021/04/02/14-research-institutes-paving-the-way-for-a-responsible-use-of-ai-for-good/ [ 11]: https://www.energy.gov/sites/default/files/2019/05/f62/STEM-Education-Strategic-Plan-2018.pdf [ 12]: https://www.weforum.org/platforms/shaping-the-future-of-technology-governance-artificial-intelligence-and-machine-learning [ 13]: https://venturebeat.com/2021/05/14/ai-weekly-how-to-implement-ai-responsibly/ [ 14]: https://www.prnewswire.com/news-releases/joint-artificial-intelligence-center-to-pilot-a-responsible-ai-procurement-process-301343009.html [ 15]: https://www.hypergiant.com/ [ 16]: https://events.venturebeat.com/transform2021/ [ 17]: https://venturebeat.com/venturebeat-membership-plans/ 
Classification
Language: English
Publication-Type: Web Blog
Journal Code: VNTR-6695
Subject: ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); CIVIL RIGHTS (90%); DEMOCRACIES (90%); ETHICS (90%); STANDARDS & MEASUREMENTS (90%); TYPES OF GOVERNMENT (90%); EDUCATION & TRAINING (78%); POLITICAL & SOCIAL IDEOLOGIES (78%); SCHOOL DESEGREGATION (78%); NUCLEAR WEAPONS (77%); US PRESIDENTS (77%); ARMS RACE (71%); DEFENSE SPENDING (71%); MILITARY WEAPONS (71%); WEAPONS & ARMS (70%); BOMBS & EXPLOSIVE DEVICES (65%); EXECUTIVE ORDERS (62%); PROFESSIONAL WORKERS (60%); AI (%); category-/News (%); ethical AI (%); VB Home Page (%)
Industry: ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); BANKING & FINANCE (90%); NUCLEAR WEAPONS (77%); DEFENSE SPENDING (71%); MILITARY WEAPONS (71%)
Geographic: UNITED STATES (96%); CHINA (79%)
Load-Date: August 28, 2021","Aug 28, 2021( VentureBeat: http://venturebeat.com/ Delivered by Newstex)  
 How open API standards will transform financial services Open standards will have a huge impact on driving innovation in banking. Learn the status in the U.S. - and the bold new opportunities open standards are set to usher in. 
 Register here[1]  
The Transform Technology Summits start October 13th with Low-Code/No Code: Enabling Enterprise Agility. Register now![2] 
 Earlier this summer, the National Artificial Intelligence Research Resource (NAIRR) Task Force released a request[3] for information (RFI) on how to build an implementation roadmap for a shared AI research infrastructure. Along with requests for ideas on how to own and implement this agenda, it requested guidance on how to best ensure that privacy, civil liberties, and civil rights are protected going forward. To accomplish this objective, values-based ethical reasoning education and training resources must be at the core of the Task Force's strategy. 
What's at stake 
Congress's passage of the ​​National Defense Authorization Act for Fiscal 2021 directing the Biden White House to create the NAIRR Task Force could be as consequential to America's democratic ideals as numerous wars, policy, and civil rights movements in our past. 
 While the early public announcements of the NAIRR Task Force do not refer explicitly to foreign governments, make no mistake that geopolitical competition with China, Russia, and other nation-states looms large in the urgency of its mission. 
 Not since the Manhattan Project and the race to develop the atomic bomb has a technology been as important in its potential to reshape the balance of power between Western democracy and what Stanford's Institute for Human-Centered Artificial Intelligence calls 'digital authoritarianism[4].' Similar to the nuclear arms race, the path the United States takes in developing and deploying this technology will determine the ambit of freedom and quality of life for billions of people on Earth. The stakes are that high. 
The precedents are clear 
 While the stated delivery date for the NAIRR Task Force's report and roadmap is not until November 2022, it is important to keep in mind that ensuring ethics in AI[5] that uphold America's values is a long process, and one extremely core to the American identity. Yet, the precedent for an ethical, inclusive roadmap is written in our history, and we can look to the military, medical, and legal professions for examples of how to do so successfully. 
 The military. On July 26, 1948, President Harry Truman issued Executive Order 9981 to initiate the desegregation of the military. This led to the establishment of the President's Committee on Equality of Treatment and Opportunity in the Armed Services [6]and one of the most consequential ethics and values reports in United States history. But it's worth noting that it wasn't until January 2, 2021, that retired 4-Star General Lloyd Austin III was appointed to be the first Black Secretary of Defense. Embedding ethics and American values in the AI-related disciplines will require the same sustained and unrelenting effort. 
 The medical field. The Code of Medical Ethics of the American Medical Association (AMA) is considered the gold standard in ethics and values in a professional discipline, dating all the way back to fifth century BCE and the ideals of Greek physician Hippocrates to 'relieve suffering and promote well-being in a relationship of fidelity with the patient.' Despite this deep and rich history of ethics at the core of medicine, it took until 1977 for Johns Hopkins to become the first medical school in the nation to implement a required course on medical ethics in its core curriculum. 
 The law. Bar associations began to introduce ethical codes for attorneys and judges in the United States in the early 1800s, but it was not until the early twentieth century and the widespread adoption of the Harvard Method[7] in law schools that legal ethics were tied to professional responsibility and a clear set of moral duties to society were embedded into legal education and the profession. 
The road ahead 
The AI-related disciplines (computer science, engineering, and design) are far behind other professions in ethics requirements, education, and training. There are, however, dozens of promising tech ethics-driven organizations and initiatives working to promote and unify ethical reasoning education and training in AI. 
 Higher education. Embedding ethics and values training into the core curriculum of every college-trained engineer, designer, and computer scientist must be a core goal of any national AI strategy. 
 To this end, The Markkula Center for Applied Ethics[8] at the University of Santa Clara is one of the most prolific producers of actionable technology ethics curricula, case-studies, and decision-making training for students and practitioners. Likewise, MIT has started to develop an AI-specific ethics curriculum[9] for this purpose and should also be consulted during the implementation planning process. Moreover, ethics in AI institutes[10] are being created all over the world and represent fertile ground for resources to be added to the NAIRR Task Force. 
 While most of these efforts focus on higher-education and current professionals, the Task Force also has an opportunity to begin sharing values and ethics resources to the large STEM-focused high school programs emerging across the country. The Committee on STEM Education of the National Science and Technology Council has highlighted[11] the need for more ethics education at all levels of STEM education, and the NAIRR Task Force has the opportunity to distribute and unify those resources. 
 Public-private partnerships and consortiums. Leading public-private and professional organizations are building best-in-class offerings that train AI professionals on methods to build ethically sound AI. Consulting these outside groups will be essential as the NAIRR Task Force forges ahead with its national AI strategy. 
 For example, The World Economic Forum (WEF)'s Shaping The Future of Technology Governance: Artificial Intelligence and Machine Learning Platform[12] is having a significant impact on governments and corporations around the world through consulting, publicly available research, white papers, ethical toolkits, and case studies. These products can help accelerate the benefits and mitigate the risks of artificial intelligence and machine learning. 
 Similarly, the Responsible AI Institute[13] (RAI) has created the first independent, accredited certification program for Responsible AI. In fact, RAI has already been tapped by the United States Department of Defense Joint Artificial Intelligence Command (JAIC) to embed ethical and values-driven responsible AI guardrails [14]into procurement practices. 
 Looking ahead, it will take us years to embed ethics and values into AI-related professional disciplines, but it is possible. As the NAIRR Task Force institutes its roadmap, the team must reference our history, provide resources for ethical training in university settings, scale that training to high school STEM programs, and work with professional organizations to instill best in class materials to upskill those currently in the industry. If we are going to win the AI innovation race while preserving our democratic principles, we must start here and we must start now. 
 Will Griffin is Chief Ethics Officer of Hypergiant[15], an enterprise AI company based in Austin, Texas. He received the 2020 IEEE Award for Distinguished Ethical Practices and created Hypergiant's Top of Mind Ethics (TOME) framework, which won the Communitas Award for Excellence in AI Ethics. 
VentureBeat VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative technology and transact. 
Our site delivers essential information on data technologies and strategies to guide you as you lead your organizations. We invite you to become a member of our community, to access: 
 up-to-date information on the subjects of interest to you our newsletters gated thought-leader content and discounted access to our prized events, such as Transform 2021: Learn More[16] networking features, and more 
Become a member[17] 
 [ 1]: https://www.brighttalk.com/webcast/12339/501350?utm_source=vb&#38;utm_medium=aug-16-sidebar&#38;utm_content=text-link&#38;utm_campaign=aug-31-yodlee-webinar [ 2]: https://venturebeat.com/event/the-low-code-no-code-summit/register/ [ 3]: https://www.federalregister.gov/documents/2021/07/23/2021-15660/request-for-information-rfi-on-an-implementation-plan-for-a-national-artificial-intelligence [ 4]: https://hai.stanford.edu/news/ai-report-competition-grows-between-china-and-us [ 5]: https://venturebeat.com/2021/01/22/center-for-applied-data-ethics-suggests-treating-ai-like-a-bureaucracy/#:~:text=And%20VentureBeat%20has%20written%20extensively%20about%20the%20fact,framework%20to%20close%20AI%20accountability%20gaps%20within%20organizations. [ 6]: https://www.trumanlibrary.gov/library/freedom-to-serve [ 7]: https://scholarship.law.wm.edu/cgi/viewcontent.cgi?article=2041&#38;context=facpubs [ 8]: https://www.scu.edu/ethics/focus-areas/technology-ethics/ [ 9]: https://www.media.mit.edu/projects/ai-ethics-for-middle-school/overview/ [ 10]: https://thegoodai.co/2021/04/02/14-research-institutes-paving-the-way-for-a-responsible-use-of-ai-for-good/ [ 11]: https://www.energy.gov/sites/default/files/2019/05/f62/STEM-Education-Strategic-Plan-2018.pdf [ 12]: https://www.weforum.org/platforms/shaping-the-future-of-technology-governance-artificial-intelligence-and-machine-learning [ 13]: https://venturebeat.com/2021/05/14/ai-weekly-how-to-implement-ai-responsibly/ [ 14]: https://www.prnewswire.com/news-releases/joint-artificial-intelligence-center-to-pilot-a-responsible-ai-procurement-process-301343009.html [ 15]: https://www.hypergiant.com/ [ 16]: https://events.venturebeat.com/transform2021/ [ 17]: https://venturebeat.com/venturebeat-membership-plans/ 
Classification
Language: English
Publication-Type: Web Blog
Journal Code: VNTR-6695
Subject: ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); CIVIL RIGHTS (90%); DEMOCRACIES (90%); ETHICS (90%); STANDARDS & MEASUREMENTS (90%); TYPES OF GOVERNMENT (90%); EDUCATION & TRAINING (78%); POLITICAL & SOCIAL IDEOLOGIES (78%); SCHOOL DESEGREGATION (78%); NUCLEAR WEAPONS (77%); US PRESIDENTS (77%); ARMS RACE (71%); DEFENSE SPENDING (71%); MILITARY WEAPONS (71%); WEAPONS & ARMS (70%); BOMBS & EXPLOSIVE DEVICES (65%); EXECUTIVE ORDERS (62%); PROFESSIONAL WORKERS (60%); AI (%); category-/News (%); ethical AI (%); VB Home Page (%)
Industry: ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); BANKING & FINANCE (90%); NUCLEAR WEAPONS (77%); DEFENSE SPENDING (71%); MILITARY WEAPONS (71%)
Geographic: UNITED STATES (96%); CHINA (79%)
Load-Date: August 28, 2021",neutral,0.5892689228057861,balanced/neutral,"['privacy', 'access']",['equality'],"['policy', 'governance', 'standards', 'framework', 'law', 'certification', 'should', 'must']",['machine learning'],2,1,8,1
2021,Unknown Title,"Body
BOSTON: DataRobot, the leader in enterprise AI, today announced it has named Haniyeh Mahmoudian, Ph.D , as its new Global AI Ethicist, a role created to further the company’s mission to build and deliver trustworthy and ethical AI systems. Mahmoudian, an astrophysicist by training, previously served as a data science researcher for DataRobot, focusing on privacy, bias, trust, and ethics in AI. In her new role, she will lead the new DataRobot Applied AI Ethics Team, which will support thought leadership in AI ethics and create trusted technical resources and governance frameworks for DataRobot’s stakeholders.
The DataRobot Applied AI Ethics Team that Mahmoudian will lead was created to support not only thought leadership around ethical AI, but to also provide actionable guidance for the company’s customers, which cover one-third of the Fortune 50, including some of the largest banks in the world, top U.S health insurers, and defense, intelligence, and civilian agencies within the federal government.
“When used properly, AI can be a force for good and help contribute solutions to some of society’s most pressing issues, such as access to equitable healthcare,” said Mahmoudian. “The COVID-19 pandemic has inspired unprecedented interest in AI. However, to accomplish those goals we must ensure machine learning systems have trustworthy and ethical parameters built in from the start. I’m proud to lead DataRobot’s efforts to make AI tools accessible, trusted, and equitable for all stakeholders, including the medical community. ”
Mahmoudian holds a PhD in Astronomy and Astrophysics from the Rheinische Friedrich-Wilhelms-Universität Bonn. She will be speaking on the important topic of Trust in AI at DataRobot’s AI Experience Worldwide virtual conference, taking place May 11-12.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: IFN
Subject: ARTIFICIAL INTELLIGENCE ETHICS (96%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); ASTRONOMY & SPACE (89%); ASTROPHYSICS (89%); EMPLOYMENT HISTORY (78%); GOVERNMENT & PUBLIC ADMINISTRATION (78%); HEALTH EQUITY (78%); PHYSICS (77%); DATA SCIENCE (73%); MACHINE LEARNING (73%); EXPERIMENTATION & RESEARCH (72%); INTELLIGENCE SERVICES (72%); EPIDEMICS (68%); INFECTIOUS DISEASE (68%); HEALTH CARE ACCESS (67%); COVID CORONAVIRUS (66%); COVID-19 CORONAVIRUS (66%)
Company:  AI SYSTEMS (57%)
Industry: SIC7372 PREPACKAGED SOFTWARE (57%); ARTIFICIAL INTELLIGENCE ETHICS (96%); ARTIFICIAL INTELLIGENCE (90%); HEALTH EQUITY (78%); DATA SCIENCE (73%); MACHINE LEARNING (73%); HEALTH INSURANCE (68%); INSURANCE (68%); HEALTH CARE ACCESS (67%)
Load-Date: April 23, 2021","BOSTON: DataRobot, the leader in enterprise AI, today announced it has named Haniyeh Mahmoudian, Ph.D , as its new Global AI Ethicist, a role created to further the company’s mission to build and deliver trustworthy and ethical AI systems. Mahmoudian, an astrophysicist by training, previously served as a data science researcher for DataRobot, focusing on privacy, bias, trust, and ethics in AI. In her new role, she will lead the new DataRobot Applied AI Ethics Team, which will support thought leadership in AI ethics and create trusted technical resources and governance frameworks for DataRobot’s stakeholders.
The DataRobot Applied AI Ethics Team that Mahmoudian will lead was created to support not only thought leadership around ethical AI, but to also provide actionable guidance for the company’s customers, which cover one-third of the Fortune 50, including some of the largest banks in the world, top U.S health insurers, and defense, intelligence, and civilian agencies within the federal government.
“When used properly, AI can be a force for good and help contribute solutions to some of society’s most pressing issues, such as access to equitable healthcare,” said Mahmoudian. “The COVID-19 pandemic has inspired unprecedented interest in AI. However, to accomplish those goals we must ensure machine learning systems have trustworthy and ethical parameters built in from the start. I’m proud to lead DataRobot’s efforts to make AI tools accessible, trusted, and equitable for all stakeholders, including the medical community. ”
Mahmoudian holds a PhD in Astronomy and Astrophysics from the Rheinische Friedrich-Wilhelms-Universität Bonn. She will be speaking on the important topic of Trust in AI at DataRobot’s AI Experience Worldwide virtual conference, taking place May 11-12.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: IFN
Subject: ARTIFICIAL INTELLIGENCE ETHICS (96%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); ASTRONOMY & SPACE (89%); ASTROPHYSICS (89%); EMPLOYMENT HISTORY (78%); GOVERNMENT & PUBLIC ADMINISTRATION (78%); HEALTH EQUITY (78%); PHYSICS (77%); DATA SCIENCE (73%); MACHINE LEARNING (73%); EXPERIMENTATION & RESEARCH (72%); INTELLIGENCE SERVICES (72%); EPIDEMICS (68%); INFECTIOUS DISEASE (68%); HEALTH CARE ACCESS (67%); COVID CORONAVIRUS (66%); COVID-19 CORONAVIRUS (66%)
Company:  AI SYSTEMS (57%)
Industry: SIC7372 PREPACKAGED SOFTWARE (57%); ARTIFICIAL INTELLIGENCE ETHICS (96%); ARTIFICIAL INTELLIGENCE (90%); HEALTH EQUITY (78%); DATA SCIENCE (73%); MACHINE LEARNING (73%); HEALTH INSURANCE (68%); INSURANCE (68%); HEALTH CARE ACCESS (67%)
Load-Date: April 23, 2021",positive,0.6856122612953186,balanced/neutral,"['privacy', 'bias', 'access']",['equity'],"['governance', 'must']",['machine learning'],3,1,2,1
2021,Unknown Title,"Body
London: Rolls-Royce Holdings plc has issued the following press release:
Rolls-Royce has released asimple and effectivenew data bias tool toitspioneering artificial intelligence(AI)ethics and trustworthiness toolkit, The Aletheia Framework.We have also announced AI ethics collaborations with musiccataloguing start-up, Musiio; and with internationalAIoncologyexperts.
Bias in the requirements, algorithms and data used to train AIs impacts the effectiveness and trustworthiness of AI and is one of the hardest challenges to overcome. It causes inaccuracyandnegativebiasin the way the AI analyses data and subsequently makes decisions, eroding trust in a technology that should be a valuable partner in our daily lives at home or at work.
Sitting as part ofTheAletheia Framework2.0ecosystem, released today, thenewtoolisbased on atried and testedmethod ofidentifying and managingrisk in very complex and novel systems. It hasbeen adapted to perform the same role inAI,helpingdevelopers and organisationsachievehighlyaccurateand faireroutcomes from theiruse ofthe technology.
Caroline Gorski, Group Director for Rolls-Royce's data innovation unit, R2Data Labs, said: “We ’ reexcitedto be addingeven greater practicality toThe Aletheia Framework, whichisuniquely conciseand focusedonnavigating theday-to-day intricacies of applyingAIin anethical and trustworthy way, such as bias in data.
“In the year since wefirstpublishedthe framework, we ’ ve beenhumbled by the level of interest, feedback and enthusiasmforsomething that startedoutas an answer to an internal challenge – crucially in abusiness-criticalcontext.
“Toenhanceitseffectiveness, not only are we adding this newAIbias tool, butwe ’ vealsosought out collaborationswith Musiio;international AI oncology expertstotesthowthe frameworkperformsandto hear how it can be moreuser-friendly and flexible.All these lessons have been included in The Aletheia Framework v2.0, which is releasedtodayandwebelieve thatitcanbeapplied to any use of AI,eitheras a templateora general guidefor organisationstostructure theirthinking on this complex topic. ”
Thenew data biastool also extends the ability of The Aletheia Framework to enable organisations to apply rigor across the entire life of their AI product: from pre-development ethical considerations; to training data bias mitigation; and then the trustworthiness check on the decisions an AI makes after it has been deployed.
Crucially,The Aletheia Frameworkdoes not scrutinise algorithms themselves, which are highly complex, often commercially sensitive and always evolving. Instead,it focuses on the inputstoandcontinuously checkstheoutputsfromthose algorithms. This makes it simple and fast to use, as well as being applicable in any AI context.
Examples of how The Aletheia Framework has been used
Music
Hazel Savage, co-founder and Chief Executive Officer of Musiio,said: “There are more than 60,000 songs being released on to streaming services every day, which is an unmanageable amount to processmanually.We ’ vetrainedan AI that can listen to music.I was already having many of these thoughts and ideas around ethical AI and when I saw The Aletheia Framework, I thought someone has aand we ’ re now using The Aletheia Framework to guide our product strategy in terms of how we think around using AIfrom an ethicalperspective. ”
See Hazel ’ s full story here and how she used The Aletheia Framework here.
Oncology
Massachusetts, USA-basedMatthew Katz MD, Partner in Radiation Oncology Associates, said: “As a doctor, my purpose is to help people with decision making, they are often difficult decisions in cancer care. Wehave totrust the tools that we have, including artificial intelligence. The data in healthcare mostly relies on clinical trials and research often published in elite institutions. The selection bias in that process means many people may not be included in those data sets,so what resonated for me aboutThe Aletheia Framework was the potential for transparency in how data works;in making sure that the data available applies fully to the person in front of me, even if it ’ s incomplete data;or if it requires my clinical judgement to be included. I am accountable to patients and the artificial intelligence systems should be also and the framework captures that. ”
Dr.Marianne Aznar,Senior Lecturer in Adaptive Radiotherapy at The University of Manchester,UK,said: “When I heard about the Aletheia Framework and that Rolls-Royce was working with artificial intelligence and ethics, I thought this was yet another chance for us in the radiotherapy community to learn from that field and to apply their work to our own processes.So far, a lot of our research has been around the accuracy of the solutions. But what The Aletheia Framework is going to help us to do it to start discussions in the other areas so that we can bring AI solutions from research and really into the daily clinic workflow.“
See the full story of the oncology collaboration here and the oncology version of The Aletheia Framework here.
The AI in oncology working group also included Dr. Raj Jena from Cambridge University Hospitals NHS Foundation Trust; Dr. Matthew Williams, Imperial College Healthcare NHS Trust; Dr. Issam El Naqa, Moffitt Cancer Centre; and Clifton David Fuller MD from The University of Texas MD Anderson Cancer Center.
Education
Lord Tim Clement-Jones, chair of The Institute for Ethical AI in Education, said:“I commented on theoriginalversion of The AletheiaFramework,and it deals with many of the same areas in educationas it does for Rolls-Royce in manufacturing – ethics, impact,compliance,data protection.So,I saw an equivalence there andwe adapted TheAletheiaFrameworkfor our needs. ”
See Lord Clement-Jones's full story hereand the institute ’ s recommendations here.
The Aletheia Framework v2.0 can be downloaded from Rolls-Royce.com website, along with the data bias tool, which is a pre-configured Excel spreadsheet, as well as an FAQ. User guides and other case studies can also be accessed.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: IFN
Subject: ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ETHICS (90%); HOLDING COMPANIES (90%); PRESS RELEASES (90%); ONCOLOGY (89%); COMPANY PRESS RELEASES (78%); COMPANY STRATEGY (72%); ASSOCIATIONS & ORGANIZATIONS (68%); EXECUTIVES (68%)
Company:  ROLLS-ROYCE HOLDINGS PLC (94%)
Ticker: RRS (FRA) (94%); RR (LSE) (94%)
Industry: NAICS336412 AIRCRAFT ENGINE & ENGINE PARTS MANUFACTURING (94%); NAICS333611 TURBINE & TURBINE GENERATOR SET UNITS MANUFACTURING (94%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ONCOLOGY (89%); STREAMING MEDIA (73%)
Load-Date: December 19, 2021","London: Rolls-Royce Holdings plc has issued the following press release:
Rolls-Royce has released asimple and effectivenew data bias tool toitspioneering artificial intelligence(AI)ethics and trustworthiness toolkit, The Aletheia Framework.We have also announced AI ethics collaborations with musiccataloguing start-up, Musiio; and with internationalAIoncologyexperts.
Bias in the requirements, algorithms and data used to train AIs impacts the effectiveness and trustworthiness of AI and is one of the hardest challenges to overcome. It causes inaccuracyandnegativebiasin the way the AI analyses data and subsequently makes decisions, eroding trust in a technology that should be a valuable partner in our daily lives at home or at work.
Sitting as part ofTheAletheia Framework2.0ecosystem, released today, thenewtoolisbased on atried and testedmethod ofidentifying and managingrisk in very complex and novel systems. It hasbeen adapted to perform the same role inAI,helpingdevelopers and organisationsachievehighlyaccurateand faireroutcomes from theiruse ofthe technology.
Caroline Gorski, Group Director for Rolls-Royce's data innovation unit, R2Data Labs, said: “We ’ reexcitedto be addingeven greater practicality toThe Aletheia Framework, whichisuniquely conciseand focusedonnavigating theday-to-day intricacies of applyingAIin anethical and trustworthy way, such as bias in data.
“In the year since wefirstpublishedthe framework, we ’ ve beenhumbled by the level of interest, feedback and enthusiasmforsomething that startedoutas an answer to an internal challenge – crucially in abusiness-criticalcontext.
“Toenhanceitseffectiveness, not only are we adding this newAIbias tool, butwe ’ vealsosought out collaborationswith Musiio;international AI oncology expertstotesthowthe frameworkperformsandto hear how it can be moreuser-friendly and flexible.All these lessons have been included in The Aletheia Framework v2.0, which is releasedtodayandwebelieve thatitcanbeapplied to any use of AI,eitheras a templateora general guidefor organisationstostructure theirthinking on this complex topic. ”
Thenew data biastool also extends the ability of The Aletheia Framework to enable organisations to apply rigor across the entire life of their AI product: from pre-development ethical considerations; to training data bias mitigation; and then the trustworthiness check on the decisions an AI makes after it has been deployed.
Crucially,The Aletheia Frameworkdoes not scrutinise algorithms themselves, which are highly complex, often commercially sensitive and always evolving. Instead,it focuses on the inputstoandcontinuously checkstheoutputsfromthose algorithms. This makes it simple and fast to use, as well as being applicable in any AI context.
Examples of how The Aletheia Framework has been used
Music
Hazel Savage, co-founder and Chief Executive Officer of Musiio,said: “There are more than 60,000 songs being released on to streaming services every day, which is an unmanageable amount to processmanually.We ’ vetrainedan AI that can listen to music.I was already having many of these thoughts and ideas around ethical AI and when I saw The Aletheia Framework, I thought someone has aand we ’ re now using The Aletheia Framework to guide our product strategy in terms of how we think around using AIfrom an ethicalperspective. ”
See Hazel ’ s full story here and how she used The Aletheia Framework here.
Oncology
Massachusetts, USA-basedMatthew Katz MD, Partner in Radiation Oncology Associates, said: “As a doctor, my purpose is to help people with decision making, they are often difficult decisions in cancer care. Wehave totrust the tools that we have, including artificial intelligence. The data in healthcare mostly relies on clinical trials and research often published in elite institutions. The selection bias in that process means many people may not be included in those data sets,so what resonated for me aboutThe Aletheia Framework was the potential for transparency in how data works;in making sure that the data available applies fully to the person in front of me, even if it ’ s incomplete data;or if it requires my clinical judgement to be included. I am accountable to patients and the artificial intelligence systems should be also and the framework captures that. ”
Dr.Marianne Aznar,Senior Lecturer in Adaptive Radiotherapy at The University of Manchester,UK,said: “When I heard about the Aletheia Framework and that Rolls-Royce was working with artificial intelligence and ethics, I thought this was yet another chance for us in the radiotherapy community to learn from that field and to apply their work to our own processes.So far, a lot of our research has been around the accuracy of the solutions. But what The Aletheia Framework is going to help us to do it to start discussions in the other areas so that we can bring AI solutions from research and really into the daily clinic workflow.“
See the full story of the oncology collaboration here and the oncology version of The Aletheia Framework here.
The AI in oncology working group also included Dr. Raj Jena from Cambridge University Hospitals NHS Foundation Trust; Dr. Matthew Williams, Imperial College Healthcare NHS Trust; Dr. Issam El Naqa, Moffitt Cancer Centre; and Clifton David Fuller MD from The University of Texas MD Anderson Cancer Center.
Education
Lord Tim Clement-Jones, chair of The Institute for Ethical AI in Education, said:“I commented on theoriginalversion of The AletheiaFramework,and it deals with many of the same areas in educationas it does for Rolls-Royce in manufacturing – ethics, impact,compliance,data protection.So,I saw an equivalence there andwe adapted TheAletheiaFrameworkfor our needs. ”
See Lord Clement-Jones's full story hereand the institute ’ s recommendations here.
The Aletheia Framework v2.0 can be downloaded from Rolls-Royce.com website, along with the data bias tool, which is a pre-configured Excel spreadsheet, as well as an FAQ. User guides and other case studies can also be accessed.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: IFN
Subject: ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ETHICS (90%); HOLDING COMPANIES (90%); PRESS RELEASES (90%); ONCOLOGY (89%); COMPANY PRESS RELEASES (78%); COMPANY STRATEGY (72%); ASSOCIATIONS & ORGANIZATIONS (68%); EXECUTIVES (68%)
Company:  ROLLS-ROYCE HOLDINGS PLC (94%)
Ticker: RRS (FRA) (94%); RR (LSE) (94%)
Industry: NAICS336412 AIRCRAFT ENGINE & ENGINE PARTS MANUFACTURING (94%); NAICS333611 TURBINE & TURBINE GENERATOR SET UNITS MANUFACTURING (94%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ONCOLOGY (89%); STREAMING MEDIA (73%)
Load-Date: December 19, 2021",neutral,0.7331001162528992,balanced/neutral,"['bias', 'transparency']",[],"['framework', 'compliance', 'should']",[],2,0,3,0
2021,Unknown Title,"Byline: Hannah Basali
Body
Some students and professors are pushing their peers to be leaders in the conversation around ethics and social responsibility in tech given Stanford's unique role as an institution at the heart of Silicon Valley,
""Stanford is the premier educator of the visionary founders and hardware and software engineers who populate Silicon Valley,"" political science professor Rob Reich M.A. '98 Ph.D. '98, who teaches a course on ethics in technology and computer science, wrote in a statement to The Daily. ""Stanford should not merely produce technological innovation; it should study and teach about its ethical and social ramifications.""
In recent years, the University has made strides toward this goal, including implementing new academic initiatives that focus on the intersection of ethics and technology. To educate students and communities beyond Stanford about ethics in tech, the University's computer science department has implemented courses into its curriculum bridging the humanities, social sciences and CS, thereby exemplifying an interdisciplinary approach to tech, Reich wrote.
One course that Reich teaches, CS 182: ""Ethics, Public Policy and Technological Change,"" has even been offered to the broader Silicon Valley community through Stanford's Continuing Studies program, while another version of the class was offered to tech professionals in San Francisco. The CS department has also started an Embedded Ethics initiative, inserting ethics modules into courses in the undergraduate CS curriculum.
""Stanford bears more responsibility than any other university for inventing the digital technologies that have upended industries and profoundly affected the lives of people and societies across the globe,"" Reich wrote. 
This shift toward a focus on ethics in the technology industry's priorities is epitomized by the stories of two Stanford founders, according to a FastCompany article written by Reich, political science professor Jeremey Weinstein and computer science professor Mehran Sahami Ph.D. '99.
In the article, the professors compare the lives of Joshua Browder '18 - the founder of DoNotPay, and who they wrote represents the quick rise of Silicon Valley tech leaders - and Aaron Swartz, a tech entrepreneur who played a key role in the foundational stages of Reddit and whose work focused more on accessibility and accountability in tech.
The professors invoke the two Stanford alumni, writing that the ""rise of the Joshua Browders and the decline of the Aaron Swartzes encapsulate the challenge the world confronts with Silicon Valley.""
By grounding students in personal, professional, social and political ethics, Reich wrote that he hopes Stanford can challenge students to ""think about their role as enablers and shapers of technological change in society"" and ""internalize a commitment to their responsibilities as innovators, designers, coders, engineers, policymakers, citizens and consumers.""
Computer science students Devin Green '24 and Afnaan Hashmi '24 have also noticed the shift in campus and Silicon Valley culture surrounding technological innovation and its ethical implications. 
Green said that those who code for the sake of advocacy and inclusive innovation are often left behind because ""technology and programming, in the big-tech world especially, is more about making something cool that also makes money."" He referenced Timnit Gebru, who was fired from Google after advocating for less bias within Google's artificial intelligence algorithms.
Green said that amid the rise of the next generation of coders and innovators, he hopes that the technology industry can focus on coding for good instead of for wealth. 
Students at Stanford want to help lead the new wave in ethical tech. ""Innovation needs to exist in tandem with ethics and discussions surrounding the ethical implications of a given piece of technology,"" Hashmi said. He said that Stanford's ethics in CS and technology class is a step in the right direction and that he hopes to see this effort expanded to K-12 curricula.
Reich hopes that Stanford will lead the transition to a more ethical tech industry. 
""With every new innovation, we want students to ask, What am I enabling others to do? What responsibilities does this imply for me as an innovator, a citizen, and a human being?"" Reich wrote.
Classification
Language: ENGLISH
Publication-Type: Newspaper
Subject: ETHICS (91%); COLLEGE & UNIVERSITY PROFESSORS (90%); COMPUTER SCIENCE (90%); CURRICULA (90%); HUMANITIES & SOCIAL SCIENCE (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (90%); STUDENTS & STUDENT LIFE (90%); TEACHING & TEACHERS (90%); EMERGING TECHNOLOGY (89%); MATH & SCIENCE EDUCATION (89%); POLITICAL SCIENCE (89%); POLITICS (89%); PRODUCT INNOVATION (89%); SOCIAL SCIENCE EDUCATION (89%); TECHNICIANS & TECHNOLOGICAL WORKERS (89%); GOVERNMENT ETHICS (78%); COMPUTER ENGINEERING (77%); ENGINEERING (77%); SOFTWARE DEVELOPERS (77%); PUBLIC POLICY (76%); WRITERS (75%); ENTREPRENEURSHIP (65%)
Industry: COLLEGE & UNIVERSITY PROFESSORS (90%); COMPUTER SCIENCE (90%); SOFTWARE SERVICES & APPLICATIONS (78%); COMPUTER ENGINEERING (77%); ENGINEERING (77%); INFORMATION TECHNOLOGY INDUSTRY (77%); SOFTWARE DEVELOPERS (77%); WRITERS (75%); COMPUTER SOFTWARE (72%); SOFTWARE DEVELOPMENT & ENGINEERING (72%)
Geographic: SAN FRANCISCO BAY AREA, CA, USA (94%); SILICON VALLEY, CA, USA (94%); SAN FRANCISCO, CA, USA (79%); CALIFORNIA, USA (79%); Stanford; CA
Load-Date: October 5, 2021","Some students and professors are pushing their peers to be leaders in the conversation around ethics and social responsibility in tech given Stanford's unique role as an institution at the heart of Silicon Valley,
""Stanford is the premier educator of the visionary founders and hardware and software engineers who populate Silicon Valley,"" political science professor Rob Reich M.A. '98 Ph.D. '98, who teaches a course on ethics in technology and computer science, wrote in a statement to The Daily. ""Stanford should not merely produce technological innovation; it should study and teach about its ethical and social ramifications.""
In recent years, the University has made strides toward this goal, including implementing new academic initiatives that focus on the intersection of ethics and technology. To educate students and communities beyond Stanford about ethics in tech, the University's computer science department has implemented courses into its curriculum bridging the humanities, social sciences and CS, thereby exemplifying an interdisciplinary approach to tech, Reich wrote.
One course that Reich teaches, CS 182: ""Ethics, Public Policy and Technological Change,"" has even been offered to the broader Silicon Valley community through Stanford's Continuing Studies program, while another version of the class was offered to tech professionals in San Francisco. The CS department has also started an Embedded Ethics initiative, inserting ethics modules into courses in the undergraduate CS curriculum.
""Stanford bears more responsibility than any other university for inventing the digital technologies that have upended industries and profoundly affected the lives of people and societies across the globe,"" Reich wrote. 
This shift toward a focus on ethics in the technology industry's priorities is epitomized by the stories of two Stanford founders, according to a FastCompany article written by Reich, political science professor Jeremey Weinstein and computer science professor Mehran Sahami Ph.D. '99.
In the article, the professors compare the lives of Joshua Browder '18 - the founder of DoNotPay, and who they wrote represents the quick rise of Silicon Valley tech leaders - and Aaron Swartz, a tech entrepreneur who played a key role in the foundational stages of Reddit and whose work focused more on accessibility and accountability in tech.
The professors invoke the two Stanford alumni, writing that the ""rise of the Joshua Browders and the decline of the Aaron Swartzes encapsulate the challenge the world confronts with Silicon Valley.""
By grounding students in personal, professional, social and political ethics, Reich wrote that he hopes Stanford can challenge students to ""think about their role as enablers and shapers of technological change in society"" and ""internalize a commitment to their responsibilities as innovators, designers, coders, engineers, policymakers, citizens and consumers.""
Computer science students Devin Green '24 and Afnaan Hashmi '24 have also noticed the shift in campus and Silicon Valley culture surrounding technological innovation and its ethical implications. 
Green said that those who code for the sake of advocacy and inclusive innovation are often left behind because ""technology and programming, in the big-tech world especially, is more about making something cool that also makes money."" He referenced Timnit Gebru, who was fired from Google after advocating for less bias within Google's artificial intelligence algorithms.
Green said that amid the rise of the next generation of coders and innovators, he hopes that the technology industry can focus on coding for good instead of for wealth. 
Students at Stanford want to help lead the new wave in ethical tech. ""Innovation needs to exist in tandem with ethics and discussions surrounding the ethical implications of a given piece of technology,"" Hashmi said. He said that Stanford's ethics in CS and technology class is a step in the right direction and that he hopes to see this effort expanded to K-12 curricula.
Reich hopes that Stanford will lead the transition to a more ethical tech industry. 
""With every new innovation, we want students to ask, What am I enabling others to do? What responsibilities does this imply for me as an innovator, a citizen, and a human being?"" Reich wrote.
Classification
Language: ENGLISH
Publication-Type: Newspaper
Subject: ETHICS (91%); COLLEGE & UNIVERSITY PROFESSORS (90%); COMPUTER SCIENCE (90%); CURRICULA (90%); HUMANITIES & SOCIAL SCIENCE (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (90%); STUDENTS & STUDENT LIFE (90%); TEACHING & TEACHERS (90%); EMERGING TECHNOLOGY (89%); MATH & SCIENCE EDUCATION (89%); POLITICAL SCIENCE (89%); POLITICS (89%); PRODUCT INNOVATION (89%); SOCIAL SCIENCE EDUCATION (89%); TECHNICIANS & TECHNOLOGICAL WORKERS (89%); GOVERNMENT ETHICS (78%); COMPUTER ENGINEERING (77%); ENGINEERING (77%); SOFTWARE DEVELOPERS (77%); PUBLIC POLICY (76%); WRITERS (75%); ENTREPRENEURSHIP (65%)
Industry: COLLEGE & UNIVERSITY PROFESSORS (90%); COMPUTER SCIENCE (90%); SOFTWARE SERVICES & APPLICATIONS (78%); COMPUTER ENGINEERING (77%); ENGINEERING (77%); INFORMATION TECHNOLOGY INDUSTRY (77%); SOFTWARE DEVELOPERS (77%); WRITERS (75%); COMPUTER SOFTWARE (72%); SOFTWARE DEVELOPMENT & ENGINEERING (72%)
Geographic: SAN FRANCISCO BAY AREA, CA, USA (94%); SILICON VALLEY, CA, USA (94%); SAN FRANCISCO, CA, USA (79%); CALIFORNIA, USA (79%); Stanford; CA
Load-Date: October 5, 2021",neutral,0.5067808032035828,balanced/neutral,"['bias', 'accountability']",[],"['policy', 'should']",[],2,0,2,0
2021,Unknown Title,"Dateline: NEW YORK, Feb. 24, 2021 
Body
PR Newswire
 Genpact(NYSE: G), a global professional services firm focused on delivering digital transformation, today announced it has received one of the 2021 World's Most Ethical Companiesawards, presented by the Ethisphere Institute, a global leader in defining and advancing the standards of ethical business practices.
This is the third year that Ethisphere has recognized Genpact, which is one of only four honorees in the consulting services category for 2021. The award emphasizes the quality of Genpact's ethics and compliance program, organizational culture, corporate citizenship and responsibility, governance, leadership, innovation, and reputation. Ethisphere also recognizes Genpact for demonstrating the connection between good ethical practices and performance.
""Genpact's purpose is the relentless pursuit of a world that works better for people, and ethics are a cornerstone of achieving and living up to that purpose. It is an honor that Ethisphere has recognized us as one of the world's most ethical companies,"" saidTiger Tyagarajan, chief executive officer, Genpact. ""This award is a great recognition of the commitment of our more than 90,000 people, who embody curiosity, courage, incisiveness, and integrity to provide value to our clients, shareholders, and communities around the world.""
""While addressing the tough challenges of 2020, we saw companies lead – above all other institutions – on earning the trust of stakeholders through resilience and a commitment to ethics and integrity,"" said Timothy Erblich, chief executive officer, Ethisphere. ""The World's Most Ethical Companies honorees continue to demonstrate an unwavering commitment to the highest values and positively impacting the communities they serve. Congratulations to everyone at Genpact for earning the World's Most Ethical Companies designation.""
In 2021, the Ethisphere Institute recognized 135 honorees spanning 22 countries and 47 industries. Seehttps://worldsmostethicalcompanies.com/honoreesfor the complete list.
Methodology and ScoringGrounded in Ethisphere's proprietary Ethics Quotient®, the World's Most Ethical Companies assessment process includes more than 200 questions on culture, environmental and social practices, ethics and compliance activities, governance, diversity, and initiatives to support a strong value chain. The process serves as an operating framework to capture and codify the leading practices of organizations across industries and around the globe.
This year, Ethisphere streamlined the process and expanded question set to gauge how applicants are adapting and responding to the global health pandemic; environmental, social, and governance factors; safety; equity and inclusion; and social justice.
About the Ethisphere InstituteThe Ethisphere® Institute is the global leader in defining and advancing the standards of ethical business practices that fuel corporate character, marketplace trust and business success. Ethisphere has deep expertise in measuring and defining core ethics standards using data-driven insights that help companies enhance corporate character and measure and improve culture. Ethisphere honors superior achievement through its World's Most Ethical Companies recognition program and provides a community of industry experts with the Business Ethics Leadership Alliance (BELA). More information about Ethisphere can be found at: https://ethisphere.com. 
About GenpactGenpact (NYSE: G) is a global professional services firm that makes business transformation real. We drive digital-led innovation and digitally-enabled intelligent operations for our clients, guided by our experience running thousands of processes primarily for Global Fortune 500 companies. We think with design, dream in digital, and solve problems with data and analytics.  Combining our expertise in end-to-end operations and our AI-based platform, Genpact Cora, we focus on the details – all 90,000+ of us. From New York to New Delhi and more than 30 countries in between, we connect every dot, reimagine every process, and reinvent companies' ways of working. We know that reimagining each step from start to finish creates better business outcomes. Whatever it is, we'll be there with you – accelerating digital transformation to create bold, lasting results – becausetransformation happens here. Get to know us atGenpact.com and onLinkedIn, Twitter,YouTube, and Facebook.
 View original content to download multimedia:http://www.prnewswire.com/news-releases/genpact-named-one-of-worlds-most-ethical-companies-by-ethisphere-institute-for-third-year-301234456.html
SOURCE Genpact
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ETHICS (93%); BUSINESS ETHICS (91%); COMPANY ACTIVITIES & MANAGEMENT (90%); PRESS RELEASES (90%); PUBLIC COMPANIES (90%); BUSINESS ETHICS & CORPORATE CITIZENSHIP (89%); CORPORATE CULTURE (89%); ESG FACTORS - SOCIAL (89%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (89%); ASSOCIATIONS & ORGANIZATIONS (78%); CORPORATE SOCIAL RESPONSIBILITY (78%); SHAREHOLDERS (78%); ESG FACTORS (77%); EXECUTIVES (77%); SOCIAL JUSTICE (77%); REGULATORY COMPLIANCE (73%); VALUE CHAIN (73%); SAFETY (63%); Genpact-Ethisphere (%); AWD Awards (%)
Company:  GENPACT LTD (90%); Genpact
Ticker: G (NYSE) (90%); G (NYSE)
Industry: NAICS561499 ALL OTHER BUSINESS SUPPORT SERVICES (90%); NAICS541519 OTHER COMPUTER RELATED SERVICES (90%); DIGITALIZATION & DIGITAL TRANSFORMATION (78%); PROFESSIONAL SERVICES (78%); CONSULTING SERVICES (71%); CPR Computer; Electronics Products (%); DTA Data Analytics (%); PUB Publishing; Information Services (%)
Geographic: New York
Load-Date: February 24, 2021","PR Newswire
 Genpact(NYSE: G), a global professional services firm focused on delivering digital transformation, today announced it has received one of the 2021 World's Most Ethical Companiesawards, presented by the Ethisphere Institute, a global leader in defining and advancing the standards of ethical business practices.
This is the third year that Ethisphere has recognized Genpact, which is one of only four honorees in the consulting services category for 2021. The award emphasizes the quality of Genpact's ethics and compliance program, organizational culture, corporate citizenship and responsibility, governance, leadership, innovation, and reputation. Ethisphere also recognizes Genpact for demonstrating the connection between good ethical practices and performance.
""Genpact's purpose is the relentless pursuit of a world that works better for people, and ethics are a cornerstone of achieving and living up to that purpose. It is an honor that Ethisphere has recognized us as one of the world's most ethical companies,"" saidTiger Tyagarajan, chief executive officer, Genpact. ""This award is a great recognition of the commitment of our more than 90,000 people, who embody curiosity, courage, incisiveness, and integrity to provide value to our clients, shareholders, and communities around the world.""
""While addressing the tough challenges of 2020, we saw companies lead – above all other institutions – on earning the trust of stakeholders through resilience and a commitment to ethics and integrity,"" said Timothy Erblich, chief executive officer, Ethisphere. ""The World's Most Ethical Companies honorees continue to demonstrate an unwavering commitment to the highest values and positively impacting the communities they serve. Congratulations to everyone at Genpact for earning the World's Most Ethical Companies designation.""
In 2021, the Ethisphere Institute recognized 135 honorees spanning 22 countries and 47 industries. Seehttps://worldsmostethicalcompanies.com/honoreesfor the complete list.
Methodology and ScoringGrounded in Ethisphere's proprietary Ethics Quotient®, the World's Most Ethical Companies assessment process includes more than 200 questions on culture, environmental and social practices, ethics and compliance activities, governance, diversity, and initiatives to support a strong value chain. The process serves as an operating framework to capture and codify the leading practices of organizations across industries and around the globe.
This year, Ethisphere streamlined the process and expanded question set to gauge how applicants are adapting and responding to the global health pandemic; environmental, social, and governance factors; safety; equity and inclusion; and social justice.
About the Ethisphere InstituteThe Ethisphere® Institute is the global leader in defining and advancing the standards of ethical business practices that fuel corporate character, marketplace trust and business success. Ethisphere has deep expertise in measuring and defining core ethics standards using data-driven insights that help companies enhance corporate character and measure and improve culture. Ethisphere honors superior achievement through its World's Most Ethical Companies recognition program and provides a community of industry experts with the Business Ethics Leadership Alliance (BELA). More information about Ethisphere can be found at: https://ethisphere.com. 
About GenpactGenpact (NYSE: G) is a global professional services firm that makes business transformation real. We drive digital-led innovation and digitally-enabled intelligent operations for our clients, guided by our experience running thousands of processes primarily for Global Fortune 500 companies. We think with design, dream in digital, and solve problems with data and analytics.  Combining our expertise in end-to-end operations and our AI-based platform, Genpact Cora, we focus on the details – all 90,000+ of us. From New York to New Delhi and more than 30 countries in between, we connect every dot, reimagine every process, and reinvent companies' ways of working. We know that reimagining each step from start to finish creates better business outcomes. Whatever it is, we'll be there with you – accelerating digital transformation to create bold, lasting results – becausetransformation happens here. Get to know us atGenpact.com and onLinkedIn, Twitter,YouTube, and Facebook.
 View original content to download multimedia:http://www.prnewswire.com/news-releases/genpact-named-one-of-worlds-most-ethical-companies-by-ethisphere-institute-for-third-year-301234456.html
SOURCE Genpact
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ETHICS (93%); BUSINESS ETHICS (91%); COMPANY ACTIVITIES & MANAGEMENT (90%); PRESS RELEASES (90%); PUBLIC COMPANIES (90%); BUSINESS ETHICS & CORPORATE CITIZENSHIP (89%); CORPORATE CULTURE (89%); ESG FACTORS - SOCIAL (89%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (89%); ASSOCIATIONS & ORGANIZATIONS (78%); CORPORATE SOCIAL RESPONSIBILITY (78%); SHAREHOLDERS (78%); ESG FACTORS (77%); EXECUTIVES (77%); SOCIAL JUSTICE (77%); REGULATORY COMPLIANCE (73%); VALUE CHAIN (73%); SAFETY (63%); Genpact-Ethisphere (%); AWD Awards (%)
Company:  GENPACT LTD (90%); Genpact
Ticker: G (NYSE) (90%); G (NYSE)
Industry: NAICS561499 ALL OTHER BUSINESS SUPPORT SERVICES (90%); NAICS541519 OTHER COMPUTER RELATED SERVICES (90%); DIGITALIZATION & DIGITAL TRANSFORMATION (78%); PROFESSIONAL SERVICES (78%); CONSULTING SERVICES (71%); CPR Computer; Electronics Products (%); DTA Data Analytics (%); PUB Publishing; Information Services (%)
Geographic: New York
Load-Date: February 24, 2021",positive,0.7837190628051758,balanced/neutral,['safety'],"['character', 'justice', 'equity', 'justice']","['governance', 'standards', 'framework', 'compliance']",[],1,4,4,0
2021,Unknown Title,"Byline: Kyle Wiggers
Body
Aug 26, 2021( VentureBeat: http://venturebeat.com/ Delivered by Newstex)  
 How open API standards will transform financial services Open standards will have a huge impact on driving innovation in banking. Learn the status in the U.S. - and the bold new opportunities open standards are set to usher in. 
 Register here[1]  
The Transform Technology Summits start October 13th with Low-Code/No Code: Enabling Enterprise Agility. Register now![2] 
 AI governance, or the process of defining policies to guide AI development, is a fast-growing market opportunity. A report[3] from StrategyR highlights this, predicting that AI governance software and services could be worth $402 million by 2026, up from $49.3 million in 2020. 
 'Amid the COVID-19 crisis, the global market for AI governance [has grown significantly],' StrategyR wrote in a press release. 'The report presents fresh perspectives on opportunities and challenges in a significantly transformed post-COVID-19 marketplace.' 
AI governance adoption 
The pandemic forced companies to rethink models used to manage AI risk, but many face continued challenges. According to a Deloitte analysis[4], as of March, 38% of organizations either lacked or had an insufficient governance structure for handling data and AI models. And a survey[5] by Pegasystems predicts that if the current trend holds, a lack of accountability within the private sector will lead to governments taking over responsibility for AI regulation over the next five years. 
 Last year, the University of California, Berkeley Center for Long-Term Cybersecurity published a report[6] positing that AI governance has gone through three stages since 2016. The first stage was marked by the release of ethics principles by tech companies and governments, followed by consensus around themes like privacy, human control, explainability, and fairness. The third stage, which began in 2019, is converting principles into practice. 
 Responsible AI practices including governance can bring major business value to bear. A study[7] by Capgemini found customers and employees will reward organizations that practice ethical AI with greater loyalty, more business, and even a willingness to advocate for them. 
 This being the case, not all organizations have gotten onboard. In a recent KPMG report, 94% of IT decision makers said that they feel that firms need to focus more on corporate responsibility and ethics while developing AI solutions. Analysts like StrategyR are betting that emerging laws such as the European Union's algorithm framework[8] and 'AI registries' in Amsterdam, Helsinki,[9] and other cities will spur companies into action, accelerating the demand for AI governance solutions that ease the adoption of best governance practices. 
 'In jurisdictions worldwide, new policy initiatives and regulations concerning the governance of data and AI signal the end of self-regulation and the rise of new oversight,' researchers at KPMG wrote in the aforementioned report. 'As the regulatory environment continues to evolve at traditional pace, leading organizations are addressing AI ethics and governance proactively rather than waiting for requirements to be enforced upon them.' VentureBeat VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative technology and transact. 
Our site delivers essential information on data technologies and strategies to guide you as you lead your organizations. We invite you to become a member of our community, to access: 
 up-to-date information on the subjects of interest to you our newsletters gated thought-leader content and discounted access to our prized events, such as Transform 2021: Learn More[10] networking features, and more 
Become a member[11] 
 [ 1]: https://www.brighttalk.com/webcast/12339/501350?utm_source=vb&#38;utm_medium=aug-16-sidebar&#38;utm_content=text-link&#38;utm_campaign=aug-31-yodlee-webinar [ 2]: https://venturebeat.com/event/the-low-code-no-code-summit/register/ [ 3]: https://www.prnewswire.com/news-releases/a-402-million-global-opportunity-for-artificial-intelligence-ai-governance-by-2026---new-research-from-strategyr-301362043.html [ 4]: https://www2.deloitte.com/content/dam/Deloitte/global/Documents/gx-risk-ai-governance-survey.pdf [ 5]: https://www.pega.com/about/news/press-releases/private-sector-expects-lose-control-ai-governance-within-five-years-says [ 6]: https://venturebeat.com/2020/05/06/how-microsoft-openai-and-oecd-are-putting-ai-ethics-principles-into-practice/ [ 7]: https://www.capgemini.com/2019/12/a-designers-view-on-ai-ethics-part-3-of-3/ [ 8]: https://venturebeat.com/2020/02/17/eus-new-ai-rules-will-focus-on-ethics-and-transparency/ [ 9]: https://venturebeat.com/2020/09/28/amsterdam-and-helsinki-launch-algorithm-registries-to-bring-transparency-to-public-deployments-of-ai/ [ 10]: https://events.venturebeat.com/transform2021/ [ 11]: https://venturebeat.com/venturebeat-membership-plans/ 
Classification
Language: English
Publication-Type: Web Blog
Journal Code: VNTR-6695
Subject: ARTIFICIAL INTELLIGENCE ETHICS (90%); FINANCIAL MARKET UPDATES (90%); STANDARDS & MEASUREMENTS (90%); AGENCY RULEMAKING (89%); ARTIFICIAL INTELLIGENCE (89%); CORPORATE GOVERNANCE (89%); ETHICS (89%); REGULATORY COMPLIANCE (89%); BLOGS & MESSAGE BOARDS (78%); BUSINESS ETHICS (78%); BUSINESS ETHICS & CORPORATE CITIZENSHIP (78%); PUBLIC POLICY (78%); SELF REGULATING ORGANIZATIONS (78%); TRENDS (75%); COVID CORONAVIRUS (74%); COVID-19 CORONAVIRUS (74%); INFECTIOUS DISEASE (74%); EUROPEAN UNION (70%); artificial intelligence (%); Big Data (%); AI (%); technology (%); StrategyR (%); category-/Business & Industrial/Business Operations (%); Dev (%); machine learning (%); ai (%); governance (%); Management (%); VB Home Page (%); Enterprise (%); services (%); software (%); Data (%); Cloud (%); Software (%)
Company:  KPMG (85%);  DELOITTE LLP (58%);  PEGASYSTEMS INC (56%);  CAPGEMINI SA (52%)
Organization: UNIVERSITY OF CALIFORNIA BERKELEY (54%)
Ticker: PEGA (NASDAQ) (56%); CAP (PAR) (52%)
Industry: NAICS541211 OFFICES OF CERTIFIED PUBLIC ACCOUNTANTS (85%); SIC8721 ACCOUNTING, AUDITING, & BOOKKEEPING SERVICES (85%); NAICS518210 DATA PROCESSING, HOSTING & RELATED SERVICES (56%); NAICS511210 SOFTWARE PUBLISHERS (56%); SIC7372 PREPACKAGED SOFTWARE (56%); NAICS551112 OFFICES OF OTHER HOLDING COMPANIES (52%); NAICS541512 COMPUTER SYSTEMS DESIGN SERVICES (52%); NAICS541511 CUSTOM COMPUTER PROGRAMMING SERVICES (52%); SIC7379 COMPUTER RELATED SERVICES, NEC (52%); SIC7373 COMPUTER INTEGRATED SYSTEMS DESIGN (52%); SIC7371 COMPUTER PROGRAMMING SERVICES (52%); SIC6719 OFFICES OF HOLDING COMPANIES, NEC (52%); ARTIFICIAL INTELLIGENCE ETHICS (90%); BANKING & FINANCE (90%); FINANCIAL MARKET UPDATES (90%); LOW CODE & NO CODE PLATFORMS (90%); ACCOUNTING & AUDITING FIRMS (89%); ARTIFICIAL INTELLIGENCE (89%); INFORMATION TECHNOLOGY INDUSTRY (79%); SOFTWARE SERVICES & APPLICATIONS (79%); BLOGS & MESSAGE BOARDS (78%); DATA GOVERNANCE & STEWARDSHIP (78%); INFORMATION SECURITY & PRIVACY (78%); AGILE DEVELOPMENT (72%)
Geographic: SAN FRANCISCO BAY AREA, CA, USA (74%); AMSTERDAM, NETHERLANDS (51%); CALIFORNIA, USA (79%); UNITED STATES (79%); NETHERLANDS (78%); FINLAND (66%); EUROPEAN UNION MEMBER STATES (51%)
Load-Date: August 25, 2021","Aug 26, 2021( VentureBeat: http://venturebeat.com/ Delivered by Newstex)  
 How open API standards will transform financial services Open standards will have a huge impact on driving innovation in banking. Learn the status in the U.S. - and the bold new opportunities open standards are set to usher in. 
 Register here[1]  
The Transform Technology Summits start October 13th with Low-Code/No Code: Enabling Enterprise Agility. Register now![2] 
 AI governance, or the process of defining policies to guide AI development, is a fast-growing market opportunity. A report[3] from StrategyR highlights this, predicting that AI governance software and services could be worth $402 million by 2026, up from $49.3 million in 2020. 
 'Amid the COVID-19 crisis, the global market for AI governance [has grown significantly],' StrategyR wrote in a press release. 'The report presents fresh perspectives on opportunities and challenges in a significantly transformed post-COVID-19 marketplace.' 
AI governance adoption 
The pandemic forced companies to rethink models used to manage AI risk, but many face continued challenges. According to a Deloitte analysis[4], as of March, 38% of organizations either lacked or had an insufficient governance structure for handling data and AI models. And a survey[5] by Pegasystems predicts that if the current trend holds, a lack of accountability within the private sector will lead to governments taking over responsibility for AI regulation over the next five years. 
 Last year, the University of California, Berkeley Center for Long-Term Cybersecurity published a report[6] positing that AI governance has gone through three stages since 2016. The first stage was marked by the release of ethics principles by tech companies and governments, followed by consensus around themes like privacy, human control, explainability, and fairness. The third stage, which began in 2019, is converting principles into practice. 
 Responsible AI practices including governance can bring major business value to bear. A study[7] by Capgemini found customers and employees will reward organizations that practice ethical AI with greater loyalty, more business, and even a willingness to advocate for them. 
 This being the case, not all organizations have gotten onboard. In a recent KPMG report, 94% of IT decision makers said that they feel that firms need to focus more on corporate responsibility and ethics while developing AI solutions. Analysts like StrategyR are betting that emerging laws such as the European Union's algorithm framework[8] and 'AI registries' in Amsterdam, Helsinki,[9] and other cities will spur companies into action, accelerating the demand for AI governance solutions that ease the adoption of best governance practices. 
 'In jurisdictions worldwide, new policy initiatives and regulations concerning the governance of data and AI signal the end of self-regulation and the rise of new oversight,' researchers at KPMG wrote in the aforementioned report. 'As the regulatory environment continues to evolve at traditional pace, leading organizations are addressing AI ethics and governance proactively rather than waiting for requirements to be enforced upon them.' VentureBeat VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative technology and transact. 
Our site delivers essential information on data technologies and strategies to guide you as you lead your organizations. We invite you to become a member of our community, to access: 
 up-to-date information on the subjects of interest to you our newsletters gated thought-leader content and discounted access to our prized events, such as Transform 2021: Learn More[10] networking features, and more 
Become a member[11] 
 [ 1]: https://www.brighttalk.com/webcast/12339/501350?utm_source=vb&#38;utm_medium=aug-16-sidebar&#38;utm_content=text-link&#38;utm_campaign=aug-31-yodlee-webinar [ 2]: https://venturebeat.com/event/the-low-code-no-code-summit/register/ [ 3]: https://www.prnewswire.com/news-releases/a-402-million-global-opportunity-for-artificial-intelligence-ai-governance-by-2026---new-research-from-strategyr-301362043.html [ 4]: https://www2.deloitte.com/content/dam/Deloitte/global/Documents/gx-risk-ai-governance-survey.pdf [ 5]: https://www.pega.com/about/news/press-releases/private-sector-expects-lose-control-ai-governance-within-five-years-says [ 6]: https://venturebeat.com/2020/05/06/how-microsoft-openai-and-oecd-are-putting-ai-ethics-principles-into-practice/ [ 7]: https://www.capgemini.com/2019/12/a-designers-view-on-ai-ethics-part-3-of-3/ [ 8]: https://venturebeat.com/2020/02/17/eus-new-ai-rules-will-focus-on-ethics-and-transparency/ [ 9]: https://venturebeat.com/2020/09/28/amsterdam-and-helsinki-launch-algorithm-registries-to-bring-transparency-to-public-deployments-of-ai/ [ 10]: https://events.venturebeat.com/transform2021/ [ 11]: https://venturebeat.com/venturebeat-membership-plans/ 
Classification
Language: English
Publication-Type: Web Blog
Journal Code: VNTR-6695
Subject: ARTIFICIAL INTELLIGENCE ETHICS (90%); FINANCIAL MARKET UPDATES (90%); STANDARDS & MEASUREMENTS (90%); AGENCY RULEMAKING (89%); ARTIFICIAL INTELLIGENCE (89%); CORPORATE GOVERNANCE (89%); ETHICS (89%); REGULATORY COMPLIANCE (89%); BLOGS & MESSAGE BOARDS (78%); BUSINESS ETHICS (78%); BUSINESS ETHICS & CORPORATE CITIZENSHIP (78%); PUBLIC POLICY (78%); SELF REGULATING ORGANIZATIONS (78%); TRENDS (75%); COVID CORONAVIRUS (74%); COVID-19 CORONAVIRUS (74%); INFECTIOUS DISEASE (74%); EUROPEAN UNION (70%); artificial intelligence (%); Big Data (%); AI (%); technology (%); StrategyR (%); category-/Business & Industrial/Business Operations (%); Dev (%); machine learning (%); ai (%); governance (%); Management (%); VB Home Page (%); Enterprise (%); services (%); software (%); Data (%); Cloud (%); Software (%)
Company:  KPMG (85%);  DELOITTE LLP (58%);  PEGASYSTEMS INC (56%);  CAPGEMINI SA (52%)
Organization: UNIVERSITY OF CALIFORNIA BERKELEY (54%)
Ticker: PEGA (NASDAQ) (56%); CAP (PAR) (52%)
Industry: NAICS541211 OFFICES OF CERTIFIED PUBLIC ACCOUNTANTS (85%); SIC8721 ACCOUNTING, AUDITING, & BOOKKEEPING SERVICES (85%); NAICS518210 DATA PROCESSING, HOSTING & RELATED SERVICES (56%); NAICS511210 SOFTWARE PUBLISHERS (56%); SIC7372 PREPACKAGED SOFTWARE (56%); NAICS551112 OFFICES OF OTHER HOLDING COMPANIES (52%); NAICS541512 COMPUTER SYSTEMS DESIGN SERVICES (52%); NAICS541511 CUSTOM COMPUTER PROGRAMMING SERVICES (52%); SIC7379 COMPUTER RELATED SERVICES, NEC (52%); SIC7373 COMPUTER INTEGRATED SYSTEMS DESIGN (52%); SIC7371 COMPUTER PROGRAMMING SERVICES (52%); SIC6719 OFFICES OF HOLDING COMPANIES, NEC (52%); ARTIFICIAL INTELLIGENCE ETHICS (90%); BANKING & FINANCE (90%); FINANCIAL MARKET UPDATES (90%); LOW CODE & NO CODE PLATFORMS (90%); ACCOUNTING & AUDITING FIRMS (89%); ARTIFICIAL INTELLIGENCE (89%); INFORMATION TECHNOLOGY INDUSTRY (79%); SOFTWARE SERVICES & APPLICATIONS (79%); BLOGS & MESSAGE BOARDS (78%); DATA GOVERNANCE & STEWARDSHIP (78%); INFORMATION SECURITY & PRIVACY (78%); AGILE DEVELOPMENT (72%)
Geographic: SAN FRANCISCO BAY AREA, CA, USA (74%); AMSTERDAM, NETHERLANDS (51%); CALIFORNIA, USA (79%); UNITED STATES (79%); NETHERLANDS (78%); FINLAND (66%); EUROPEAN UNION MEMBER STATES (51%)
Load-Date: August 25, 2021",neutral,0.5100160241127014,balanced/neutral,"['privacy', 'fairness', 'transparency', 'explainability', 'accountability', 'security', 'agency', 'access']",['fairness'],"['regulation', 'policy', 'governance', 'oversight', 'standards', 'framework', 'compliance', 'need to', 'advocate']","['machine learning', 'algorithm']",8,1,9,2
2021,Unknown Title,"Body
Link to Image
Link to Story
While potentially offering tremendous benefit, the broad applicability of AI across society must be handled carefully and professional accountants have a key role to play.
ACCA (the Association of Chartered Certified Accountants) and Chartered Accountants Australia and New Zealand (CA ANZ) reveal in a new report the pressing need for the accountancy profession to make the necessary connections between Artificial Intelligence (AI) and its relationship to environmental, social and governance (ESG) dimensions.
Polling over 5,700 respondents across 21 countries and geographies, the research reveals a cautious tone, with fewer than half (43%) believing that the impact of AI on their rights as an individual is positive - such as safety and personal security, and levels of fairness, choice and transparency.
ACCA and CA ANZ say in Ethics for sustainable AI adoption: Connecting AI and ESG that professional accountants, with their explicit and long-standing commitment to ethical practices, are well placed to guide organisations along a responsible path for AI adoption.
This presents a wake-up call for the accountancy profession to lead the way and become the super connectors needed to ensure an ethical approach. Their management of the transition to mass usage of AI in an ethical, responsible manner is essential if sustainable long-term value is to be secured from it.
The report's nine recommendations include the need to set tone at the top on AI adoption by prioritising an approach that is consistent with organisational values such as diversity and inclusion in considering the impact of AI on under-represented groups, or fairness when it comes to recruitment or surveillance of employees; and transparency such as appropriately disclosing AI use to customers.
Another recommendation for the profession is to challenge greenwashing and seek insights from AI tools to help with professional scepticism in examining whether the organisation's claims about sustainability, such as on achieving net zero targets, are matched by its performance. Suspect claims need to be challenged.
Helen Brand, chief executive of ACCA says: 'AI adoption must consider the needs of all, especially the under-represented and vulnerable in society. That's why one of our recommendations is to ensure the profession exercises its professional judgement, because AI may create previously unseen situations. We recommend that professional accountants need to avoid over-reliance on simplistic checklist-based approaches which don't give the full picture or leave space for unintended consequences.'
Ainslie van Onselen, chief executive of CA ANZ adds: 'Our report found that in order to ethically and sustainably adopt AI, organisations need effective governance mechanisms. This starts with setting the right tone and culture at the top and covers a range of areas from oversight and delivery procedures, to regulation and data governance. AI is a strategic endeavour that should be spearheaded by leaders who know and execute on the difference between what we have a right to do and what is the right thing to do. It's important to build knowledge and skills at the intersection of AI, ethics and sustainable development. This aligns well to the accountancy profession which can play a key role in driving responsible adoption.'
Key findings from the research amongst accountancy and finance professionals show that:
• 66% believe that their leaders prioritise ethics as highly as profits.
• 64% believe that the impact of AI on overall standard of living in society is positive, but only half that proportion (32%) consider its impact on levels of inequality to be positive. On the latter with 28% recording negative impact, the net positive balance was just 4%.
• Three in four reports being effective / very effective at managing confidentiality, and two in three at managing data quality.
• Just over half (51%) believe that the impact of AI on their ability to live according to their values is positive.31% are aware of AI use within their industry. Fewer than half (48%) have a basic understanding of how an AI algorithm works.
The global accountancy profession is bound by the Code of Ethics (the 'Code') and its five fundamental principles as set out by the International Ethics Standards Board for Accountants (IESBA). These are integrity, objectivity, professional competence and due care, confidentiality, and professional behaviour.
Ethics for sustainable AI adoption: Connecting AI and ESG can be downloaded here:
MENAFN23082021005332011918ID1102665829
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: PROFESSIONAL WORKERS (92%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); ESG FACTORS (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (90%); ASSOCIATIONS & ORGANIZATIONS (89%); GREENWASHING (78%); ESG FACTORS - GOVERNANCE (76%); SAFETY (75%); SURVEILLANCE (72%); EXECUTIVES (71%); DIVERSITY & INCLUSION (50%)
Organization: ASSOCIATION OF CHARTERED CERTIFIED ACCOUNTANTS (92%)
Industry: ACCOUNTING (90%); ARTIFICIAL INTELLIGENCE (90%); GREENWASHING (78%); ACCOUNTING & AUDITING FIRMS (73%); DATA GOVERNANCE & STEWARDSHIP (69%)
Geographic: AUSTRALIA & NEW ZEALAND (79%); NEW ZEALAND (79%); AUSTRALIA (78%)
Load-Date: September 1, 2021","Link to Image
Link to Story
While potentially offering tremendous benefit, the broad applicability of AI across society must be handled carefully and professional accountants have a key role to play.
ACCA (the Association of Chartered Certified Accountants) and Chartered Accountants Australia and New Zealand (CA ANZ) reveal in a new report the pressing need for the accountancy profession to make the necessary connections between Artificial Intelligence (AI) and its relationship to environmental, social and governance (ESG) dimensions.
Polling over 5,700 respondents across 21 countries and geographies, the research reveals a cautious tone, with fewer than half (43%) believing that the impact of AI on their rights as an individual is positive - such as safety and personal security, and levels of fairness, choice and transparency.
ACCA and CA ANZ say in Ethics for sustainable AI adoption: Connecting AI and ESG that professional accountants, with their explicit and long-standing commitment to ethical practices, are well placed to guide organisations along a responsible path for AI adoption.
This presents a wake-up call for the accountancy profession to lead the way and become the super connectors needed to ensure an ethical approach. Their management of the transition to mass usage of AI in an ethical, responsible manner is essential if sustainable long-term value is to be secured from it.
The report's nine recommendations include the need to set tone at the top on AI adoption by prioritising an approach that is consistent with organisational values such as diversity and inclusion in considering the impact of AI on under-represented groups, or fairness when it comes to recruitment or surveillance of employees; and transparency such as appropriately disclosing AI use to customers.
Another recommendation for the profession is to challenge greenwashing and seek insights from AI tools to help with professional scepticism in examining whether the organisation's claims about sustainability, such as on achieving net zero targets, are matched by its performance. Suspect claims need to be challenged.
Helen Brand, chief executive of ACCA says: 'AI adoption must consider the needs of all, especially the under-represented and vulnerable in society. That's why one of our recommendations is to ensure the profession exercises its professional judgement, because AI may create previously unseen situations. We recommend that professional accountants need to avoid over-reliance on simplistic checklist-based approaches which don't give the full picture or leave space for unintended consequences.'
Ainslie van Onselen, chief executive of CA ANZ adds: 'Our report found that in order to ethically and sustainably adopt AI, organisations need effective governance mechanisms. This starts with setting the right tone and culture at the top and covers a range of areas from oversight and delivery procedures, to regulation and data governance. AI is a strategic endeavour that should be spearheaded by leaders who know and execute on the difference between what we have a right to do and what is the right thing to do. It's important to build knowledge and skills at the intersection of AI, ethics and sustainable development. This aligns well to the accountancy profession which can play a key role in driving responsible adoption.'
Key findings from the research amongst accountancy and finance professionals show that:
• 66% believe that their leaders prioritise ethics as highly as profits.
• 64% believe that the impact of AI on overall standard of living in society is positive, but only half that proportion (32%) consider its impact on levels of inequality to be positive. On the latter with 28% recording negative impact, the net positive balance was just 4%.
• Three in four reports being effective / very effective at managing confidentiality, and two in three at managing data quality.
• Just over half (51%) believe that the impact of AI on their ability to live according to their values is positive.31% are aware of AI use within their industry. Fewer than half (48%) have a basic understanding of how an AI algorithm works.
The global accountancy profession is bound by the Code of Ethics (the 'Code') and its five fundamental principles as set out by the International Ethics Standards Board for Accountants (IESBA). These are integrity, objectivity, professional competence and due care, confidentiality, and professional behaviour.
Ethics for sustainable AI adoption: Connecting AI and ESG can be downloaded here:
MENAFN23082021005332011918ID1102665829
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: PROFESSIONAL WORKERS (92%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); ESG FACTORS (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (90%); ASSOCIATIONS & ORGANIZATIONS (89%); GREENWASHING (78%); ESG FACTORS - GOVERNANCE (76%); SAFETY (75%); SURVEILLANCE (72%); EXECUTIVES (71%); DIVERSITY & INCLUSION (50%)
Organization: ASSOCIATION OF CHARTERED CERTIFIED ACCOUNTANTS (92%)
Industry: ACCOUNTING (90%); ARTIFICIAL INTELLIGENCE (90%); GREENWASHING (78%); ACCOUNTING & AUDITING FIRMS (73%); DATA GOVERNANCE & STEWARDSHIP (69%)
Geographic: AUSTRALIA & NEW ZEALAND (79%); NEW ZEALAND (79%); AUSTRALIA (78%)
Load-Date: September 1, 2021",neutral,0.7531618475914001,balanced/neutral,"['surveillance', 'fairness', 'transparency', 'safety', 'security', 'inequality']",['fairness'],"['regulation', 'governance', 'oversight', 'standards', 'should', 'must', 'need to', 'recommend']",['algorithm'],6,1,8,1
2021,Unknown Title,"Dateline: India 
Body
India, April 24 -- The Department of Foreign Affairs and Trade (DFAT) has awarded the Centre for International Security Studies (CISS) $800,000 over two years to develop accords for the ethical use of quantum technologies.
Announced by the Honourable Marise Payne, Minister for Foreign Affairs, the grant is part of the Australia-India Cyber and Critical Technology Partnership which promotes a free, open and rules-based Indo-Pacific region.
""This is the moment to address the social, ethical and geopolitical implications of a quantum future,"" said CISS Director Professor James Der Derian.
The Quantum Meta-Ethics project will be led by Professor Der Derian, in partnership with a team under the direction of Dr Rajeswari Rajagopalan at the Delhi-based Observer Research Foundation (ORF). The two organisations will be joined by leading experts from the Indo-Pacific region, including: quantum physicist Shohini Ghose, global strategy advisor Parag Khanna, and political scientist Nisha Shah. The project will build upon the work of Professor Der Derian's existing research Project Q: Peace and Security in a Quantum Age, funded by the Carnegie Corporation of New York.
Over the next two years, Quantum Meta-Ethics will initiate conversations about what constitutes ethical or unethical behaviour, good or bad practices, productive or destructive applications of emerging quantum technologies. The project will bring together academics, business leaders, government and military representatives, legal and policy experts, to develop world-first quantum accords that will inform international governance of quantum technologies.
The potential benefits of quantum innovation stretch across multiple socioeconomic sectors. They include new drug development and discovery, assisting the creation of provably secure communications systems, modelling of financial markets and building quantum sensors.
If realised, quantum computers promise to revolutionise information technology by solving problems beyond the scope of classical computers in fields as diverse as cryptography, medicine, finance, artificial intelligence and logistics.
""But there are also new dangers ahead,"" said Professor Der Derian.
States, tech companies and universities are already jostling for quantum advantage, with serious societal and geopolitical implications. A quantum race is ratcheting up in the Indo-Pacific, in vital security areas of encryption and decryption, surveillance and targeting, and most alarmingly, weaponised artificial intelligence.
""From the nuclear, space and information technology races, we have learned, often too late, what can happen when ethics, norms and governance lag behind,"" Professor Der Derian said. ""It is time to apply the lessons learned from those races but also to recognise the unique qualities of emerging quantum technologies. No single technology, event or state will define the quantum future; nor can any single ethical framework be applied to all actors and all quantum technologies across all stages of development.""
""Rather than proposing a formal set of universal rules, the project will seek a consensus among stakeholders on ethics, best practices and progressive applications of quantum technologies. As a theory of reality and enabler of new technologies, quantum increasingly touches everything.""
Published by HT Digital Content Services with permission from India Education Diary. For any query with respect to this article or any other content requirement, please contact Editor at contentservices@htlive.com
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (93%); INTERNATIONAL RELATIONS & NATIONAL SECURITY (91%); COMMERCE DEPARTMENTS (90%); FOREIGN POLICY (90%); RESEARCH INSTITUTES (90%); STATE DEPARTMENTS & FOREIGN SERVICES (90%); COLLEGE & UNIVERSITY PROFESSORS (89%); EMERGING TECHNOLOGY (89%); QUANTUM COMPUTING (89%); CRYPTOLOGY (78%); ECONOMICS (78%); FOUNDATIONS (78%); GOVERNMENT & PUBLIC ADMINISTRATION (78%); GOVERNMENT ADVISORS & MINISTERS (78%); POLITICAL SCIENCE (78%); PUBLIC POLICY (78%); QUANTUM MECHANICS (78%); SCIENCE & TECHNOLOGY (78%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (78%); SURVEILLANCE (78%); RESEARCH & DEVELOPMENT (77%); ALLIANCES & PARTNERSHIPS (76%); ARTIFICIAL INTELLIGENCE (76%); BEST PRACTICES (76%); DRUG DESIGN & DISCOVERY (72%); ASSOCIATIONS & ORGANIZATIONS (70%); FINANCIAL MARKETS & INVESTING (66%)
Organization: CARNEGIE CORPORATION OF NEW YORK (55%)
Industry: COLLEGE & UNIVERSITY PROFESSORS (89%); COMPUTING & INFORMATION TECHNOLOGY (89%); INFORMATION TECHNOLOGY INDUSTRY (89%); QUANTUM COMPUTING (89%); CRYPTOLOGY (78%); ARTIFICIAL INTELLIGENCE (76%); BANKING & FINANCE REGULATION & POLICY (76%); PHARMACEUTICALS INDUSTRY (74%); BANKING & FINANCE (73%); DRUG DESIGN & DISCOVERY (72%); FINANCIAL MARKETS & INVESTING (66%); ONLINE SECURITY & PRIVACY (63%)
Geographic: INDIA (93%)
Load-Date: April 24, 2021","India, April 24 -- The Department of Foreign Affairs and Trade (DFAT) has awarded the Centre for International Security Studies (CISS) $800,000 over two years to develop accords for the ethical use of quantum technologies.
Announced by the Honourable Marise Payne, Minister for Foreign Affairs, the grant is part of the Australia-India Cyber and Critical Technology Partnership which promotes a free, open and rules-based Indo-Pacific region.
""This is the moment to address the social, ethical and geopolitical implications of a quantum future,"" said CISS Director Professor James Der Derian.
The Quantum Meta-Ethics project will be led by Professor Der Derian, in partnership with a team under the direction of Dr Rajeswari Rajagopalan at the Delhi-based Observer Research Foundation (ORF). The two organisations will be joined by leading experts from the Indo-Pacific region, including: quantum physicist Shohini Ghose, global strategy advisor Parag Khanna, and political scientist Nisha Shah. The project will build upon the work of Professor Der Derian's existing research Project Q: Peace and Security in a Quantum Age, funded by the Carnegie Corporation of New York.
Over the next two years, Quantum Meta-Ethics will initiate conversations about what constitutes ethical or unethical behaviour, good or bad practices, productive or destructive applications of emerging quantum technologies. The project will bring together academics, business leaders, government and military representatives, legal and policy experts, to develop world-first quantum accords that will inform international governance of quantum technologies.
The potential benefits of quantum innovation stretch across multiple socioeconomic sectors. They include new drug development and discovery, assisting the creation of provably secure communications systems, modelling of financial markets and building quantum sensors.
If realised, quantum computers promise to revolutionise information technology by solving problems beyond the scope of classical computers in fields as diverse as cryptography, medicine, finance, artificial intelligence and logistics.
""But there are also new dangers ahead,"" said Professor Der Derian.
States, tech companies and universities are already jostling for quantum advantage, with serious societal and geopolitical implications. A quantum race is ratcheting up in the Indo-Pacific, in vital security areas of encryption and decryption, surveillance and targeting, and most alarmingly, weaponised artificial intelligence.
""From the nuclear, space and information technology races, we have learned, often too late, what can happen when ethics, norms and governance lag behind,"" Professor Der Derian said. ""It is time to apply the lessons learned from those races but also to recognise the unique qualities of emerging quantum technologies. No single technology, event or state will define the quantum future; nor can any single ethical framework be applied to all actors and all quantum technologies across all stages of development.""
""Rather than proposing a formal set of universal rules, the project will seek a consensus among stakeholders on ethics, best practices and progressive applications of quantum technologies. As a theory of reality and enabler of new technologies, quantum increasingly touches everything.""
Published by HT Digital Content Services with permission from India Education Diary. For any query with respect to this article or any other content requirement, please contact Editor at contentservices@htlive.com
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (93%); INTERNATIONAL RELATIONS & NATIONAL SECURITY (91%); COMMERCE DEPARTMENTS (90%); FOREIGN POLICY (90%); RESEARCH INSTITUTES (90%); STATE DEPARTMENTS & FOREIGN SERVICES (90%); COLLEGE & UNIVERSITY PROFESSORS (89%); EMERGING TECHNOLOGY (89%); QUANTUM COMPUTING (89%); CRYPTOLOGY (78%); ECONOMICS (78%); FOUNDATIONS (78%); GOVERNMENT & PUBLIC ADMINISTRATION (78%); GOVERNMENT ADVISORS & MINISTERS (78%); POLITICAL SCIENCE (78%); PUBLIC POLICY (78%); QUANTUM MECHANICS (78%); SCIENCE & TECHNOLOGY (78%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (78%); SURVEILLANCE (78%); RESEARCH & DEVELOPMENT (77%); ALLIANCES & PARTNERSHIPS (76%); ARTIFICIAL INTELLIGENCE (76%); BEST PRACTICES (76%); DRUG DESIGN & DISCOVERY (72%); ASSOCIATIONS & ORGANIZATIONS (70%); FINANCIAL MARKETS & INVESTING (66%)
Organization: CARNEGIE CORPORATION OF NEW YORK (55%)
Industry: COLLEGE & UNIVERSITY PROFESSORS (89%); COMPUTING & INFORMATION TECHNOLOGY (89%); INFORMATION TECHNOLOGY INDUSTRY (89%); QUANTUM COMPUTING (89%); CRYPTOLOGY (78%); ARTIFICIAL INTELLIGENCE (76%); BANKING & FINANCE REGULATION & POLICY (76%); PHARMACEUTICALS INDUSTRY (74%); BANKING & FINANCE (73%); DRUG DESIGN & DISCOVERY (72%); FINANCIAL MARKETS & INVESTING (66%); ONLINE SECURITY & PRIVACY (63%)
Geographic: INDIA (93%)
Load-Date: April 24, 2021",neutral,0.6885348558425903,balanced/neutral,"['privacy', 'surveillance', 'security']",[],"['regulation', 'policy', 'governance', 'framework']",[],3,0,4,0
2021,Unknown Title,"Byline: States News Service
Dateline: MOSCOW, Russia 
Body
The following information was released by Sberbank:
In the framework of the international forum Ethics of Artificial Intelligence: The Beginning of Trust, Russia's largest tech companies adopted an AI code of ethics, which was drafted by the Artificial Intelligence Alliance. The document was signed by Sber, Yandex, MTS, VK, Gazprom Neft, and the Russian Direct Investment Fund (RDIF), as well as leading companies and research organizations. First Deputy Chief of Staff of the Presidential Executive Office Sergey Kiriyenko and Deputy Prime Minister Dmitry Chernyshenko were present at the signing.
The code of ethics enshrines a human-centric and humanistic approach to the development of AI technology, the principles of non-discrimination, data security, and information security, identification of AI in communication with humans, respect for human autonomy, and responsibility for the consequences of using AI. Commitment to the code of ethics will be a key social responsibility element for companies developing and implementing AI technology in Russia.
The AI Code of Ethics is to become a recommendation document on ethics for all AI market participants: government, business, Russian and foreign developers. The code establishes general ethical principles and behavioral standards, which the participants may choose to adhere to in the field of artificial intelligence.
As a tool for implementing the aforementioned principles and provisions, the code provides for the creation of a National AI Ethics Commission, the appointment of ethics commissioners, as well as the possibility of creating collegial industry ethics bodies. The Alliance will also develop methodological recommendations and a set of best and/or worst practices for dealing with emerging ethical issues in the AI life cycle.
Alexander Vedyakhin, first deputy chairman of the executive board, Sberbank:
""Many people consider AI technology to be controversial: on the one hand, it is a powerful driver of development for all of humanity; on the other hand, AI has a certain degree of autonomy in its decision-making, and it is not always obvious how these decisions are made. Today we have before us a great challenge developing trust in AI technology. This is impossible without defining the rules for its ethical use. I am certain that the code we have adopted, in the drafting of which Sber has taken an active part, will be an important step in building trust in AI among our citizens and, as a result, in the development of the entire domestic artificial intelligence industry.""
Vyacheslav Nikolaev, president, MTS:
""In the near future, it is solutions based on AI technology that will provide a real breakthrough in terms of the digitalization of healthcare and education and the realization of everyday tasks. At the same time, we can confidently say that AI is the technology that causes the most alarm in society. Now is the time to define the principles that will help us use AI technology with maximum benefit to society. At MTS, we have always paid particular attention to ethical issues in the development of products, including those that use artificial intelligence, and we are happy to share this experience during the preparation of the Russian AI Code of Ethics.""
Anatoly Braverman, first deputy CEO, Russian Direct Investment Fund (RDIF):
""The shaping and development of the AI market requires effective interaction between its participants. The joint preparation, adoption, and future application of the AI Code of Ethics is an important stage in the formation of key infrastructure tools and the creation of success stories for developers, users, and investors in the field of artificial intelligence. Together with its co-investors and portfolio companies, RDIF has traditionally paid great attention to ethics and reputation issues, and it is ready to actively share its own best practices and experience and adopt those of others, too.""
The document was prepared with the support of the Presidential Executive Office of the Russian Federation, the Analytical Center for the Government of the Russian Federation, and the Ministry of Economic Development. It underwent expert and public discussion on platforms hosted by Digital Economy, the Federation Council, and the Civic Chamber.
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ETHICS (97%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (89%); BOARDS OF DIRECTORS (78%); BUSINESS ETHICS (78%); BUSINESS ETHICS & CORPORATE CITIZENSHIP (78%); FUNDS & INVESTMENT TRUSTS (78%); GOVERNMENT & PUBLIC ADMINISTRATION (76%); HEADS OF STATE & GOVERNMENT (76%); NEGATIVE SOCIETAL NEWS (74%); ESG FACTORS - SOCIAL (73%); APPOINTMENTS (71%); ASSOCIATIONS & ORGANIZATIONS (71%); PRIME MINISTERS (70%)
Company:  OAO GAZPROM NEFT (72%);  SBERBANK ROSSII OAO (58%);  YANDEX NV (58%)
Ticker: SIBN (RTS) (72%); GAZ (LSE) (72%); SBER (RTS) (58%); SBER (LSE) (58%); YNDX (NASDAQ) (58%)
Industry: NAICS211130 NATURAL GAS EXTRACTION (72%); NAICS211120 CRUDE PETROLEUM EXTRACTION (72%); SIC1311 CRUDE PETROLEUM & NATURAL GAS (72%); NAICS522293 INTERNATIONAL TRADE FINANCING (58%); NAICS522110 COMMERCIAL BANKING (58%); SIC6082 FOREIGN TRADE & INTERNATIONAL BANKING INSTITUTIONS (58%); SIC6021 NATIONAL COMMERCIAL BANKS (58%); NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (58%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); BIG TECH (90%); INFORMATION TECHNOLOGY INDUSTRY (90%); DATA SECURITY (78%); DIGITALIZATION & DIGITAL TRANSFORMATION (78%); FUNDS & INVESTMENT TRUSTS (78%); INFORMATION SECURITY & PRIVACY (78%)
Geographic: MOSCOW, RUSSIA (74%); RUSSIA (91%)
Load-Date: October 27, 2021","The following information was released by Sberbank:
In the framework of the international forum Ethics of Artificial Intelligence: The Beginning of Trust, Russia's largest tech companies adopted an AI code of ethics, which was drafted by the Artificial Intelligence Alliance. The document was signed by Sber, Yandex, MTS, VK, Gazprom Neft, and the Russian Direct Investment Fund (RDIF), as well as leading companies and research organizations. First Deputy Chief of Staff of the Presidential Executive Office Sergey Kiriyenko and Deputy Prime Minister Dmitry Chernyshenko were present at the signing.
The code of ethics enshrines a human-centric and humanistic approach to the development of AI technology, the principles of non-discrimination, data security, and information security, identification of AI in communication with humans, respect for human autonomy, and responsibility for the consequences of using AI. Commitment to the code of ethics will be a key social responsibility element for companies developing and implementing AI technology in Russia.
The AI Code of Ethics is to become a recommendation document on ethics for all AI market participants: government, business, Russian and foreign developers. The code establishes general ethical principles and behavioral standards, which the participants may choose to adhere to in the field of artificial intelligence.
As a tool for implementing the aforementioned principles and provisions, the code provides for the creation of a National AI Ethics Commission, the appointment of ethics commissioners, as well as the possibility of creating collegial industry ethics bodies. The Alliance will also develop methodological recommendations and a set of best and/or worst practices for dealing with emerging ethical issues in the AI life cycle.
Alexander Vedyakhin, first deputy chairman of the executive board, Sberbank:
""Many people consider AI technology to be controversial: on the one hand, it is a powerful driver of development for all of humanity; on the other hand, AI has a certain degree of autonomy in its decision-making, and it is not always obvious how these decisions are made. Today we have before us a great challenge developing trust in AI technology. This is impossible without defining the rules for its ethical use. I am certain that the code we have adopted, in the drafting of which Sber has taken an active part, will be an important step in building trust in AI among our citizens and, as a result, in the development of the entire domestic artificial intelligence industry.""
Vyacheslav Nikolaev, president, MTS:
""In the near future, it is solutions based on AI technology that will provide a real breakthrough in terms of the digitalization of healthcare and education and the realization of everyday tasks. At the same time, we can confidently say that AI is the technology that causes the most alarm in society. Now is the time to define the principles that will help us use AI technology with maximum benefit to society. At MTS, we have always paid particular attention to ethical issues in the development of products, including those that use artificial intelligence, and we are happy to share this experience during the preparation of the Russian AI Code of Ethics.""
Anatoly Braverman, first deputy CEO, Russian Direct Investment Fund (RDIF):
""The shaping and development of the AI market requires effective interaction between its participants. The joint preparation, adoption, and future application of the AI Code of Ethics is an important stage in the formation of key infrastructure tools and the creation of success stories for developers, users, and investors in the field of artificial intelligence. Together with its co-investors and portfolio companies, RDIF has traditionally paid great attention to ethics and reputation issues, and it is ready to actively share its own best practices and experience and adopt those of others, too.""
The document was prepared with the support of the Presidential Executive Office of the Russian Federation, the Analytical Center for the Government of the Russian Federation, and the Ministry of Economic Development. It underwent expert and public discussion on platforms hosted by Digital Economy, the Federation Council, and the Civic Chamber.
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ETHICS (97%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (89%); BOARDS OF DIRECTORS (78%); BUSINESS ETHICS (78%); BUSINESS ETHICS & CORPORATE CITIZENSHIP (78%); FUNDS & INVESTMENT TRUSTS (78%); GOVERNMENT & PUBLIC ADMINISTRATION (76%); HEADS OF STATE & GOVERNMENT (76%); NEGATIVE SOCIETAL NEWS (74%); ESG FACTORS - SOCIAL (73%); APPOINTMENTS (71%); ASSOCIATIONS & ORGANIZATIONS (71%); PRIME MINISTERS (70%)
Company:  OAO GAZPROM NEFT (72%);  SBERBANK ROSSII OAO (58%);  YANDEX NV (58%)
Ticker: SIBN (RTS) (72%); GAZ (LSE) (72%); SBER (RTS) (58%); SBER (LSE) (58%); YNDX (NASDAQ) (58%)
Industry: NAICS211130 NATURAL GAS EXTRACTION (72%); NAICS211120 CRUDE PETROLEUM EXTRACTION (72%); SIC1311 CRUDE PETROLEUM & NATURAL GAS (72%); NAICS522293 INTERNATIONAL TRADE FINANCING (58%); NAICS522110 COMMERCIAL BANKING (58%); SIC6082 FOREIGN TRADE & INTERNATIONAL BANKING INSTITUTIONS (58%); SIC6021 NATIONAL COMMERCIAL BANKS (58%); NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (58%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); BIG TECH (90%); INFORMATION TECHNOLOGY INDUSTRY (90%); DATA SECURITY (78%); DIGITALIZATION & DIGITAL TRANSFORMATION (78%); FUNDS & INVESTMENT TRUSTS (78%); INFORMATION SECURITY & PRIVACY (78%)
Geographic: MOSCOW, RUSSIA (74%); RUSSIA (91%)
Load-Date: October 27, 2021",neutral,0.8009227514266968,balanced/neutral,"['privacy', 'discrimination', 'security', 'autonomy']",['autonomy'],"['standards', 'framework']",[],4,1,2,0
2021,Unknown Title,"Highlight: IBM told Reuters its AI Ethics Board has begun discussing how to police an emerging frontier: implants and wearables that wire computers to brains.
Body
In September last year, Google's cloud unit looked into using artificial intelligence to help a financial firm decide whom to lend money to.
It turned down the client's idea after weeks of internal discussions, deeming the project too ethically dicey because the AI technology could perpetuate biases like those around race and gender.
Since early last year, Google has also blocked new AI features analysing emotions, fearing cultural insensitivity, while Microsoft restricted software mimicking voices and IBM rejected a client request for an advanced facial-recognition system.
All these technologies were curbed by panels of executives or other leaders, according to interviews with AI ethics chiefs at the three US technology giants.
Reuters reported for the first time their vetoes and the deliberations that led to them reflect a nascent industry-wide drive to balance the pursuit of lucrative AI systems with a greater consideration of social responsibility.
""There are opportunities and harms, and our job is to maximize opportunities and minimize harms,"" said Tracy Pizzo Frey, who sits on two ethics committees at Google Cloud as its managing director for Responsible AI.
Regulating AI
Microsoft, for instance, had to balance the benefit of using its voice mimicry tech to restore impaired people's speech against risks such as enabling political deepfakes, said Natasha Crampton, the company's chief responsible AI officer.
Rights activists say decisions with potentially broad consequences for society should not be made internally alone. They argue ethics committees cannot be truly independent and their public transparency is limited by competitive pressures.
Jascha Galaski, advocacy officer at Civil Liberties Union for Europe, views external oversight as the way forward, and US and European authorities are indeed drawing rules for the fledgling area.
If companies' AI ethics committees ""really become transparent and independent – and this is all very utopist – then this could be even better than any other solution, but I don't think it's realistic,"" Galaski said.
The companies said they would welcome clear regulation on the use of AI, and that this was essential both for customer and public confidence, akin to car safety rules. They said it was also in their financial interests to act responsibly.
They are keen, though, for any rules to be flexible enough to keep up with innovation and the new dilemmas it creates.
Among complex considerations to come, IBM told Reuters its AI Ethics Board has begun discussing how to police an emerging frontier: implants and wearables that wire computers to brains.
Such neurotechnologies could help impaired people control movement but raise concerns such as the prospect of hackers manipulating thoughts, said IBM Chief Privacy Officer Christina Montgomery.
Ethics committees
Tech companies acknowledge that just five years ago they were launching AI services such as chatbots and photo-tagging with few ethical safeguards, and tackling misuse or biased results with subsequent updates.
But as political and public scrutiny of AI failings grew, Microsoft in 2017 and Google and IBM in 2018 established ethics committees to review new services from the start.
Google said it was presented with its money-lending quandary last September when a financial services company figured AI could assess people's creditworthiness better than other methods.
The project appeared well-suited for Google Cloud, whose expertise in developing AI tools that help in areas such as detecting abnormal transactions has attracted clients like Deutsche Bank, HSBC and BNY Mellon.
                                              Bill Foster                                                                   US Congressman                                  
Google's unit anticipated AI-based credit scoring could become a market worth billions of dollars a year and wanted a foothold.
However, its ethics committee of about 20 managers, social scientists and engineers who review potential deals unanimously voted against the project at an October meeting, Pizzo Frey said.
The AI system would need to learn from past data and patterns, the committee concluded, and thus risked repeating discriminatory practices from around the world against people of colour and other marginalized groups.
What's more the committee, internally known as ""Lemonaid,"" enacted a policy to skip all financial services deals related to creditworthiness until such concerns could be resolved.
Google also said its second Cloud ethics committee, known as Iced Tea, this year placed under review a service released in 2015 for categorising photos of people by four expressions: joy, sorrow, anger and surprise.
The move followed a ruling last year by Google's company-wide ethics panel, the Advanced Technology Review Council (ATRC), holding back new services related to reading emotion.
The ATRC - over a dozen top executives and engineers - determined that inferring emotions could be insensitive because facial cues are associated differently with feelings across cultures, among other reasons, said Jen Gennai, founder and lead of Google's Responsible Innovation team.
Reproducing voices and faces
Microsoft, meanwhile, developed software that could reproduce someone's voice from a short sample, but the company's Sensitive Uses panel then spent more than two years debating the ethics around its use and consulted company President Brad Smith, senior AI officer Crampton told Reuters.
She said the panel - specialists in fields such as human rights, data science and engineering - eventually gave the green light for Custom Neural Voice to be fully released in February this year. But it placed restrictions on its use, including that subjects' consent is verified and a team with ""Responsible AI Champs"" trained on corporate policy approve purchases.
IBM's AI board, comprising about 20 department leaders, wrestled with its own dilemma when early in the COVID-19 pandemic it examined a client request to customize facial-recognition technology to spot fevers and face coverings.
Montgomery said the board, which she co-chairs, declined the invitation, concluding that manual checks would suffice with less intrusion on privacy because photos would not be retained for any AI database.
Six months later, IBM announced it was discontinuing its face-recognition service.
What is the EU saying about it?
In an attempt to protect privacy and other freedoms, lawmakers in the European Union and the United States are pursuing far-reaching controls on AI systems.
The EU's Artificial Intelligence Act, on track to be passed next year, would bar real-time face recognition in public spaces and require tech companies to vet high-risk applications, such as those used in hiring, credit scoring and law enforcement.
US Congressman Bill Foster, who has held hearings on how algorithms carry forward discrimination in financial services and housing, said new laws to govern AI would ensure an even field for vendors.
""When you ask a company to take a hit in profits to accomplish societal goals, they say, 'What about our shareholders and our competitors?' That's why you need sophisticated regulation,"" the Democrat from Illinois said.
""There may be areas which are so sensitive that you will see tech firms staying out deliberately until there are clear rules of road"".
Indeed some AI advances may simply be on hold until companies can counter ethical risks without dedicating enormous engineering resources.
After Google Cloud turned down the request for custom financial AI last October, the Lemonaid committee told the sales team that the unit aims to start developing credit-related applications someday.
First, research into combating unfair biases must catch up with Google Cloud's ambitions to increase financial inclusion through the ""highly sensitive"" technology, it said in the policy circulated to staff.
""Until that time, we are not in a position to deploy solutions"".
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (95%); ARTIFICIAL INTELLIGENCE ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); DEEPFAKE TECHNOLOGY (90%); EXECUTIVES (89%); GENERATIVE AI (89%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (79%); CHATBOTS (79%); BUSINESS ETHICS & CORPORATE CITIZENSHIP (78%); COMPUTER CRIME (78%); EMOTIONS (78%); NEGATIVE NEWS (78%); SAFETY (78%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (78%); HUMAN RIGHTS ORGANIZATIONS (73%); BIOMETRICS (72%); HUMAN RIGHTS & CIVIL LIBERTIES LAW (71%); MANAGERS & SUPERVISORS (71%); SAFETY REGULATION & POLICY (71%); AUTOMOTIVE SAFETY (50%); Artificial intelligence (%); cloud-computing (%); Technology (%); Google (%); Microsoft (%); Computers (%)
Company: GOOGLE LLC (95%);  MICROSOFT CORP (57%);  AI SYSTEMS (56%)
Ticker: MSFT (NASDAQ) (57%)
Industry: NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (95%); SIC7372 PREPACKAGED SOFTWARE (57%); NAICS511210 SOFTWARE PUBLISHERS (57%); ARTIFICIAL INTELLIGENCE ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); BIG TECH (90%); DEEPFAKE TECHNOLOGY (90%); GENERATIVE AI (89%); INFORMATION TECHNOLOGY INDUSTRY (89%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (79%); CHATBOTS (79%); PATTERN RECOGNITION (79%); BANKING & FINANCE (78%); COMPUTER CRIME (78%); COMPUTER SOFTWARE (78%); AUTOMOTIVE SAFETY (50%)
Geographic: EUROPE (90%)
Load-Date: September 8, 2021","In September last year, Google's cloud unit looked into using artificial intelligence to help a financial firm decide whom to lend money to.
It turned down the client's idea after weeks of internal discussions, deeming the project too ethically dicey because the AI technology could perpetuate biases like those around race and gender.
Since early last year, Google has also blocked new AI features analysing emotions, fearing cultural insensitivity, while Microsoft restricted software mimicking voices and IBM rejected a client request for an advanced facial-recognition system.
All these technologies were curbed by panels of executives or other leaders, according to interviews with AI ethics chiefs at the three US technology giants.
Reuters reported for the first time their vetoes and the deliberations that led to them reflect a nascent industry-wide drive to balance the pursuit of lucrative AI systems with a greater consideration of social responsibility.
""There are opportunities and harms, and our job is to maximize opportunities and minimize harms,"" said Tracy Pizzo Frey, who sits on two ethics committees at Google Cloud as its managing director for Responsible AI.
Regulating AI
Microsoft, for instance, had to balance the benefit of using its voice mimicry tech to restore impaired people's speech against risks such as enabling political deepfakes, said Natasha Crampton, the company's chief responsible AI officer.
Rights activists say decisions with potentially broad consequences for society should not be made internally alone. They argue ethics committees cannot be truly independent and their public transparency is limited by competitive pressures.
Jascha Galaski, advocacy officer at Civil Liberties Union for Europe, views external oversight as the way forward, and US and European authorities are indeed drawing rules for the fledgling area.
If companies' AI ethics committees ""really become transparent and independent – and this is all very utopist – then this could be even better than any other solution, but I don't think it's realistic,"" Galaski said.
The companies said they would welcome clear regulation on the use of AI, and that this was essential both for customer and public confidence, akin to car safety rules. They said it was also in their financial interests to act responsibly.
They are keen, though, for any rules to be flexible enough to keep up with innovation and the new dilemmas it creates.
Among complex considerations to come, IBM told Reuters its AI Ethics Board has begun discussing how to police an emerging frontier: implants and wearables that wire computers to brains.
Such neurotechnologies could help impaired people control movement but raise concerns such as the prospect of hackers manipulating thoughts, said IBM Chief Privacy Officer Christina Montgomery.
Ethics committees
Tech companies acknowledge that just five years ago they were launching AI services such as chatbots and photo-tagging with few ethical safeguards, and tackling misuse or biased results with subsequent updates.
But as political and public scrutiny of AI failings grew, Microsoft in 2017 and Google and IBM in 2018 established ethics committees to review new services from the start.
Google said it was presented with its money-lending quandary last September when a financial services company figured AI could assess people's creditworthiness better than other methods.
The project appeared well-suited for Google Cloud, whose expertise in developing AI tools that help in areas such as detecting abnormal transactions has attracted clients like Deutsche Bank, HSBC and BNY Mellon.
                                              Bill Foster                                                                   US Congressman                                  
Google's unit anticipated AI-based credit scoring could become a market worth billions of dollars a year and wanted a foothold.
However, its ethics committee of about 20 managers, social scientists and engineers who review potential deals unanimously voted against the project at an October meeting, Pizzo Frey said.
The AI system would need to learn from past data and patterns, the committee concluded, and thus risked repeating discriminatory practices from around the world against people of colour and other marginalized groups.
What's more the committee, internally known as ""Lemonaid,"" enacted a policy to skip all financial services deals related to creditworthiness until such concerns could be resolved.
Google also said its second Cloud ethics committee, known as Iced Tea, this year placed under review a service released in 2015 for categorising photos of people by four expressions: joy, sorrow, anger and surprise.
The move followed a ruling last year by Google's company-wide ethics panel, the Advanced Technology Review Council (ATRC), holding back new services related to reading emotion.
The ATRC - over a dozen top executives and engineers - determined that inferring emotions could be insensitive because facial cues are associated differently with feelings across cultures, among other reasons, said Jen Gennai, founder and lead of Google's Responsible Innovation team.
Reproducing voices and faces
Microsoft, meanwhile, developed software that could reproduce someone's voice from a short sample, but the company's Sensitive Uses panel then spent more than two years debating the ethics around its use and consulted company President Brad Smith, senior AI officer Crampton told Reuters.
She said the panel - specialists in fields such as human rights, data science and engineering - eventually gave the green light for Custom Neural Voice to be fully released in February this year. But it placed restrictions on its use, including that subjects' consent is verified and a team with ""Responsible AI Champs"" trained on corporate policy approve purchases.
IBM's AI board, comprising about 20 department leaders, wrestled with its own dilemma when early in the COVID-19 pandemic it examined a client request to customize facial-recognition technology to spot fevers and face coverings.
Montgomery said the board, which she co-chairs, declined the invitation, concluding that manual checks would suffice with less intrusion on privacy because photos would not be retained for any AI database.
Six months later, IBM announced it was discontinuing its face-recognition service.
What is the EU saying about it?
In an attempt to protect privacy and other freedoms, lawmakers in the European Union and the United States are pursuing far-reaching controls on AI systems.
The EU's Artificial Intelligence Act, on track to be passed next year, would bar real-time face recognition in public spaces and require tech companies to vet high-risk applications, such as those used in hiring, credit scoring and law enforcement.
US Congressman Bill Foster, who has held hearings on how algorithms carry forward discrimination in financial services and housing, said new laws to govern AI would ensure an even field for vendors.
""When you ask a company to take a hit in profits to accomplish societal goals, they say, 'What about our shareholders and our competitors?' That's why you need sophisticated regulation,"" the Democrat from Illinois said.
""There may be areas which are so sensitive that you will see tech firms staying out deliberately until there are clear rules of road"".
Indeed some AI advances may simply be on hold until companies can counter ethical risks without dedicating enormous engineering resources.
After Google Cloud turned down the request for custom financial AI last October, the Lemonaid committee told the sales team that the unit aims to start developing credit-related applications someday.
First, research into combating unfair biases must catch up with Google Cloud's ambitions to increase financial inclusion through the ""highly sensitive"" technology, it said in the policy circulated to staff.
""Until that time, we are not in a position to deploy solutions"".
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (95%); ARTIFICIAL INTELLIGENCE ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); DEEPFAKE TECHNOLOGY (90%); EXECUTIVES (89%); GENERATIVE AI (89%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (79%); CHATBOTS (79%); BUSINESS ETHICS & CORPORATE CITIZENSHIP (78%); COMPUTER CRIME (78%); EMOTIONS (78%); NEGATIVE NEWS (78%); SAFETY (78%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (78%); HUMAN RIGHTS ORGANIZATIONS (73%); BIOMETRICS (72%); HUMAN RIGHTS & CIVIL LIBERTIES LAW (71%); MANAGERS & SUPERVISORS (71%); SAFETY REGULATION & POLICY (71%); AUTOMOTIVE SAFETY (50%); Artificial intelligence (%); cloud-computing (%); Technology (%); Google (%); Microsoft (%); Computers (%)
Company: GOOGLE LLC (95%);  MICROSOFT CORP (57%);  AI SYSTEMS (56%)
Ticker: MSFT (NASDAQ) (57%)
Industry: NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (95%); SIC7372 PREPACKAGED SOFTWARE (57%); NAICS511210 SOFTWARE PUBLISHERS (57%); ARTIFICIAL INTELLIGENCE ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); BIG TECH (90%); DEEPFAKE TECHNOLOGY (90%); GENERATIVE AI (89%); INFORMATION TECHNOLOGY INDUSTRY (89%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (79%); CHATBOTS (79%); PATTERN RECOGNITION (79%); BANKING & FINANCE (78%); COMPUTER CRIME (78%); COMPUTER SOFTWARE (78%); AUTOMOTIVE SAFETY (50%)
Geographic: EUROPE (90%)
Load-Date: September 8, 2021",neutral,0.5404092669487,balanced/neutral,"['privacy', 'discrimination', 'transparency', 'safety', 'human rights', 'consent']",[],"['regulation', 'policy', 'oversight', 'law', 'should', 'must', 'need to']",['generative ai'],6,0,7,1
2021,Unknown Title,"Body
Melbourne: Law Institute of Victoria has issued the following news release:
Legal ethics will continue to be challenged by the rapid advance of artificial intelligence. Moral judgment will always be needed by lawyers when faced with questions about ethical obligations in practice. This cannot be replaced by AI.
In Henry VI, William Shakespeare ’ s character Dick the Butcher says: “The first thing we do, let ’ s kill all the lawyers”. While originally meant to be complimentary of honest lawyers in the context of Dick plotting treason to install a new English monarch, ever since the 1600s the world has regarded the words as a worthy pursuit for wider society, especially in the US. In recent times, there has been much written about the possible role for artificial intelligence (AI) in the practice of law and the administration of justice. There is much to commend this approach from an efficiency and cost-saving viewpoint, but it does have some significant limitations, especially in the area of lawyers ’ ethics, including their paramount duty to the court and administration of justice, which attaches to all lawyers including in-house counsel.History of AI and the law
The use of AI in the law in Australia really came of age in the early 1980s in the Royal Commission on the Activities of Federated Ship Painters and Dockers Union, conducted by Frank Costigan QC and assisted by Senior Counsel Douglas Meagher QC.
In its Final Report, Volume 1, published in October 1984, the Commission outlined the course it had taken to use AI to efficiently and effectively manage the huge amount of information and documents which it had gathered in the course of its far-reaching investigations across Australia:
“The Commission commenced its hearings in the first week of October 1980. In the first few weeks, its main task was to travel around Australia to each capital city and obtain from the Union all of its records. This exercise produced a massive number of documents. It became apparent in the first week of the Commission that the documents which would be acquired would be so large in number that no manual filing system could sensibly cope with them, and they would be impossible to analyse without the use of modern management systems . . . ”
Since the 1980s, there have been far-reaching advances in the use of AI both in society generally and the administration of justice in particular. There is no doubt that in the “info-world” in which we now live, the use of AI and a raft of other ever-increasing technology solutions is essential for democracy to function in a time when effective communication between governments and citizens is so essential. The rule of law is one of the basic pillars of that system of societal government.
Internationally, lawyers ’ ethical duties regarding technology have been explicitly outlined in professional conduct rules, imposing on lawyers an obligation to “keep abreast of the changes in the law and its practice, including the benefits and risks associated with relevant technology”. While Australian lawyers are not subject to such an imposition, the rate at which technological changes are being implemented as a result of our current climate may make this subject to change.The present day
Recently, a career columnist and former general counsel in the US, Elizabeth A Colombo, put AI into the context of today ’ s world:
“When I first saw Terminator 2: Judgment Day in grade school, August 29, 1997 seemed so far away. When I first heard Billy Joel ’ s Miami 2017 as a child, 2017 seemed so far away. Now, 1997 and 2017 have come and gone. Thankfully, Skynet has not become self-aware, but technology is way beyond where I thought it would be by 2019 . . .
“. . . 2029 may feel far away right now, but all of this makes me wonder what in-house law might look like in 10 years. What will in-house law be like in an age of artificial intelligence (AI)?”
The author went on to predict, that with AI handling simpler processes and reviews, lawyers would be freed up for more challenging jobs that require human brains, including reviewing complex contracts, managing strategic planning, handling multifaceted legal matters, developing creative solutions, collaborating across departments, negotiating with external clients, applying knowledge of corporate policy to devise and implement legal strategies, and fostering tactical associations across internal business functions and hierarchies, as well as with key external stakeholders, to facilitate optimum solutions and advance corporate objectives. She opined that these complex tasks are more fulfilling than the mundane tasks that AI can perform, so a lawyer ’ s workday will become more rewarding thanks to legal tech.
At the time this article was written in September 2019, no one could have foreseen that within a few short months the world would find itself in the midst of a global pandemic causing widespread devastating illness, death in the millions, and financial chaos in the world ’ s economies.
With what feels like the speed of light, the legal world (including in-house lawyers) has had to adapt to find new and clever ways of working in order to survive and serve their clients. The response has involved a rapid rise in lawyers, support staff, and even the courts in many instances, working from home or attending meetings, conferences, directions hearings, and mediations conducted on electronic platforms such as Zoom. There has also been increased use of technology for remote signing and witnessing legal and business-related documents. In the midst of this maelstrom, where will legal ethics sit? This is a critical question for all lawyers, especially in-house counsel who are often regarded as the moral gatekeepers for their employer organisations.
For example, Ms Colombo addressed her view of the future of remote-working for in-house lawyers in terms which reflect the new employment world in which we find ourselves living:
“Remote work, either entirely or in part, is becoming more the norm in corporate legal departments. I anticipate the continuation of this trend and possibly an uptick in remote work possibilities for in-house legal departments.
“Some companies employ an entirely remote legal department. It saves the organisation overheads, and in 2019, and certainly in 2029, productivity does not suffer. We are in an era in which work can be done anywhere by a motivated and conscientious employee.
“In traditional physical employment locations, I foresee more flexible working arrangements given that technology allows for work to be done outside of the office. Look for more flexible schedules and remote work options even within a legal department located in bricks and mortar.
“Additionally, technology will bring more methods for employees to communicate outside of work (for example, through texting applications and social media). Communicating using different kinds of technology will bring benefits, but could also cause problems. ”
In a speech given to in-house government lawyers in the UK in March 2020, the deputy president of the Supreme Court sounded a word of warning in the context of new technology used within the law for effectively delivering commercial results:
“In all this, ethical considerations, the interests of the consumer, and the need for privacy and data integrity will have to be balanced carefully against the potential benefits the new technology brings in terms of lowering transaction costs, broadening access to commerce, increasing market efficiency and enhancing consumer choice. It will be a most challenging task with important ramifications for the wellbeing of our societies in the years to come. ”
AI and ethics – the future
Whatever the outcome of this growing debate, one area that will continue to resist the advance of AI in the law is the field of legal ethics. In teaching ethics to solicitors, we always make the point to our audiences, that very often there are no black and white answers to ethical questions – only lots of maybe yes, maybe no answers.
There is no doubt that much of the learning and research upon which we rely in legal ethics could be harnessed usefully in a digital format to save time and make us focus on what is relevant; however, in the end, human personal and moral judgments will need to play a significant part in the final outcome.
Former Law Council of Australia president Morry Bailes relevantly made comparisons between a human lawyer and an AI lawyer in a 2017 speech delivered while he was president-elect. Specifically, he emphasised that a human lawyer considers all possible options so as to avoid failure and find a solution that suits the needs of an individual client. Conversely, an AI lawyer requires failure to adapt and learn, failure which the client will likely not appreciate. This further exemplifies that the moral judgment of a human lawyer is required where an answer is not definitively black or white.
We must always remember that, as officers of the court, our paramount duty is to the court and the administration of justice. Unless and until our judges are replaced by robots, we will need to continue to apply moral judgments in fulfilling our ethical and professional obligations.
Even if our judges were replaced by robots, the ethical and professional obligations owed to clients cannot simply vanish. Responsibility must be attributed for any breaches of ethical or professional obligations to the lawyers behind the robots. We must remember that we are responsible for any AI or technology that we utilise in practice and ensure we do not over-rely on this to the extent our moral judgment ceases to exist.
The last word
As much as Dick the Butcher may have wished to rid society of lawyers to further his own nefarious ends, it is doubtful that he will ever be placed in the situation where his plea will be: “The first thing we do, let ’ s vaporise all the robots”.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: INS
Subject: ETHICS (95%); LAWYERS (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); LEGAL ETHICS (90%); PROFESSIONAL WORKERS (89%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (89%); INVESTIGATIONS (78%); RULE OF LAW (78%); BRITISH MONARCHS (77%); NEGATIVE NEWS (76%); MONARCHIES (75%); TREASON (75%); CORPORATE COUNSEL (73%); DEMOCRACIES (73%)
Industry: LAWYERS (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); BRITISH MONARCHS (77%); CORPORATE COUNSEL (73%); PAINT & WALLPAPER CONTRACTORS (50%)
Geographic: MELBOURNE, AUSTRALIA (79%); VICTORIA, AUSTRALIA (79%); AUSTRALIA (91%); UNITED STATES (79%)
Load-Date: September 6, 2021","Melbourne: Law Institute of Victoria has issued the following news release:
Legal ethics will continue to be challenged by the rapid advance of artificial intelligence. Moral judgment will always be needed by lawyers when faced with questions about ethical obligations in practice. This cannot be replaced by AI.
In Henry VI, William Shakespeare ’ s character Dick the Butcher says: “The first thing we do, let ’ s kill all the lawyers”. While originally meant to be complimentary of honest lawyers in the context of Dick plotting treason to install a new English monarch, ever since the 1600s the world has regarded the words as a worthy pursuit for wider society, especially in the US. In recent times, there has been much written about the possible role for artificial intelligence (AI) in the practice of law and the administration of justice. There is much to commend this approach from an efficiency and cost-saving viewpoint, but it does have some significant limitations, especially in the area of lawyers ’ ethics, including their paramount duty to the court and administration of justice, which attaches to all lawyers including in-house counsel.History of AI and the law
The use of AI in the law in Australia really came of age in the early 1980s in the Royal Commission on the Activities of Federated Ship Painters and Dockers Union, conducted by Frank Costigan QC and assisted by Senior Counsel Douglas Meagher QC.
In its Final Report, Volume 1, published in October 1984, the Commission outlined the course it had taken to use AI to efficiently and effectively manage the huge amount of information and documents which it had gathered in the course of its far-reaching investigations across Australia:
“The Commission commenced its hearings in the first week of October 1980. In the first few weeks, its main task was to travel around Australia to each capital city and obtain from the Union all of its records. This exercise produced a massive number of documents. It became apparent in the first week of the Commission that the documents which would be acquired would be so large in number that no manual filing system could sensibly cope with them, and they would be impossible to analyse without the use of modern management systems . . . ”
Since the 1980s, there have been far-reaching advances in the use of AI both in society generally and the administration of justice in particular. There is no doubt that in the “info-world” in which we now live, the use of AI and a raft of other ever-increasing technology solutions is essential for democracy to function in a time when effective communication between governments and citizens is so essential. The rule of law is one of the basic pillars of that system of societal government.
Internationally, lawyers ’ ethical duties regarding technology have been explicitly outlined in professional conduct rules, imposing on lawyers an obligation to “keep abreast of the changes in the law and its practice, including the benefits and risks associated with relevant technology”. While Australian lawyers are not subject to such an imposition, the rate at which technological changes are being implemented as a result of our current climate may make this subject to change.The present day
Recently, a career columnist and former general counsel in the US, Elizabeth A Colombo, put AI into the context of today ’ s world:
“When I first saw Terminator 2: Judgment Day in grade school, August 29, 1997 seemed so far away. When I first heard Billy Joel ’ s Miami 2017 as a child, 2017 seemed so far away. Now, 1997 and 2017 have come and gone. Thankfully, Skynet has not become self-aware, but technology is way beyond where I thought it would be by 2019 . . .
“. . . 2029 may feel far away right now, but all of this makes me wonder what in-house law might look like in 10 years. What will in-house law be like in an age of artificial intelligence (AI)?”
The author went on to predict, that with AI handling simpler processes and reviews, lawyers would be freed up for more challenging jobs that require human brains, including reviewing complex contracts, managing strategic planning, handling multifaceted legal matters, developing creative solutions, collaborating across departments, negotiating with external clients, applying knowledge of corporate policy to devise and implement legal strategies, and fostering tactical associations across internal business functions and hierarchies, as well as with key external stakeholders, to facilitate optimum solutions and advance corporate objectives. She opined that these complex tasks are more fulfilling than the mundane tasks that AI can perform, so a lawyer ’ s workday will become more rewarding thanks to legal tech.
At the time this article was written in September 2019, no one could have foreseen that within a few short months the world would find itself in the midst of a global pandemic causing widespread devastating illness, death in the millions, and financial chaos in the world ’ s economies.
With what feels like the speed of light, the legal world (including in-house lawyers) has had to adapt to find new and clever ways of working in order to survive and serve their clients. The response has involved a rapid rise in lawyers, support staff, and even the courts in many instances, working from home or attending meetings, conferences, directions hearings, and mediations conducted on electronic platforms such as Zoom. There has also been increased use of technology for remote signing and witnessing legal and business-related documents. In the midst of this maelstrom, where will legal ethics sit? This is a critical question for all lawyers, especially in-house counsel who are often regarded as the moral gatekeepers for their employer organisations.
For example, Ms Colombo addressed her view of the future of remote-working for in-house lawyers in terms which reflect the new employment world in which we find ourselves living:
“Remote work, either entirely or in part, is becoming more the norm in corporate legal departments. I anticipate the continuation of this trend and possibly an uptick in remote work possibilities for in-house legal departments.
“Some companies employ an entirely remote legal department. It saves the organisation overheads, and in 2019, and certainly in 2029, productivity does not suffer. We are in an era in which work can be done anywhere by a motivated and conscientious employee.
“In traditional physical employment locations, I foresee more flexible working arrangements given that technology allows for work to be done outside of the office. Look for more flexible schedules and remote work options even within a legal department located in bricks and mortar.
“Additionally, technology will bring more methods for employees to communicate outside of work (for example, through texting applications and social media). Communicating using different kinds of technology will bring benefits, but could also cause problems. ”
In a speech given to in-house government lawyers in the UK in March 2020, the deputy president of the Supreme Court sounded a word of warning in the context of new technology used within the law for effectively delivering commercial results:
“In all this, ethical considerations, the interests of the consumer, and the need for privacy and data integrity will have to be balanced carefully against the potential benefits the new technology brings in terms of lowering transaction costs, broadening access to commerce, increasing market efficiency and enhancing consumer choice. It will be a most challenging task with important ramifications for the wellbeing of our societies in the years to come. ”
AI and ethics – the future
Whatever the outcome of this growing debate, one area that will continue to resist the advance of AI in the law is the field of legal ethics. In teaching ethics to solicitors, we always make the point to our audiences, that very often there are no black and white answers to ethical questions – only lots of maybe yes, maybe no answers.
There is no doubt that much of the learning and research upon which we rely in legal ethics could be harnessed usefully in a digital format to save time and make us focus on what is relevant; however, in the end, human personal and moral judgments will need to play a significant part in the final outcome.
Former Law Council of Australia president Morry Bailes relevantly made comparisons between a human lawyer and an AI lawyer in a 2017 speech delivered while he was president-elect. Specifically, he emphasised that a human lawyer considers all possible options so as to avoid failure and find a solution that suits the needs of an individual client. Conversely, an AI lawyer requires failure to adapt and learn, failure which the client will likely not appreciate. This further exemplifies that the moral judgment of a human lawyer is required where an answer is not definitively black or white.
We must always remember that, as officers of the court, our paramount duty is to the court and the administration of justice. Unless and until our judges are replaced by robots, we will need to continue to apply moral judgments in fulfilling our ethical and professional obligations.
Even if our judges were replaced by robots, the ethical and professional obligations owed to clients cannot simply vanish. Responsibility must be attributed for any breaches of ethical or professional obligations to the lawyers behind the robots. We must remember that we are responsible for any AI or technology that we utilise in practice and ensure we do not over-rely on this to the extent our moral judgment ceases to exist.
The last word
As much as Dick the Butcher may have wished to rid society of lawyers to further his own nefarious ends, it is doubtful that he will ever be placed in the situation where his plea will be: “The first thing we do, let ’ s vaporise all the robots”.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: INS
Subject: ETHICS (95%); LAWYERS (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); LEGAL ETHICS (90%); PROFESSIONAL WORKERS (89%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (89%); INVESTIGATIONS (78%); RULE OF LAW (78%); BRITISH MONARCHS (77%); NEGATIVE NEWS (76%); MONARCHIES (75%); TREASON (75%); CORPORATE COUNSEL (73%); DEMOCRACIES (73%)
Industry: LAWYERS (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); BRITISH MONARCHS (77%); CORPORATE COUNSEL (73%); PAINT & WALLPAPER CONTRACTORS (50%)
Geographic: MELBOURNE, AUSTRALIA (79%); VICTORIA, AUSTRALIA (79%); AUSTRALIA (91%); UNITED STATES (79%)
Load-Date: September 6, 2021",neutral,0.7556520700454712,balanced/neutral,"['privacy', 'access']","['character', 'justice', 'justice']","['policy', 'law', 'must', 'need to']",[],2,3,4,0
2021,Unknown Title,"Byline: Thomas Fox
Body
Sep 24, 2021( JD Supra: http://www.jdsupra.com Delivered by Newstex)  
 A successful whistleblowing program doesn't start with installing a helpline-it starts with fostering an environment that protects whistleblowers, makes them feel supported, and makes clear the value they bring to the business. So how do you build that ""speak-up culture?"" Join this session to hear from a panel of practitioners who manage whistleblowing programs and whistleblower advocates who'll share their insights, experiences, and challenges they've Seemore+ A successful whistleblowing program doesn't start with installing a helpline-it starts with fostering an environment that protects whistleblowers, makes them feel supported, and makes clear the value they bring to the business. So how do you build that ""speak-up culture?"" Join this session to hear from a panel of practitioners who manage whistleblowing programs and whistleblower advocates who'll share their insights, experiences, and challenges they've faced. 
 In this episode of The Ethics Movement, I visit Philip Winterburn, Convercent by OneTrust Chief Strategy Officer and talk about his presentation at Converge21 on Digital Ethics: AI, Privacy, and More. We discuss why it's high time to consider the ethics behind practices, including AI, machine learning, and consumer data collection, and how companies can embrace new technologies, scale quickly, and continue evolving while maintaining their values. Seeless- 
Classification
Language: English
Publication-Type: Web Blog
Journal Code: JSUP-5263
Subject: ETHICS (90%); PSYCHOLOGICAL SAFETY (90%); WHISTLEBLOWER PROTECTION (90%); WHISTLEBLOWERS (90%); ARTIFICIAL INTELLIGENCE (77%); MACHINE LEARNING (72%); COMPANY STRATEGY (71%); EXECUTIVES (71%)
Industry: PSYCHOLOGICAL SAFETY (90%); ARTIFICIAL INTELLIGENCE (77%); MACHINE LEARNING (72%)
Load-Date: September 24, 2021","Sep 24, 2021( JD Supra: http://www.jdsupra.com Delivered by Newstex)  
 A successful whistleblowing program doesn't start with installing a helpline-it starts with fostering an environment that protects whistleblowers, makes them feel supported, and makes clear the value they bring to the business. So how do you build that ""speak-up culture?"" Join this session to hear from a panel of practitioners who manage whistleblowing programs and whistleblower advocates who'll share their insights, experiences, and challenges they've Seemore+ A successful whistleblowing program doesn't start with installing a helpline-it starts with fostering an environment that protects whistleblowers, makes them feel supported, and makes clear the value they bring to the business. So how do you build that ""speak-up culture?"" Join this session to hear from a panel of practitioners who manage whistleblowing programs and whistleblower advocates who'll share their insights, experiences, and challenges they've faced. 
 In this episode of The Ethics Movement, I visit Philip Winterburn, Convercent by OneTrust Chief Strategy Officer and talk about his presentation at Converge21 on Digital Ethics: AI, Privacy, and More. We discuss why it's high time to consider the ethics behind practices, including AI, machine learning, and consumer data collection, and how companies can embrace new technologies, scale quickly, and continue evolving while maintaining their values. Seeless- 
Classification
Language: English
Publication-Type: Web Blog
Journal Code: JSUP-5263
Subject: ETHICS (90%); PSYCHOLOGICAL SAFETY (90%); WHISTLEBLOWER PROTECTION (90%); WHISTLEBLOWERS (90%); ARTIFICIAL INTELLIGENCE (77%); MACHINE LEARNING (72%); COMPANY STRATEGY (71%); EXECUTIVES (71%)
Industry: PSYCHOLOGICAL SAFETY (90%); ARTIFICIAL INTELLIGENCE (77%); MACHINE LEARNING (72%)
Load-Date: September 24, 2021",neutral,0.7870039939880371,balanced/neutral,"['privacy', 'safety']",[],[],['machine learning'],2,0,0,1
2021,Unknown Title,"Body
The recent apartment building collapse in Miami, Florida, is a tragic reminder of the huge impacts engineering can have on our lives. Disasters such as this force engineers to reflect on their practice and perhaps fundamentally change their approach. Specifically, we should give much greater weight to ethics when training engineers. 
Engineers work in a vast range of fields that pose ethical concerns. These include artificial intelligence, data privacy, building construction, public health, and activity on shared environments (including Indigenous communities). The decisions engineers make, if not fully thought through, can have unintended consequences - including building failures and climate change. 
Engineers have ethical obligations (such as Engineers Australia's code of ethics) that they must follow. However, as identified at UNSW, the complexity of emerging social concerns creates a need for engineers' education to equip them with much deeper ethical skill sets. 
Engineering is seen as a trusted and ethical profession. In a 2019 Gallup poll, 66% rated the honesty and ethical standards of engineers as high/very high, on a par with medical doctors (65%). 
However, ethics as a body of knowledge is massive. There are nearly as many academic papers on ethics as mathematics, and clearly more than on artificial intelligence. 
Comparison of numbers of research papers by keyword (mathematics, ethics and AI).
With such a rich backdrop of knowledge, engineers must embrace ethics in a way that previous generations embraced mathematics. Complex societal problems make much greater demands on engineering thinking than in the past. We need to consider whole and complex systems, not just issues as individual challenges. 
Ethics and the construction industry
The construction industry provides a topical example of such complexity. Opal Tower in Sydney, Lacrosse building in Melbourne, Grenfell Tower in London and Torch Tower in Dubai became household names for all the wrong reasons. 
Importantly, these issues of poor quality and performance don't arise from new technology or know-how. They involve well-established technical domains of engineering: combustible cladding, fire safety, structural adequacy and so on. A fragmented design and delivery process with unclear responsibility and/or accountability has led to poor outcomes. 
These issues prompted the Australian Building Ministers' Forum to commission the Shergold Weir Report, followed by a task force to implement its recommendations across Australia. 
There are real shortcomings in the legal and contractual processes for allocating and ""commoditising"" risk in the industry. However, ethics should do the heavy lifting when legal frameworks are lacking. One key question is whether erosion of professional ethics has played a part in this state of affairs. The answer is a likely ""yes"". 
Engineers face ethical dilemmas such as: 
""Should I accept a narrow or inadequately framed design commission within a design and build delivery model when there is no certainty my design will be appropriately integrated with other parts of the project?""   
""How can I accept a commission when my client provides no budget for my oversight of the construction to ensure the technical integrity of my design is maintained when built?""   
""How do I play in a commercially competitive landscape with pressures to produce ""leaner"" designs to save cost without compromising safety and long-term performance of my design?""   
""Do I hide behind the contractual clauses (or minimum requirements of codes of practice) when I know the overall process is flawed and does not deliver quality and/or value for money for the end user?""
 Or worse: ""Do I resort to phoenixing to avoid any accountability?""The ethics of engineering involve much more than ensuring buildings don't collapse. Chad Davis/Flickr, CC BY-SAEngineering on Country The enduring connection of Aboriginal Australians to Country requires engineers to navigate ethical considerations in Indigenous communities. Engineers must reconcile the legal, technical and regulatory requirements of their projects with Indigenous cultural values and needs. They might not be properly equipped to navigate ethical scenarios when they encounter unfamiliar cultural connections, or regulations are insufficient.  Consider, for example, the sacred sites of the McArthur River Mine. Traditional owners have raised concerns that current mining activities do not adequately protect sacred and cultural heritage sites. Evidence given by community leaders provides insight into the intimate and diverse relationship that traditional owners have with the land.  In considering such evidence, engineers must be able to evaluate both physical site risks (such as acidification of mine tailings and contamination of water bodies) and cultural risks (such as failing to identify all locations of cultural value).  How might we tackle such complicated projects? By properly engaging with traditional communities and by having diverse teams with multiple worldviews and experiences, along with strong technical skills. The broad field of ethical knowledge provides the skill sets to attempt to reconcile the diverse considerations. Cornell Dam on the Croton River near Croton-on-Hudson, New York, was the tallest dam in the world when completed in 1906. The dam was built with beauty and the environment in mind, but protests and disputes still impacted its construction. Malinda Rathnayake/Flickr, CC BYWhat should the curriculum look like? Engineering students' ethical development requires a holistic approach. One assessment suggested: 
""[...] that institutions integrate ethics instruction throughout the formal curriculum, support use of varied approaches that foster high‐quality experiences, and leverage both influences of co‐curricular experiences and students' desires to engage in positive ethical behaviours.""
The curriculum should include: 
skills/expertise - the underlying intellectual basis for discerning what is ethical and what is not, which is much more than codes of conduct or a prescriptive, formulaic approach   
practice - practical know-how in terms of ethical solutions that engineers can apply   
mindset - having an individual and group culture of acting ethically. The engineers' problem-solving mindset must be supplemented by constant reflection on the decisions made and their ethical consequences.
 Ethics is not an ""add-on"" subject. It must permeate all aspects of tertiary education - teaching, research and professional behaviour.  While the arguments for acting now are strong, market realities will also drive the process. The upcoming generation will likely displace those who are slow or reluctant to adapt.  For instance, engineering firms are under pressure from their own staff on the issue of climate change. More than 1,900 Australian engineers and nearly 180 engineering organisations have signed a declaration committing them to evaluate all new projects against the need to mitigate climate change.  Future engineers must transcend any remaining single-solution mindsets from the past. They'll need to embrace a much more complex and socially minded ethics. And that begins with their university education.     This article is republished from The Conversation under a Creative Commons license. Read the original article.    READ MORE:Reforms give construction regulator power to reject projects, push for national adoption
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (96%); ARTIFICIAL INTELLIGENCE (90%); ENGINEERING (90%); PROFESSIONAL WORKERS (90%); SOCIETAL ISSUES (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (90%); TECHNICIANS & TECHNOLOGICAL WORKERS (90%); CONSTRUCTION SECTOR PERFORMANCE (89%); NEGATIVE NEWS (89%); 2017 GRENFELL TOWER FIRE (78%); ARTIFICIAL INTELLIGENCE ETHICS (78%); BUILDING COLLAPSES (78%); NEGATIVE SOCIETAL NEWS (78%); RESEARCH REPORTS (78%); SAFETY (78%); SAFETY, ACCIDENTS & DISASTERS (73%); PHYSICIANS & SURGEONS (72%); POLLS & SURVEYS (72%); PUBLIC HEALTH (72%); INDIGENOUS PEOPLES (71%); FIRE PREVENTION & SAFETY (69%)
Industry: ARTIFICIAL INTELLIGENCE (90%); CONSTRUCTION (90%); ENGINEERING (90%); CONSTRUCTION SECTOR PERFORMANCE (89%); ARTIFICIAL INTELLIGENCE ETHICS (78%); KNOWLEDGE MANAGEMENT (76%); PHYSICIANS & SURGEONS (72%); INFORMATION SECURITY & PRIVACY (56%)
Geographic: MELBOURNE, AUSTRALIA (71%); SYDNEY, AUSTRALIA (71%); MIAMI, FL, USA (58%); LONDON, ENGLAND (53%); DUBAI, UNITED ARAB EMIRATES (79%); FLORIDA, USA (79%); AUSTRALIA (91%); UNITED ARAB EMIRATES (73%)
Load-Date: July 19, 2021","The recent apartment building collapse in Miami, Florida, is a tragic reminder of the huge impacts engineering can have on our lives. Disasters such as this force engineers to reflect on their practice and perhaps fundamentally change their approach. Specifically, we should give much greater weight to ethics when training engineers. 
Engineers work in a vast range of fields that pose ethical concerns. These include artificial intelligence, data privacy, building construction, public health, and activity on shared environments (including Indigenous communities). The decisions engineers make, if not fully thought through, can have unintended consequences - including building failures and climate change. 
Engineers have ethical obligations (such as Engineers Australia's code of ethics) that they must follow. However, as identified at UNSW, the complexity of emerging social concerns creates a need for engineers' education to equip them with much deeper ethical skill sets. 
Engineering is seen as a trusted and ethical profession. In a 2019 Gallup poll, 66% rated the honesty and ethical standards of engineers as high/very high, on a par with medical doctors (65%). 
However, ethics as a body of knowledge is massive. There are nearly as many academic papers on ethics as mathematics, and clearly more than on artificial intelligence. 
Comparison of numbers of research papers by keyword (mathematics, ethics and AI).
With such a rich backdrop of knowledge, engineers must embrace ethics in a way that previous generations embraced mathematics. Complex societal problems make much greater demands on engineering thinking than in the past. We need to consider whole and complex systems, not just issues as individual challenges. 
Ethics and the construction industry
The construction industry provides a topical example of such complexity. Opal Tower in Sydney, Lacrosse building in Melbourne, Grenfell Tower in London and Torch Tower in Dubai became household names for all the wrong reasons. 
Importantly, these issues of poor quality and performance don't arise from new technology or know-how. They involve well-established technical domains of engineering: combustible cladding, fire safety, structural adequacy and so on. A fragmented design and delivery process with unclear responsibility and/or accountability has led to poor outcomes. 
These issues prompted the Australian Building Ministers' Forum to commission the Shergold Weir Report, followed by a task force to implement its recommendations across Australia. 
There are real shortcomings in the legal and contractual processes for allocating and ""commoditising"" risk in the industry. However, ethics should do the heavy lifting when legal frameworks are lacking. One key question is whether erosion of professional ethics has played a part in this state of affairs. The answer is a likely ""yes"". 
Engineers face ethical dilemmas such as: 
""Should I accept a narrow or inadequately framed design commission within a design and build delivery model when there is no certainty my design will be appropriately integrated with other parts of the project?""   
""How can I accept a commission when my client provides no budget for my oversight of the construction to ensure the technical integrity of my design is maintained when built?""   
""How do I play in a commercially competitive landscape with pressures to produce ""leaner"" designs to save cost without compromising safety and long-term performance of my design?""   
""Do I hide behind the contractual clauses (or minimum requirements of codes of practice) when I know the overall process is flawed and does not deliver quality and/or value for money for the end user?""
 Or worse: ""Do I resort to phoenixing to avoid any accountability?""The ethics of engineering involve much more than ensuring buildings don't collapse. Chad Davis/Flickr, CC BY-SAEngineering on Country The enduring connection of Aboriginal Australians to Country requires engineers to navigate ethical considerations in Indigenous communities. Engineers must reconcile the legal, technical and regulatory requirements of their projects with Indigenous cultural values and needs. They might not be properly equipped to navigate ethical scenarios when they encounter unfamiliar cultural connections, or regulations are insufficient.  Consider, for example, the sacred sites of the McArthur River Mine. Traditional owners have raised concerns that current mining activities do not adequately protect sacred and cultural heritage sites. Evidence given by community leaders provides insight into the intimate and diverse relationship that traditional owners have with the land.  In considering such evidence, engineers must be able to evaluate both physical site risks (such as acidification of mine tailings and contamination of water bodies) and cultural risks (such as failing to identify all locations of cultural value).  How might we tackle such complicated projects? By properly engaging with traditional communities and by having diverse teams with multiple worldviews and experiences, along with strong technical skills. The broad field of ethical knowledge provides the skill sets to attempt to reconcile the diverse considerations. Cornell Dam on the Croton River near Croton-on-Hudson, New York, was the tallest dam in the world when completed in 1906. The dam was built with beauty and the environment in mind, but protests and disputes still impacted its construction. Malinda Rathnayake/Flickr, CC BYWhat should the curriculum look like? Engineering students' ethical development requires a holistic approach. One assessment suggested: 
""[...] that institutions integrate ethics instruction throughout the formal curriculum, support use of varied approaches that foster high‐quality experiences, and leverage both influences of co‐curricular experiences and students' desires to engage in positive ethical behaviours.""
The curriculum should include: 
skills/expertise - the underlying intellectual basis for discerning what is ethical and what is not, which is much more than codes of conduct or a prescriptive, formulaic approach   
practice - practical know-how in terms of ethical solutions that engineers can apply   
mindset - having an individual and group culture of acting ethically. The engineers' problem-solving mindset must be supplemented by constant reflection on the decisions made and their ethical consequences.
 Ethics is not an ""add-on"" subject. It must permeate all aspects of tertiary education - teaching, research and professional behaviour.  While the arguments for acting now are strong, market realities will also drive the process. The upcoming generation will likely displace those who are slow or reluctant to adapt.  For instance, engineering firms are under pressure from their own staff on the issue of climate change. More than 1,900 Australian engineers and nearly 180 engineering organisations have signed a declaration committing them to evaluate all new projects against the need to mitigate climate change.  Future engineers must transcend any remaining single-solution mindsets from the past. They'll need to embrace a much more complex and socially minded ethics. And that begins with their university education.     This article is republished from The Conversation under a Creative Commons license. Read the original article.    READ MORE:Reforms give construction regulator power to reject projects, push for national adoption
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (96%); ARTIFICIAL INTELLIGENCE (90%); ENGINEERING (90%); PROFESSIONAL WORKERS (90%); SOCIETAL ISSUES (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (90%); TECHNICIANS & TECHNOLOGICAL WORKERS (90%); CONSTRUCTION SECTOR PERFORMANCE (89%); NEGATIVE NEWS (89%); 2017 GRENFELL TOWER FIRE (78%); ARTIFICIAL INTELLIGENCE ETHICS (78%); BUILDING COLLAPSES (78%); NEGATIVE SOCIETAL NEWS (78%); RESEARCH REPORTS (78%); SAFETY (78%); SAFETY, ACCIDENTS & DISASTERS (73%); PHYSICIANS & SURGEONS (72%); POLLS & SURVEYS (72%); PUBLIC HEALTH (72%); INDIGENOUS PEOPLES (71%); FIRE PREVENTION & SAFETY (69%)
Industry: ARTIFICIAL INTELLIGENCE (90%); CONSTRUCTION (90%); ENGINEERING (90%); CONSTRUCTION SECTOR PERFORMANCE (89%); ARTIFICIAL INTELLIGENCE ETHICS (78%); KNOWLEDGE MANAGEMENT (76%); PHYSICIANS & SURGEONS (72%); INFORMATION SECURITY & PRIVACY (56%)
Geographic: MELBOURNE, AUSTRALIA (71%); SYDNEY, AUSTRALIA (71%); MIAMI, FL, USA (58%); LONDON, ENGLAND (53%); DUBAI, UNITED ARAB EMIRATES (79%); FLORIDA, USA (79%); AUSTRALIA (91%); UNITED ARAB EMIRATES (73%)
Load-Date: July 19, 2021",negative,0.6865031719207764,balanced/neutral,"['privacy', 'accountability', 'safety', 'security']",[],"['oversight', 'standards', 'should', 'must', 'need to']",[],4,0,5,0
2021,Unknown Title,"Byline: Allen & Overy LLP
Body
Jan 08, 2021( JD Supra: http://www.jdsupra.com Delivered by Newstex)  Just before Christmas, the House of Lords Liaison Committee published a new report on Artificial Intelligence (AI): 'AI in the UK: No Room for Complacency[1]'. The report examines the progress made by the UK government in implementing the recommendations of the Select Committee on Artificial Intelligence in their 2018 report entitled 'AI in the UK: ready, willing and able[2]?'. The Select Committee had been appointed 'to consider the economic, ethical and social implications of advances in artificial intelligence', and its 2018 report made a large number of recommendations to the UK government on a range of technical, organisational and societal topics, as well as a recommended ethical framework for AI.The new Liaison Committee report notes that, since the publication of the Select Committee report two and a half years earlier, investment in AI in the UK has grown significantly. It observes that AI has been deployed in the UK in a growing number of fields and use cases, and that the COVID-19 pandemic accelerated this trend still further as states and technology companies alike deployed AI to help tackle the pandemic (e.g. as part of test and trace technologies) and the various lockdown restrictions resulted in greater reliance on digital technologies.The report suggests that this has placed far greater importance on topics such as transparency and data protection. 
It states that as the deployment of AI systems (and with it the processing of personal data) accelerates, 'the public's understanding of the technology, and the ability to give informed consent, could be left behind'.The report concludes that this makes it all the more important that we focus on better understanding AI's opportunities and risks, particularly in relation to data and privacy, and that our regulatory frameworks are equipped to manage these risks.The report contains various recommendations for how the UK government can develop a policy framework addressing these (and other) topics. Some of the key recommendations are summarised below.Public understanding of AI systemsThe report recommends that the government rapidly develop policies to improve public understanding of AI systems and, in particular, to safeguard personal data used by AI systems before the ability to introduce those safeguards is outstripped by technological developments.To this end, the report advocates for a stronger and more active role for the AI Council, the body set up by the UK government to explore how to develop and deploy safe, fair, legal and ethical data sharing frameworks.Data trustsThe report also recommends pressing ahead with establishing data trusts. Data trusts were first proposed in an independent review carried out in 2017 by Dame Wendy Hall and Jerome Pesenti, and subsequently developed further by the Open Data Institute and the Office for AI and Innovate UK. The Hall-Pesenti review envisaged that data trusts would facilitate the sharing of datasets between individuals and organisations within a framework that allows for decisions about those datasets to be monitored. At a high level, the Hall-Pesenti review envisaged that data trusts could give individuals who have their personal data within these trusts insight into, and some say over, how the trust uses their data.The Open Data Institute defines a data trust as 'an approach to looking after and making decisions about data in a similar way that trusts have been used to look after and make decisions about other forms of asset in the past, such as land trusts that steward land on behalf of local communities.'However, while recommending that data trusts be established, the report does not address how that would be achieved in practice and what form the trust would take. The original Hall-Pesenti review envisaged that data trusts 'would not be a legal entity or institution, but rather a set of relationships underpinned by a repeatable frameworkto share data in a fair, safe and equitable way'.EthicsA key focus of the report is on operationalising data ethics, and instilling ethical principles in the development and deployment of AI systems. The report acknowledges that a large number of organisations have published their own ethical codes of conduct, but suggests that self-regulation risks 'a lack of uniformity and enforceability'. For instance, the report cites concerns that public trust in new technologies is to a large extent contingent on regulatory oversight of those technologies.Among other recommendations in this area, the report recommends that the Centre for Data Ethics and Innovation (CDEI) establish national standards for the ethical development and deployment of AI. These standards must be capable of being well understood by the general public, as well as being generally applicable to developers of AI systems (acknowledging that some targeted sector-specific exceptions may be required). The report suggests that the standards should consist of two frameworks, one for the ethical development of AI, including issues of prejudice and bias, and the other for the ethical use of AI by policymakers and businesses. These two frameworks should reflect the different risks and considerations at each stage of AI use.Jobs and skillsThe report concludes that while we do not yet have a clear sense of the impact that AI will have on jobs or the speed at which this impact will be felt, complacency is a significant risk in the jobs market. The report suggests that as the COVID-19 pandemic recedes, and the government seeks to address the resultant economic fall-out, the nature of work will likely change and different jobs will be required. Accordingly, the report highlights a digital skills shortage in the UK, with recommendations for an expanded national retraining scheme to equip people to work alongside AI and, more generally, for improving the digital skills base in the UK. The report also recommends that the AI Council should identify those industries most at risk and the specific skills gaps in those industries, with the government then formulating training schemes to address those gaps.RegulationIn its 2018 report, the Select Committee concluded that 'Blanket AI-specific regulation, at this stage, would be inappropriate. We believe that existing sector-specific regulators are best placed to consider the impact on their sectors of any subsequent regulation which may be needed. We welcome that the [General Data Protection Regulation] appears to address many of the concernsregarding the handling of personal data'.The new report endorses this position and agrees that the challenges posed by the development and deployment of AI cannot currently be tackled by cross-cutting regulation.However, the report does raise significant concerns about gaps in regulation, including deficiencies in the existing legal framework for the use of AI by social media companies or in facial recognition technology. It also states that a sector-specific approach relies on relevant regulators being skilled at understanding the risks of AI and how to mitigate them and, as Simon Taylor of the CDEI puts it, the gap in 'understanding how to make sense of our existing laws, regulations and ethical standards' in the context of AI.The report suggests that the CDEI and Office for AI can play a cross-cutting role, along with the Information Commissioner's Office (ICO), to provide training and upskilling for sector-specific regulators. It recommends that the ICO (alongside the CDEI, the Office for AI and the Alan Turing Institute) develop a training course for use by regulators to ensure that their staff have a grounding in the ethical and appropriate use of personal data and AI systems, and its opportunities and risks. This training should be prepared and rolled out by July 2021.Role of governmentThe report cites various leading AI practitioners commending the collaboration that is happening across government and research institutions, and the success in establishing appropriate bodies to steward the development of AI in the UK. That said, the report suggests that more needs to be done. It recommends that a Cabinet Committee be established to set the strategic direction of AI policy in the UK, and the strategy for the use of data and technology by national and local government. It suggests that the first task of this committee should be to commission and approve a five year strategy for AI.The report also stresses the immediate need to appoint a Government Chief Data Officer. The government agreed the need for this role in 2017, and despite repeated commitments to the idea, has so far not taken steps to recruit for it.The report also underlines the importance of maintaining the UK's leading position on AI, and that attracting and retaining top AI talent should be a key focus of any future immigration policy.At this stage, the recommendations contained in the report are just that, and we will need to see the government's response to understand whether any will be adopted (and by when). [ 1]: https://publications.parliament.uk/pa/ld5801/ldselect/ldliaison/196/196.pdf [ 2]: https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100/100.pdf 
Classification
Language: English
Publication-Type: Web Blog
Journal Code: JSUP-5263
Subject: ARTIFICIAL INTELLIGENCE (90%); BRITISH PARLIAMENT (90%); GOVERNMENT & PUBLIC ADMINISTRATION (90%); LEGISLATIVE BODIES (90%); ETHICS (89%); PUBLIC RECORDS (77%); TRENDS (77%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (76%); DATA PROTECTION LAWS (72%); COVID-19 CORONAVIRUS REGULATION & POLICY (71%); APPOINTMENTS (56%)
Industry: ARTIFICIAL INTELLIGENCE (90%); INFORMATION SECURITY & PRIVACY (88%); INFORMATION TECHNOLOGY INDUSTRY (78%); OPEN DATA (78%); DATA PROTECTION LAWS (72%); DATA SECURITY (69%)
Geographic: UNITED KINGDOM (96%)
Load-Date: January 8, 2021","Jan 08, 2021( JD Supra: http://www.jdsupra.com Delivered by Newstex)  Just before Christmas, the House of Lords Liaison Committee published a new report on Artificial Intelligence (AI): 'AI in the UK: No Room for Complacency[1]'. The report examines the progress made by the UK government in implementing the recommendations of the Select Committee on Artificial Intelligence in their 2018 report entitled 'AI in the UK: ready, willing and able[2]?'. The Select Committee had been appointed 'to consider the economic, ethical and social implications of advances in artificial intelligence', and its 2018 report made a large number of recommendations to the UK government on a range of technical, organisational and societal topics, as well as a recommended ethical framework for AI.The new Liaison Committee report notes that, since the publication of the Select Committee report two and a half years earlier, investment in AI in the UK has grown significantly. It observes that AI has been deployed in the UK in a growing number of fields and use cases, and that the COVID-19 pandemic accelerated this trend still further as states and technology companies alike deployed AI to help tackle the pandemic (e.g. as part of test and trace technologies) and the various lockdown restrictions resulted in greater reliance on digital technologies.The report suggests that this has placed far greater importance on topics such as transparency and data protection. 
It states that as the deployment of AI systems (and with it the processing of personal data) accelerates, 'the public's understanding of the technology, and the ability to give informed consent, could be left behind'.The report concludes that this makes it all the more important that we focus on better understanding AI's opportunities and risks, particularly in relation to data and privacy, and that our regulatory frameworks are equipped to manage these risks.The report contains various recommendations for how the UK government can develop a policy framework addressing these (and other) topics. Some of the key recommendations are summarised below.Public understanding of AI systemsThe report recommends that the government rapidly develop policies to improve public understanding of AI systems and, in particular, to safeguard personal data used by AI systems before the ability to introduce those safeguards is outstripped by technological developments.To this end, the report advocates for a stronger and more active role for the AI Council, the body set up by the UK government to explore how to develop and deploy safe, fair, legal and ethical data sharing frameworks.Data trustsThe report also recommends pressing ahead with establishing data trusts. Data trusts were first proposed in an independent review carried out in 2017 by Dame Wendy Hall and Jerome Pesenti, and subsequently developed further by the Open Data Institute and the Office for AI and Innovate UK. The Hall-Pesenti review envisaged that data trusts would facilitate the sharing of datasets between individuals and organisations within a framework that allows for decisions about those datasets to be monitored. At a high level, the Hall-Pesenti review envisaged that data trusts could give individuals who have their personal data within these trusts insight into, and some say over, how the trust uses their data.The Open Data Institute defines a data trust as 'an approach to looking after and making decisions about data in a similar way that trusts have been used to look after and make decisions about other forms of asset in the past, such as land trusts that steward land on behalf of local communities.'However, while recommending that data trusts be established, the report does not address how that would be achieved in practice and what form the trust would take. The original Hall-Pesenti review envisaged that data trusts 'would not be a legal entity or institution, but rather a set of relationships underpinned by a repeatable frameworkto share data in a fair, safe and equitable way'.EthicsA key focus of the report is on operationalising data ethics, and instilling ethical principles in the development and deployment of AI systems. The report acknowledges that a large number of organisations have published their own ethical codes of conduct, but suggests that self-regulation risks 'a lack of uniformity and enforceability'. For instance, the report cites concerns that public trust in new technologies is to a large extent contingent on regulatory oversight of those technologies.Among other recommendations in this area, the report recommends that the Centre for Data Ethics and Innovation (CDEI) establish national standards for the ethical development and deployment of AI. These standards must be capable of being well understood by the general public, as well as being generally applicable to developers of AI systems (acknowledging that some targeted sector-specific exceptions may be required). The report suggests that the standards should consist of two frameworks, one for the ethical development of AI, including issues of prejudice and bias, and the other for the ethical use of AI by policymakers and businesses. These two frameworks should reflect the different risks and considerations at each stage of AI use.Jobs and skillsThe report concludes that while we do not yet have a clear sense of the impact that AI will have on jobs or the speed at which this impact will be felt, complacency is a significant risk in the jobs market. The report suggests that as the COVID-19 pandemic recedes, and the government seeks to address the resultant economic fall-out, the nature of work will likely change and different jobs will be required. Accordingly, the report highlights a digital skills shortage in the UK, with recommendations for an expanded national retraining scheme to equip people to work alongside AI and, more generally, for improving the digital skills base in the UK. The report also recommends that the AI Council should identify those industries most at risk and the specific skills gaps in those industries, with the government then formulating training schemes to address those gaps.RegulationIn its 2018 report, the Select Committee concluded that 'Blanket AI-specific regulation, at this stage, would be inappropriate. We believe that existing sector-specific regulators are best placed to consider the impact on their sectors of any subsequent regulation which may be needed. We welcome that the [General Data Protection Regulation] appears to address many of the concernsregarding the handling of personal data'.The new report endorses this position and agrees that the challenges posed by the development and deployment of AI cannot currently be tackled by cross-cutting regulation.However, the report does raise significant concerns about gaps in regulation, including deficiencies in the existing legal framework for the use of AI by social media companies or in facial recognition technology. It also states that a sector-specific approach relies on relevant regulators being skilled at understanding the risks of AI and how to mitigate them and, as Simon Taylor of the CDEI puts it, the gap in 'understanding how to make sense of our existing laws, regulations and ethical standards' in the context of AI.The report suggests that the CDEI and Office for AI can play a cross-cutting role, along with the Information Commissioner's Office (ICO), to provide training and upskilling for sector-specific regulators. It recommends that the ICO (alongside the CDEI, the Office for AI and the Alan Turing Institute) develop a training course for use by regulators to ensure that their staff have a grounding in the ethical and appropriate use of personal data and AI systems, and its opportunities and risks. This training should be prepared and rolled out by July 2021.Role of governmentThe report cites various leading AI practitioners commending the collaboration that is happening across government and research institutions, and the success in establishing appropriate bodies to steward the development of AI in the UK. That said, the report suggests that more needs to be done. It recommends that a Cabinet Committee be established to set the strategic direction of AI policy in the UK, and the strategy for the use of data and technology by national and local government. It suggests that the first task of this committee should be to commission and approve a five year strategy for AI.The report also stresses the immediate need to appoint a Government Chief Data Officer. The government agreed the need for this role in 2017, and despite repeated commitments to the idea, has so far not taken steps to recruit for it.The report also underlines the importance of maintaining the UK's leading position on AI, and that attracting and retaining top AI talent should be a key focus of any future immigration policy.At this stage, the recommendations contained in the report are just that, and we will need to see the government's response to understand whether any will be adopted (and by when). [ 1]: https://publications.parliament.uk/pa/ld5801/ldselect/ldliaison/196/196.pdf [ 2]: https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100/100.pdf 
Classification
Language: English
Publication-Type: Web Blog
Journal Code: JSUP-5263
Subject: ARTIFICIAL INTELLIGENCE (90%); BRITISH PARLIAMENT (90%); GOVERNMENT & PUBLIC ADMINISTRATION (90%); LEGISLATIVE BODIES (90%); ETHICS (89%); PUBLIC RECORDS (77%); TRENDS (77%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (76%); DATA PROTECTION LAWS (72%); COVID-19 CORONAVIRUS REGULATION & POLICY (71%); APPOINTMENTS (56%)
Industry: ARTIFICIAL INTELLIGENCE (90%); INFORMATION SECURITY & PRIVACY (88%); INFORMATION TECHNOLOGY INDUSTRY (78%); OPEN DATA (78%); DATA PROTECTION LAWS (72%); DATA SECURITY (69%)
Geographic: UNITED KINGDOM (96%)
Load-Date: January 8, 2021",neutral,0.7804497480392456,balanced/neutral,"['privacy', 'bias', 'transparency', 'security', 'consent']",[],"['regulation', 'policy', 'oversight', 'standards', 'framework', 'should', 'must', 'need to']",['facial recognition'],5,0,8,1
2021,Unknown Title,"Byline: Martin Coulter
Body
Jan 21, 2021( The Business Insider: http://www.businessinsider.com/ Delivered by Newstex)  Google's new union has hit out against the firm's investigation of another AI ethicist. The tech giant has locked Margaret Mitchell out of her accounts, saying she exfiltrated and shared thousands of sensitive files with external accounts. The Alphabet Workers Union sided with Mitchell and hit out at Google for 'directly attacking' her. 
Visit Business Insider's homepage for more stories[1].The newly formed Alphabet Workers Union has hit out at Google after the firm confirmed it was investigating another senior AI ethicist within its ranks.On Wednesday, the tech giant told Insider it had suspended the corporate account of Margaret Mitchell[2], a lead on its ethical AI team. The firm said she had downloaded and shared thousands of sensitive documents with external accounts.The news was first reported by Dr. Timnit Gebru, who previously co-led Google's ethical AI division alongside Mitchell, but left the company at the end of last year after she said she was fired[3].""[Margaret's] not fired **yet**. But apparently they've told her she will be locked out for at least a few days,"" Gebru wrote on Twitter[4].A Google spokesperson confirmed Mitchell had been locked out of her corporate account and said it was because she had shared ""thousands"" of files with external accounts.""Our security systems automatically lock an employee's corporate account when they detect that the account is at risk of compromise due to credential problems or when an automated rule involving the handling of sensitive data has been triggered,"" the person said. The firm added that it was ""actively investigating"" the matter.Now the Alphabet Workers Union, which went public earlier this month[5], has hit out at the decision to suspend Mitchell, branding the move an ""attack on the people who are trying to make Google's technology more ethical.""The Alphabet Workers Union has sided with Mitchell.In a statement[6], the Union said: ""Regardless of the outcome of the company's investigation, the ongoing targeting of leaders in this organization calls into question Google's commitment to ethics - in AI and in their business practices.""Many members of the Ethical AI team are AWU members and the membership of our union recognizes the crucial work that they do and stands in solidarity with them in this moment.""The union claimed Google had departed from protocol by issuing any statement at all on Mitchell, and criticized the firm for having ""directly attacked Margaret.""It added: ""Being an individual targeted by one of the world's largest corporations is terrifying, and reinforces the need for unions in the workplace.""Whatever the outcome of the investigation into Mitchell, internal tensions are unlikely to die down. Several other members of Google's AI team publicly expressed support of Timnit Gebru[7] and criticized company AI chief Jeff Dean for the handling of her departure. Some have likewise expressed solidarity[8] for Mitchell.Business Insider approached Google for further comment. Read the original article on Business Insider[9] [ 1]: https://www.businessinsider.com/?hprecirc-bullet [ 2]: https://www.businessinsider.com/google-says-it-is-investigatin-ai-ethicist-margaret-mitchell-2021-1 [ 3]: https://www.businessinsider.com/inside-googles-firing-of-a-top-ai-ethicist-timnit-gebru-2020-12 [ 4]: https://twitter.com/timnitGebru/status/1351713824295038980 [ 5]: https://www.businessinsider.com/how-year-rising-tension-inside-google-led-its-first-union-2021-1 [ 6]: https://alphabetworkersunion.org/press/releases/retaliation-against-margaret-mitchell/ [ 7]: https://www.businessinsider.com/timnit-gebru-ethical-ai-fired-google-2020-12 [ 8]: https://twitter.com/alexhanna/status/1352070932605530114 [ 9]: https://www.businessinsider.com/margaret-mitchell-google-alphabet-workers-union-ai-ethicist-2021-1 
Classification
Language: English
Publication-Type: Web Blog
Journal Code: BZIN-5352
Subject: ETHICS (91%); ARTIFICIAL INTELLIGENCE ETHICS (90%); BUSINESS INVESTIGATIONS (90%); INVESTIGATIONS (90%); LABOR UNIONS (90%); NEGATIVE BUSINESS NEWS (90%); COMPANY ACTIVITIES & MANAGEMENT (78%); INTERNET SOCIAL NETWORKING (78%); DISMISSALS (76%); LAYOFFS & DISMISSALS (76%); AI (%); Tech Insider (%); Google (%); Technology (%); Timnit Gebru (%)
Company: GOOGLE LLC (97%)
Industry: NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (97%); ARTIFICIAL INTELLIGENCE ETHICS (90%); BIG TECH (89%); INTERNET SOCIAL NETWORKING (78%)
Load-Date: January 21, 2021","Jan 21, 2021( The Business Insider: http://www.businessinsider.com/ Delivered by Newstex)  Google's new union has hit out against the firm's investigation of another AI ethicist. The tech giant has locked Margaret Mitchell out of her accounts, saying she exfiltrated and shared thousands of sensitive files with external accounts. The Alphabet Workers Union sided with Mitchell and hit out at Google for 'directly attacking' her. 
Visit Business Insider's homepage for more stories[1].The newly formed Alphabet Workers Union has hit out at Google after the firm confirmed it was investigating another senior AI ethicist within its ranks.On Wednesday, the tech giant told Insider it had suspended the corporate account of Margaret Mitchell[2], a lead on its ethical AI team. The firm said she had downloaded and shared thousands of sensitive documents with external accounts.The news was first reported by Dr. Timnit Gebru, who previously co-led Google's ethical AI division alongside Mitchell, but left the company at the end of last year after she said she was fired[3].""[Margaret's] not fired **yet**. But apparently they've told her she will be locked out for at least a few days,"" Gebru wrote on Twitter[4].A Google spokesperson confirmed Mitchell had been locked out of her corporate account and said it was because she had shared ""thousands"" of files with external accounts.""Our security systems automatically lock an employee's corporate account when they detect that the account is at risk of compromise due to credential problems or when an automated rule involving the handling of sensitive data has been triggered,"" the person said. The firm added that it was ""actively investigating"" the matter.Now the Alphabet Workers Union, which went public earlier this month[5], has hit out at the decision to suspend Mitchell, branding the move an ""attack on the people who are trying to make Google's technology more ethical.""The Alphabet Workers Union has sided with Mitchell.In a statement[6], the Union said: ""Regardless of the outcome of the company's investigation, the ongoing targeting of leaders in this organization calls into question Google's commitment to ethics - in AI and in their business practices.""Many members of the Ethical AI team are AWU members and the membership of our union recognizes the crucial work that they do and stands in solidarity with them in this moment.""The union claimed Google had departed from protocol by issuing any statement at all on Mitchell, and criticized the firm for having ""directly attacked Margaret.""It added: ""Being an individual targeted by one of the world's largest corporations is terrifying, and reinforces the need for unions in the workplace.""Whatever the outcome of the investigation into Mitchell, internal tensions are unlikely to die down. Several other members of Google's AI team publicly expressed support of Timnit Gebru[7] and criticized company AI chief Jeff Dean for the handling of her departure. Some have likewise expressed solidarity[8] for Mitchell.Business Insider approached Google for further comment. Read the original article on Business Insider[9] [ 1]: https://www.businessinsider.com/?hprecirc-bullet [ 2]: https://www.businessinsider.com/google-says-it-is-investigatin-ai-ethicist-margaret-mitchell-2021-1 [ 3]: https://www.businessinsider.com/inside-googles-firing-of-a-top-ai-ethicist-timnit-gebru-2020-12 [ 4]: https://twitter.com/timnitGebru/status/1351713824295038980 [ 5]: https://www.businessinsider.com/how-year-rising-tension-inside-google-led-its-first-union-2021-1 [ 6]: https://alphabetworkersunion.org/press/releases/retaliation-against-margaret-mitchell/ [ 7]: https://www.businessinsider.com/timnit-gebru-ethical-ai-fired-google-2020-12 [ 8]: https://twitter.com/alexhanna/status/1352070932605530114 [ 9]: https://www.businessinsider.com/margaret-mitchell-google-alphabet-workers-union-ai-ethicist-2021-1 
Classification
Language: English
Publication-Type: Web Blog
Journal Code: BZIN-5352
Subject: ETHICS (91%); ARTIFICIAL INTELLIGENCE ETHICS (90%); BUSINESS INVESTIGATIONS (90%); INVESTIGATIONS (90%); LABOR UNIONS (90%); NEGATIVE BUSINESS NEWS (90%); COMPANY ACTIVITIES & MANAGEMENT (78%); INTERNET SOCIAL NETWORKING (78%); DISMISSALS (76%); LAYOFFS & DISMISSALS (76%); AI (%); Tech Insider (%); Google (%); Technology (%); Timnit Gebru (%)
Company: GOOGLE LLC (97%)
Industry: NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (97%); ARTIFICIAL INTELLIGENCE ETHICS (90%); BIG TECH (89%); INTERNET SOCIAL NETWORKING (78%)
Load-Date: January 21, 2021",neutral,0.62262362241745,balanced/neutral,['security'],[],[],[],1,0,0,0
2021,Unknown Title,"Byline: Targeted News Service
Dateline: PARIS, France 
Body
The United Nations Educational, Scientific and Cultural Organization issued the following news release on Nov. 25, 2021:
Audrey Azoulay, Director-General of UNESCO presented Thursday the first ever global standard on the ethics of artificial intelligence adopted by the member states of UNESCO at the General Conference.
This historical text defines the common values and principles which will guide the construction of the necessary legal infrastructure to ensure the healthy development of AI.
AI is pervasive, and enables many of our daily routines - booking flights, steering driverless cars, and personalising our morning news feeds. AI also supports the decision-making of governments and the private sector.
AI technologies are delivering remarkable results in highly specialized fields such as cancer screening and building inclusive environments for people with disabilities. They also help combat global problems like climate change and world hunger, and help reduce poverty by optimizing economic aid.
But the technology is also bringing new unprecedented challenges. We see increased gender and ethnic bias, significant threats to privacy, dignity and agency, dangers of mass surveillance, and increased use of unreliable AI technologies in law enforcement, to name a few. Until now, there were no universal standards to provide an answer to these issues.
In 2018, Audrey Azoulay, Director-General of UNESCO, launched an ambitious project: to give the world an ethical framework for the use of artificial intelligence. Three years later, thanks to the mobilization of hundreds of experts from around the world and intense international negotiations, the 193 UNESCO's member states have just officially adopted this ethical framework.
The world needs rules for artificial intelligence to benefit humanity. The Recommendation on the ethics of AI is a major answer. It sets the first global normative framework while giving States the responsibility to apply it at their level. UNESCO will support its 193 Member States in its implementation and ask them to report regularly on their progress and practices.
Audrey Azoulay, UNESCO Director-General
The content of the recommendation
The Recommendation aims to realize the advantages AI brings to society and reduce the risks it entails. It ensures that digital transformations promote human rights and contribute to the achievement of the Sustainable Development Goals, addressing issues around transparency, accountability and privacy, with action-oriented policy chapters on data governance, education, culture, labour, healthcare and the economy.
* Protecting data
The Recommendation calls for action beyond what tech firms and governments are doing to guarantee individuals more protection by ensuring transparency, agency and control over their personal data. It states that individuals should all be able to access or even erase records of their personal data. It also includes actions to improve data protection and an individual's knowledge of, and right to control, their own data. It also increases the ability of regulatory bodies around the world to enforce this.
* Banning social scoring and mass surveillance
The Recommendation explicitly bans the use of AI systems for social scoring and mass surveillance. These types of technologies are very invasive, they infringe on human rights and fundamental freedoms, and they are used in a broad way. The Recommendation stresses that when developing regulatory frameworks, Member States should consider that ultimate responsibility and accountability must always lie with humans and that AI technologies should not be given legal personality themselves.
* Helping to monitor and evaluate
The Recommendation also sets the ground for tools that will assist in its implementation. Ethical Impact Assessment is intended to help countries and companies developing and deploying AI systems to assess the impact of those systems on individuals, on society and on the environment. Readiness Assessment Methodology helps Member States to assess how ready they are in terms of legal and technical infrastructure. This tool will assist in enhancing the institutional capacity of countries and recommend appropriate measures to be taken in order to ensure that ethics are implemented in practice. In addition, the Recommendation encourages Member States to consider adding the role of an independent AI Ethics Officer or some other mechanism to oversee auditing and continuous monitoring efforts.
* Protecting the environment
The Recommendation emphasises that AI actors should favour data, energy and resource-efficient AI methods that will help ensure that AI becomes a more prominent tool in the fight against climate change and on tackling environmental issues. The Recommendation asks governments to assess the direct and indirect environmental impact throughout the AI system life cycle. This includes its carbon footprint, energy consumption and the environmental impact of raw material extraction for supporting the manufacturing of AI technologies. It also aims at reducing the environmental impact of AI systems and data infrastructures. It incentivizes governments to invest in green tech, and if there are disproportionate negative impact of AI systems on the environment, the Recommendation instruct that they should not be used.
Decisions impacting millions of people should be fair, transparent and contestable. These new technologies must help us address the major challenges in our world today, such as increased inequalities and the environmental crisis, and not deepening them.
Gabriela Ramos, UNESCO's Assistant Director-General for Social and Human Sciences
Emerging technologies such as AI have proven their immense capacity to deliver for good. However, its negative impacts that are exacerbating an already divided and unequal world, should be controlled. AI developments should abide by the rule of law, avoiding harm, and ensuring that when harm happens, accountability and redressal mechanisms are at hand for those affected.
* Read the full text: https://unesdoc.unesco.org/ark:/48223/pf0000379920
* More on ethics of artificial intelligence: https://en.unesco.org/artificial-intelligence/ethics
* More on the 24 experts (https://en.unesco.org/artificial-intelligence/ethics#aheg) who wrote the draft Recommendation
Contact: Clare O'Hagan, +33(0)145681729, c.o-hagan@unesco.org
MSTRUCK-7677769 MSTRUCK
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ARTIFICIAL INTELLIGENCE ETHICS (91%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); UNITED NATIONS (90%); UNITED NATIONS INSTITUTIONS (90%); AGREEMENTS (89%); CUSTOMS & CULTURAL HERITAGE (89%); SURVEILLANCE (89%); ASSOCIATIONS & ORGANIZATIONS (78%); INTERNATIONAL ASSISTANCE (78%); NEGATIVE SOCIETAL NEWS (78%); SUSTAINABLE DEVELOPMENT GOALS (78%); ENVIRONMENTAL TREATIES & AGREEMENTS (77%); SUSTAINABLE DEVELOPMENT (76%); HUMAN RIGHTS (73%); HUNGER IN SOCIETY (73%); NEGATIVE NEWS (73%); LAW ENFORCEMENT (72%); PRIVACY RIGHTS (72%); TREATIES & AGREEMENTS (72%); DISABLED PERSONS (69%); SUSTAINABILITY (69%); POVERTY & HOMELESSNESS (68%); POVERTY REDUCTION (68%); CANCER (54%); CANCER SCREENING & DETECTION (54%)
Company:  AI SYSTEMS (50%)
Industry: SIC7372 PREPACKAGED SOFTWARE (50%); ARTIFICIAL INTELLIGENCE ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); INFORMATION SECURITY & PRIVACY (89%); DATA SECURITY (84%); SUSTAINABLE DEVELOPMENT GOALS (78%); DATA GOVERNANCE & STEWARDSHIP (77%); DIGITALIZATION & DIGITAL TRANSFORMATION (77%); INFORMATION TECHNOLOGY INDUSTRY (77%); SUSTAINABLE DEVELOPMENT (76%)
Geographic: PARIS, FRANCE (59%); FRANCE (59%)
Load-Date: November 26, 2021","The United Nations Educational, Scientific and Cultural Organization issued the following news release on Nov. 25, 2021:
Audrey Azoulay, Director-General of UNESCO presented Thursday the first ever global standard on the ethics of artificial intelligence adopted by the member states of UNESCO at the General Conference.
This historical text defines the common values and principles which will guide the construction of the necessary legal infrastructure to ensure the healthy development of AI.
AI is pervasive, and enables many of our daily routines - booking flights, steering driverless cars, and personalising our morning news feeds. AI also supports the decision-making of governments and the private sector.
AI technologies are delivering remarkable results in highly specialized fields such as cancer screening and building inclusive environments for people with disabilities. They also help combat global problems like climate change and world hunger, and help reduce poverty by optimizing economic aid.
But the technology is also bringing new unprecedented challenges. We see increased gender and ethnic bias, significant threats to privacy, dignity and agency, dangers of mass surveillance, and increased use of unreliable AI technologies in law enforcement, to name a few. Until now, there were no universal standards to provide an answer to these issues.
In 2018, Audrey Azoulay, Director-General of UNESCO, launched an ambitious project: to give the world an ethical framework for the use of artificial intelligence. Three years later, thanks to the mobilization of hundreds of experts from around the world and intense international negotiations, the 193 UNESCO's member states have just officially adopted this ethical framework.
The world needs rules for artificial intelligence to benefit humanity. The Recommendation on the ethics of AI is a major answer. It sets the first global normative framework while giving States the responsibility to apply it at their level. UNESCO will support its 193 Member States in its implementation and ask them to report regularly on their progress and practices.
Audrey Azoulay, UNESCO Director-General
The content of the recommendation
The Recommendation aims to realize the advantages AI brings to society and reduce the risks it entails. It ensures that digital transformations promote human rights and contribute to the achievement of the Sustainable Development Goals, addressing issues around transparency, accountability and privacy, with action-oriented policy chapters on data governance, education, culture, labour, healthcare and the economy.
* Protecting data
The Recommendation calls for action beyond what tech firms and governments are doing to guarantee individuals more protection by ensuring transparency, agency and control over their personal data. It states that individuals should all be able to access or even erase records of their personal data. It also includes actions to improve data protection and an individual's knowledge of, and right to control, their own data. It also increases the ability of regulatory bodies around the world to enforce this.
* Banning social scoring and mass surveillance
The Recommendation explicitly bans the use of AI systems for social scoring and mass surveillance. These types of technologies are very invasive, they infringe on human rights and fundamental freedoms, and they are used in a broad way. The Recommendation stresses that when developing regulatory frameworks, Member States should consider that ultimate responsibility and accountability must always lie with humans and that AI technologies should not be given legal personality themselves.
* Helping to monitor and evaluate
The Recommendation also sets the ground for tools that will assist in its implementation. Ethical Impact Assessment is intended to help countries and companies developing and deploying AI systems to assess the impact of those systems on individuals, on society and on the environment. Readiness Assessment Methodology helps Member States to assess how ready they are in terms of legal and technical infrastructure. This tool will assist in enhancing the institutional capacity of countries and recommend appropriate measures to be taken in order to ensure that ethics are implemented in practice. In addition, the Recommendation encourages Member States to consider adding the role of an independent AI Ethics Officer or some other mechanism to oversee auditing and continuous monitoring efforts.
* Protecting the environment
The Recommendation emphasises that AI actors should favour data, energy and resource-efficient AI methods that will help ensure that AI becomes a more prominent tool in the fight against climate change and on tackling environmental issues. The Recommendation asks governments to assess the direct and indirect environmental impact throughout the AI system life cycle. This includes its carbon footprint, energy consumption and the environmental impact of raw material extraction for supporting the manufacturing of AI technologies. It also aims at reducing the environmental impact of AI systems and data infrastructures. It incentivizes governments to invest in green tech, and if there are disproportionate negative impact of AI systems on the environment, the Recommendation instruct that they should not be used.
Decisions impacting millions of people should be fair, transparent and contestable. These new technologies must help us address the major challenges in our world today, such as increased inequalities and the environmental crisis, and not deepening them.
Gabriela Ramos, UNESCO's Assistant Director-General for Social and Human Sciences
Emerging technologies such as AI have proven their immense capacity to deliver for good. However, its negative impacts that are exacerbating an already divided and unequal world, should be controlled. AI developments should abide by the rule of law, avoiding harm, and ensuring that when harm happens, accountability and redressal mechanisms are at hand for those affected.
* Read the full text: https://unesdoc.unesco.org/ark:/48223/pf0000379920
* More on ethics of artificial intelligence: https://en.unesco.org/artificial-intelligence/ethics
* More on the 24 experts (https://en.unesco.org/artificial-intelligence/ethics#aheg) who wrote the draft Recommendation
Contact: Clare O'Hagan, +33(0)145681729, c.o-hagan@unesco.org
MSTRUCK-7677769 MSTRUCK
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ARTIFICIAL INTELLIGENCE ETHICS (91%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); UNITED NATIONS (90%); UNITED NATIONS INSTITUTIONS (90%); AGREEMENTS (89%); CUSTOMS & CULTURAL HERITAGE (89%); SURVEILLANCE (89%); ASSOCIATIONS & ORGANIZATIONS (78%); INTERNATIONAL ASSISTANCE (78%); NEGATIVE SOCIETAL NEWS (78%); SUSTAINABLE DEVELOPMENT GOALS (78%); ENVIRONMENTAL TREATIES & AGREEMENTS (77%); SUSTAINABLE DEVELOPMENT (76%); HUMAN RIGHTS (73%); HUNGER IN SOCIETY (73%); NEGATIVE NEWS (73%); LAW ENFORCEMENT (72%); PRIVACY RIGHTS (72%); TREATIES & AGREEMENTS (72%); DISABLED PERSONS (69%); SUSTAINABILITY (69%); POVERTY & HOMELESSNESS (68%); POVERTY REDUCTION (68%); CANCER (54%); CANCER SCREENING & DETECTION (54%)
Company:  AI SYSTEMS (50%)
Industry: SIC7372 PREPACKAGED SOFTWARE (50%); ARTIFICIAL INTELLIGENCE ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); INFORMATION SECURITY & PRIVACY (89%); DATA SECURITY (84%); SUSTAINABLE DEVELOPMENT GOALS (78%); DATA GOVERNANCE & STEWARDSHIP (77%); DIGITALIZATION & DIGITAL TRANSFORMATION (77%); INFORMATION TECHNOLOGY INDUSTRY (77%); SUSTAINABLE DEVELOPMENT (76%)
Geographic: PARIS, FRANCE (59%); FRANCE (59%)
Load-Date: November 26, 2021",neutral,0.7472674250602722,balanced/neutral,"['privacy', 'surveillance', 'bias', 'transparency', 'accountability', 'security', 'human rights', 'agency', 'access', 'environmental impact']",['dignity'],"['policy', 'governance', 'standards', 'framework', 'law', 'should', 'must', 'recommend', 'calls for']",[],10,1,9,0
2021,Unknown Title,"Byline: John Naughton
Highlight: The departure of two members of the tech firm's ethical artificial intelligence team exposes the conflict at the heart of its business model
Body
If I told you that an academic paper entitled ""On the Dangers of Stochastic Parrots""  had caused an epochal row involving one of the most powerful companies in the world, you'd have asked what I'd been smoking. And well you might: but stay tuned.
The paper has four co-authors, two from the University of Washington, and two from Google - Dr Timnit Gebru and Dr Margaret Mitchell. It provides a useful critical review of machine-learning language models (LMs) like GPT-3 , which are trained on enormous amounts of text and are capable of producing plausible-looking prose. The amount of computation (and associated carbon emissions) involved in their construction has ballooned to insane levels, and so at some point it's sensible to ask the question that is never asked in the tech industry: how much is enough?
Which is one of the questions the authors of the paper asked. In answering them, they identified ""a wide variety of costs and risks associated with the rush for ever-larger LMs, including: environmental costs (borne typically by those not benefiting from the resulting technology); financial costs, which in turn erect barriers to entry, limiting who can contribute to this research area and which languages can benefit from the most advanced techniques; opportunity cost, as researchers pour effort away from directions requiring less resources; and the risk of substantial harms, including stereotyping, denigration, increases in extremist ideology, and wrongful arrest"".
These findings provide a useful counter-narrative to the tech industry's current Gadarene rush into language modelling. There was, however, one small difficulty. In 2018, Google created a language model called Bert, which was so powerful that the company incorporated it into its search engine - its core (and most lucrative) product. Google is accordingly ultra-sensitive to critiques of such a key technology. And two of the co-authors of the research paper were Google employees.
What happened next was predictable and crass, and there are competing narratives  about it. Gebru says she was fired, while Google says she resigned. Either way, the result was the same: in English employment law it would look like ""constructive dismissal""  - where an employee feels they have no choice but to resign because of something their employer has done. But whatever the explanation, Gebru is out. And so is her co-author and colleague, Mitchell , who had been trying to ascertain the grounds for Google's opposition to the research paper.
But now comes the really absurd part of the story. Gebru and Mitchell were both leading members of Google's Ethical AI team. In other words, in co-authoring the paper they were doing their job - which is to critically examine machine-learning technologies of the kind that are now central to their employer's business. And although their treatment (and subsequent online harassment by trolls) has been traumatic, it has at least highlighted the extent to which the tech industry's recent obsession with ""ethics"" is such a manipulative fraud.
As the industry's feeding frenzy on machine learning has gathered pace, so too has the proliferation of ethics boards, panels and oversight bodies established by the same companies. In creating them they have been assisted by entrepreneurial academics anxious to get a slice of the action. In that sense, lucrative consultancies to advise on ethical issues raised by machine learning have become a vast system of out-relief for otherwise unemployable philosophers and other sages. The result is a kind of ethics theatre akin to the security theatre enacted in airports in the years when people were actually permitted to fly. And the reason this farcical charade goes on is that tech companies see it as a pre-emptive strike to ward off what they really fear - regulation by law. 
The thing is that current machine-learning systems have ethical issues the way rats have fleas. Their intrinsic flaws include bias, unfairness, gender and ethnic discrimination, huge environmental footprints, theoretical flakiness and a crippled epistemology that equates volume of data with improved understanding. These limitations, however, have not prevented tech companies from adopting the technology wholesale, and indeed in some cases (Google and Facebook, to name just two) from betting the ranch on it.
Given that, you can imagine how senior executives respond to pesky researchers who point out the ethical difficulties implicit in such Faustian bargains. It may not be much consolation to Gebru and Mitchell, but at least their traumatic experience has already triggered promising initiatives. One is a vociferous campaign  by some of their Google colleagues. Even more promising is a student campaign - #recruitmenot  - aimed at persuading fellow students from joining tech companies. After all, if they wouldn't work for a tobacco company or an arms manufacturer, why would they work for Google or Facebook?
                     What I've been reading                   
                     Evolution or revolution?There's a thoughtful post  by Scott Alexander on his new Astral Codex Ten blog about whether radical change is better than organic development.
                     Online extremismIssie Lapowsky asks why social media companies were better at deplatforming Isis than at moderating domestic terrorists in an interesting Protocol essay. 
                     The one certaintyDoc Searls has written an astonishingly vivid meditation  on Medium about the planetary impact of our species, and why death is a feature, not a bug.
Classification
Language: ENGLISH
Publication-Type: Newspaper
Journal Code: WEBONS
Subject: ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); BUSINESS ETHICS (90%); COMPANY ACTIVITIES & MANAGEMENT (90%); TECHNOLOGY (90%); WRITERS (89%); RESIGNATIONS (84%); REPORTS, REVIEWS & SECTIONS (79%); MACHINE LEARNING (78%); POLITICAL & SOCIAL IDEOLOGIES (78%); RESEARCH REPORTS (75%); EMISSIONS (69%); DISMISSALS (65%); LABOR & EMPLOYMENT (65%); LAYOFFS & DISMISSALS (65%); LABOR & EMPLOYMENT LAW (60%); LABOR REGULATION & POLICY (60%); EDITORIALS & OPINIONS (59%)
Company: GOOGLE LLC (98%)
Industry: NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (98%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); INFORMATION TECHNOLOGY INDUSTRY (89%); WRITERS (89%); MACHINE LEARNING (78%); EMISSIONS (69%); SEARCH ENGINES (65%)
Geographic: ENGLAND (52%)
Load-Date: March 13, 2021","If I told you that an academic paper entitled ""On the Dangers of Stochastic Parrots""  had caused an epochal row involving one of the most powerful companies in the world, you'd have asked what I'd been smoking. And well you might: but stay tuned.
The paper has four co-authors, two from the University of Washington, and two from Google - Dr Timnit Gebru and Dr Margaret Mitchell. It provides a useful critical review of machine-learning language models (LMs) like GPT-3 , which are trained on enormous amounts of text and are capable of producing plausible-looking prose. The amount of computation (and associated carbon emissions) involved in their construction has ballooned to insane levels, and so at some point it's sensible to ask the question that is never asked in the tech industry: how much is enough?
Which is one of the questions the authors of the paper asked. In answering them, they identified ""a wide variety of costs and risks associated with the rush for ever-larger LMs, including: environmental costs (borne typically by those not benefiting from the resulting technology); financial costs, which in turn erect barriers to entry, limiting who can contribute to this research area and which languages can benefit from the most advanced techniques; opportunity cost, as researchers pour effort away from directions requiring less resources; and the risk of substantial harms, including stereotyping, denigration, increases in extremist ideology, and wrongful arrest"".
These findings provide a useful counter-narrative to the tech industry's current Gadarene rush into language modelling. There was, however, one small difficulty. In 2018, Google created a language model called Bert, which was so powerful that the company incorporated it into its search engine - its core (and most lucrative) product. Google is accordingly ultra-sensitive to critiques of such a key technology. And two of the co-authors of the research paper were Google employees.
What happened next was predictable and crass, and there are competing narratives  about it. Gebru says she was fired, while Google says she resigned. Either way, the result was the same: in English employment law it would look like ""constructive dismissal""  - where an employee feels they have no choice but to resign because of something their employer has done. But whatever the explanation, Gebru is out. And so is her co-author and colleague, Mitchell , who had been trying to ascertain the grounds for Google's opposition to the research paper.
But now comes the really absurd part of the story. Gebru and Mitchell were both leading members of Google's Ethical AI team. In other words, in co-authoring the paper they were doing their job - which is to critically examine machine-learning technologies of the kind that are now central to their employer's business. And although their treatment (and subsequent online harassment by trolls) has been traumatic, it has at least highlighted the extent to which the tech industry's recent obsession with ""ethics"" is such a manipulative fraud.
As the industry's feeding frenzy on machine learning has gathered pace, so too has the proliferation of ethics boards, panels and oversight bodies established by the same companies. In creating them they have been assisted by entrepreneurial academics anxious to get a slice of the action. In that sense, lucrative consultancies to advise on ethical issues raised by machine learning have become a vast system of out-relief for otherwise unemployable philosophers and other sages. The result is a kind of ethics theatre akin to the security theatre enacted in airports in the years when people were actually permitted to fly. And the reason this farcical charade goes on is that tech companies see it as a pre-emptive strike to ward off what they really fear - regulation by law. 
The thing is that current machine-learning systems have ethical issues the way rats have fleas. Their intrinsic flaws include bias, unfairness, gender and ethnic discrimination, huge environmental footprints, theoretical flakiness and a crippled epistemology that equates volume of data with improved understanding. These limitations, however, have not prevented tech companies from adopting the technology wholesale, and indeed in some cases (Google and Facebook, to name just two) from betting the ranch on it.
Given that, you can imagine how senior executives respond to pesky researchers who point out the ethical difficulties implicit in such Faustian bargains. It may not be much consolation to Gebru and Mitchell, but at least their traumatic experience has already triggered promising initiatives. One is a vociferous campaign  by some of their Google colleagues. Even more promising is a student campaign - #recruitmenot  - aimed at persuading fellow students from joining tech companies. After all, if they wouldn't work for a tobacco company or an arms manufacturer, why would they work for Google or Facebook?
                     What I've been reading                   
                     Evolution or revolution?There's a thoughtful post  by Scott Alexander on his new Astral Codex Ten blog about whether radical change is better than organic development.
                     Online extremismIssie Lapowsky asks why social media companies were better at deplatforming Isis than at moderating domestic terrorists in an interesting Protocol essay. 
                     The one certaintyDoc Searls has written an astonishingly vivid meditation  on Medium about the planetary impact of our species, and why death is a feature, not a bug.
Classification
Language: ENGLISH
Publication-Type: Newspaper
Journal Code: WEBONS
Subject: ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); BUSINESS ETHICS (90%); COMPANY ACTIVITIES & MANAGEMENT (90%); TECHNOLOGY (90%); WRITERS (89%); RESIGNATIONS (84%); REPORTS, REVIEWS & SECTIONS (79%); MACHINE LEARNING (78%); POLITICAL & SOCIAL IDEOLOGIES (78%); RESEARCH REPORTS (75%); EMISSIONS (69%); DISMISSALS (65%); LABOR & EMPLOYMENT (65%); LAYOFFS & DISMISSALS (65%); LABOR & EMPLOYMENT LAW (60%); LABOR REGULATION & POLICY (60%); EDITORIALS & OPINIONS (59%)
Company: GOOGLE LLC (98%)
Industry: NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (98%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); INFORMATION TECHNOLOGY INDUSTRY (89%); WRITERS (89%); MACHINE LEARNING (78%); EMISSIONS (69%); SEARCH ENGINES (65%)
Geographic: ENGLAND (52%)
Load-Date: March 13, 2021",neutral,0.6958965063095093,balanced/neutral,"['bias', 'discrimination', 'security']",[],"['regulation', 'policy', 'oversight', 'law']","['machine learning', 'gpt', 'bert']",3,0,4,3
2021,Unknown Title,"Body
INFOSYS LTD (""INFY-N"") - Recognized as One of the World's Most Ethical Companies by - Ethisphere Institute
Infosys a global leader in next-generation digital services and consulting, announced that it has been recognized by Ethisphere Institute, the global leader in defining and advancing the standards of ethical business practices, as one of the world's most ethical companies for 2021. Infosys was distinguished for its undiluted commitment towards integrity and making value-based decisions. Through this coveted recognition, Infosys has become one of only four honorees in the Software &Services Industry globally, and one of the only three honorees in India.
In 2021, 135 honorees were recognized spanning 22 countries and 47 industries. This year, the process was streamlined, and question set expanded to gauge how companies are adapting and responding to the global health pandemic, environmental, social and governance factors, safety, equity, inclusion, and social justice. Infosys showcased a strong connection between ethical practices and solid performance in the global market. The recognition additionally spotlighted Infosys in the areas of ethics and compliance, diversity, governance, and social initiatives.
Timothy Erblich, Chief Executive Officer of Ethisphere, said, ""2020 ushered many hardships for the world putting us through one of the most difficult tests of all times. While addressing various tough challenges, we saw companies not just working towards earning the trust of stakeholders through resilience and a commitment to ethics, transparency, and integrity but advancing their corporate cultures for the greater good. We are delighted to acknowledge Infosys for its firm commitment to creating the highest value for the communities they serve while prioritizing social imperatives. We would like to congratulate everyone at Infosys for earning this designation.""
Salil Parekh, Chief Executive Officer and Managing Director, Infosys, said, ""We are extremely honored to receive this prestigious recognition from the Ethisphere Institute. Flawless execution with integrity and compliance is the cornerstone of our continued success, and we firmly believe that an integrity-based approach greatly influences business success. Infosys' core values are the foundation on which we have built our success over the years. This acknowledgement further encourages us to stay committed to operate with high ethical standards and transparency, especially in these unprecedented times.""
The complete list of 2021 World's Most Ethical Companies can be found at: https://worldsmostethicalcompanies.com/honorees
Methodology &Scoring
Grounded in Ethisphere's proprietary Ethics Quotient(R), the World's Most Ethical Companies assessment process includes more than 200 questions on culture, environmental and social practices, ethics and compliance activities, governance, diversity and initiatives to support a strong value chain. The process serves as an operating framework to capture and codify the leading practices of organizations across industries and around the globe.
About Infosys
Infosys is a global leader in next-generation digital services and consulting. We enable clients in 46 countries to navigate their digital transformation. With nearly four decades of experience in managing the systems and workings of global enterprises, we expertly steer our clients through their digital journey. We do it by enabling the enterprise with an AI-powered core that helps prioritize the execution of change. We also empower the business with agile digital at scale to deliver unprecedented levels of performance and customer delight. Our always-on learning agenda drives their continuous improvement through building and transferring digital skills, expertise, and ideas from our innovation ecosystem.
Visit www.infosys.com to see how Infosys (NYSE: INFY) can help your enterprise navigate your next.
Safe Harbor
Certain statements in this release concerning our future growth prospects, financial expectations and plans for navigating the COVID-19 impact on our employees, clients and stakeholders are forward-looking statements intended to qualify for the 'safe harbor' under the Private Securities Litigation Reform Act of 1995, which involve a number of risks and uncertainties that could cause actual results to differ materially from those in such forward-looking statements. The risks and uncertainties relating to these statements include, but are not limited to, risks and uncertainties regarding COVID-19 and the effects of government and other measures seeking to contain its spread, risks related to an economic downturn or recession in India, the United States and other countries around the world, changes in political, business, and economic conditions, fluctuations in earnings, fluctuations in foreign exchange rates, our ability to manage growth, intense competition in IT services including those factors which may affect our cost advantage, wage increases in India, our ability to attract and retain highly skilled professionals, time and cost overruns on fixed-price, fixed-time frame contracts, client concentration, restrictions on immigration, industry segment concentration, our ability to manage our international operations, reduced demand for technology in our key focus areas, disruptions in telecommunication networks or system failures, our ability to successfully complete and integrate potential acquisitions, liability for damages on our service contracts, the success of the companies in which Infosys has made strategic investments, withdrawal or expiration of governmental fiscal incentives, political instability and regional conflicts, legal restrictions on raising capital or acquiring companies outside India, unauthorized use of our intellectual property and general economic conditions affecting our industry and the outcome of pending litigation and government investigation. Additional risks that could affect our future operating results are more fully described in our United States Securities and Exchange Commission filings including our Annual Report on Form 20-F for the fiscal year ended March 31, 2020. These filings are available at www.sec.gov. Infosys may, from time to time, make additional written and oral forward-looking statements, including statements contained in the Company's filings with the Securities and Exchange Commission and our reports to shareholders. The Company does not undertake to update any forward-looking statements that may be made from time to time by or on behalf of the Company unless it is required by law. Contact Information: For further information: For more information contact [email protected]
______________________________
_______________________________________________
____________________________________________________________
(c)2021 Market News Publishing Inc. All rights reserved. Toronto:(416)366-8881 Vancouver:(604)689-1101 Fax:(604)689-1106
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: MNP
Subject: ETHICS (93%); BUSINESS ETHICS (91%); COMPANY ACTIVITIES & MANAGEMENT (90%); ESG FACTORS - SOCIAL (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (90%); CORPORATE CULTURE (89%); EXECUTIVES (89%); ESG FACTORS (78%); SOCIAL JUSTICE (78%); MANAGERS & SUPERVISORS (73%); SAFETY (73%); REGULATORY COMPLIANCE (71%); VALUE CHAIN (70%); ASSOCIATIONS & ORGANIZATIONS (68%)
Company:  INFOSYS LTD (96%); INFOSYS LTD
Ticker: INFY (PAR) (96%); INFY (NYSE) (96%); INFY (NSE) (96%); INFY; (New York)
Industry: NAICS541511 CUSTOM COMPUTER PROGRAMMING SERVICES (96%); SIC7371 COMPUTER PROGRAMMING SERVICES (96%)
Person: SALIL PAREKH (79%)
Geographic: INDIA (76%)
Load-Date: February 23, 2021","INFOSYS LTD (""INFY-N"") - Recognized as One of the World's Most Ethical Companies by - Ethisphere Institute
Infosys a global leader in next-generation digital services and consulting, announced that it has been recognized by Ethisphere Institute, the global leader in defining and advancing the standards of ethical business practices, as one of the world's most ethical companies for 2021. Infosys was distinguished for its undiluted commitment towards integrity and making value-based decisions. Through this coveted recognition, Infosys has become one of only four honorees in the Software &Services Industry globally, and one of the only three honorees in India.
In 2021, 135 honorees were recognized spanning 22 countries and 47 industries. This year, the process was streamlined, and question set expanded to gauge how companies are adapting and responding to the global health pandemic, environmental, social and governance factors, safety, equity, inclusion, and social justice. Infosys showcased a strong connection between ethical practices and solid performance in the global market. The recognition additionally spotlighted Infosys in the areas of ethics and compliance, diversity, governance, and social initiatives.
Timothy Erblich, Chief Executive Officer of Ethisphere, said, ""2020 ushered many hardships for the world putting us through one of the most difficult tests of all times. While addressing various tough challenges, we saw companies not just working towards earning the trust of stakeholders through resilience and a commitment to ethics, transparency, and integrity but advancing their corporate cultures for the greater good. We are delighted to acknowledge Infosys for its firm commitment to creating the highest value for the communities they serve while prioritizing social imperatives. We would like to congratulate everyone at Infosys for earning this designation.""
Salil Parekh, Chief Executive Officer and Managing Director, Infosys, said, ""We are extremely honored to receive this prestigious recognition from the Ethisphere Institute. Flawless execution with integrity and compliance is the cornerstone of our continued success, and we firmly believe that an integrity-based approach greatly influences business success. Infosys' core values are the foundation on which we have built our success over the years. This acknowledgement further encourages us to stay committed to operate with high ethical standards and transparency, especially in these unprecedented times.""
The complete list of 2021 World's Most Ethical Companies can be found at: https://worldsmostethicalcompanies.com/honorees
Methodology &Scoring
Grounded in Ethisphere's proprietary Ethics Quotient(R), the World's Most Ethical Companies assessment process includes more than 200 questions on culture, environmental and social practices, ethics and compliance activities, governance, diversity and initiatives to support a strong value chain. The process serves as an operating framework to capture and codify the leading practices of organizations across industries and around the globe.
About Infosys
Infosys is a global leader in next-generation digital services and consulting. We enable clients in 46 countries to navigate their digital transformation. With nearly four decades of experience in managing the systems and workings of global enterprises, we expertly steer our clients through their digital journey. We do it by enabling the enterprise with an AI-powered core that helps prioritize the execution of change. We also empower the business with agile digital at scale to deliver unprecedented levels of performance and customer delight. Our always-on learning agenda drives their continuous improvement through building and transferring digital skills, expertise, and ideas from our innovation ecosystem.
Visit www.infosys.com to see how Infosys (NYSE: INFY) can help your enterprise navigate your next.
Safe Harbor
Certain statements in this release concerning our future growth prospects, financial expectations and plans for navigating the COVID-19 impact on our employees, clients and stakeholders are forward-looking statements intended to qualify for the 'safe harbor' under the Private Securities Litigation Reform Act of 1995, which involve a number of risks and uncertainties that could cause actual results to differ materially from those in such forward-looking statements. The risks and uncertainties relating to these statements include, but are not limited to, risks and uncertainties regarding COVID-19 and the effects of government and other measures seeking to contain its spread, risks related to an economic downturn or recession in India, the United States and other countries around the world, changes in political, business, and economic conditions, fluctuations in earnings, fluctuations in foreign exchange rates, our ability to manage growth, intense competition in IT services including those factors which may affect our cost advantage, wage increases in India, our ability to attract and retain highly skilled professionals, time and cost overruns on fixed-price, fixed-time frame contracts, client concentration, restrictions on immigration, industry segment concentration, our ability to manage our international operations, reduced demand for technology in our key focus areas, disruptions in telecommunication networks or system failures, our ability to successfully complete and integrate potential acquisitions, liability for damages on our service contracts, the success of the companies in which Infosys has made strategic investments, withdrawal or expiration of governmental fiscal incentives, political instability and regional conflicts, legal restrictions on raising capital or acquiring companies outside India, unauthorized use of our intellectual property and general economic conditions affecting our industry and the outcome of pending litigation and government investigation. Additional risks that could affect our future operating results are more fully described in our United States Securities and Exchange Commission filings including our Annual Report on Form 20-F for the fiscal year ended March 31, 2020. These filings are available at www.sec.gov. Infosys may, from time to time, make additional written and oral forward-looking statements, including statements contained in the Company's filings with the Securities and Exchange Commission and our reports to shareholders. The Company does not undertake to update any forward-looking statements that may be made from time to time by or on behalf of the Company unless it is required by law. Contact Information: For further information: For more information contact [email protected]
______________________________
_______________________________________________
____________________________________________________________
(c)2021 Market News Publishing Inc. All rights reserved. Toronto:(416)366-8881 Vancouver:(604)689-1101 Fax:(604)689-1106
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: MNP
Subject: ETHICS (93%); BUSINESS ETHICS (91%); COMPANY ACTIVITIES & MANAGEMENT (90%); ESG FACTORS - SOCIAL (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (90%); CORPORATE CULTURE (89%); EXECUTIVES (89%); ESG FACTORS (78%); SOCIAL JUSTICE (78%); MANAGERS & SUPERVISORS (73%); SAFETY (73%); REGULATORY COMPLIANCE (71%); VALUE CHAIN (70%); ASSOCIATIONS & ORGANIZATIONS (68%)
Company:  INFOSYS LTD (96%); INFOSYS LTD
Ticker: INFY (PAR) (96%); INFY (NYSE) (96%); INFY (NSE) (96%); INFY; (New York)
Industry: NAICS541511 CUSTOM COMPUTER PROGRAMMING SERVICES (96%); SIC7371 COMPUTER PROGRAMMING SERVICES (96%)
Person: SALIL PAREKH (79%)
Geographic: INDIA (76%)
Load-Date: February 23, 2021",positive,0.8889617323875427,balanced/neutral,"['transparency', 'safety']","['justice', 'equity', 'justice']","['governance', 'standards', 'framework', 'law', 'compliance']",[],2,3,5,0
2021,Unknown Title,"Byline: Calum Muirhead
Body
Sensyne Health PLC (LON:SENS) said it appointed David Ruau to the newly created role of chief scientific officer with effect from September 1.
The ethical artificial intelligence (AI) firm said Ruau will lead the company's research effort aimed at applying ethical AI to improve patient care and discover and develop new medicines.
READ: Sensyne Health mulls second listing on Nasdaq
Ruau joins the company from German pharma giant Bayer, where he served as head of global data assets and decision science and led teams working on the development and application of AI solutions in oncology, radiology and women's health.
Before joining Bayer, Ruau worked as head of scientific computing solutions at AstraZeneca PLC (LON:AZN) where he focused on data science solutions in the late-phase development pipeline to improve efficiency in designing and analysing clinical trials.
""I am thrilled to be joining Sensyne at such an exciting time for the business and for the pharmaceutical AI sector. The ethical application of AI to improve healthcare for patients and life sciences research is one of my greatest passions. Sensyne has a unique model that offers the potential to transform pharmaceutical development and I am greatly looking forward to joining the team"", Ruau said in a statement.
""With his wealth of experience in bioinformatics, pharma and data science, I am delighted to welcome David to Sensyne. His experience will be invaluable as we continue to innovate and develop our work applying ethical AI to improve patient care and to accelerate the development of new medicines"", added Sensyne chief executive Paul Drayson.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: DRUG DESIGN & DISCOVERY (91%); APPOINTMENTS (90%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ETHICS (90%); EXECUTIVE MOVES (90%); RESEARCH & DEVELOPMENT (90%); EXECUTIVES (89%); EXPERIMENTATION & RESEARCH (78%); BIOINFORMATICS (73%); COMPUTATIONAL RESEARCH (73%); WOMEN'S HEALTH (73%); ONCOLOGY (72%); Small caps (%)
Company:  BAYER AG (84%);  ASTRAZENECA PLC (57%); Sensyne Health PLC
Ticker: BAYN (FRA) (84%); BAY (MCE) (84%); BAY (BIT) (84%); AZN (STO) (57%); AZN (NYSE) (57%); AZN (LSE) (57%)
Industry: NAICS325412 PHARMACEUTICAL PREPARATION MANUFACTURING (84%); NAICS325320 PESTICIDE & OTHER AGRICULTURAL CHEMICAL MANUFACTURING (84%); NAICS325211 PLASTICS MATERIAL & RESIN MANUFACTURING (84%); SIC2834 PHARMACEUTICAL PREPARATIONS (57%); DRUG DESIGN & DISCOVERY (91%); PHARMACEUTICALS & BIOTECHNOLOGY (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); PHARMACEUTICAL PREPARATION MFG (90%); PHARMACEUTICALS INDUSTRY (90%); PHARMACEUTICALS PRODUCT DEVELOPMENT (90%); COMPUTATIONAL RESEARCH (73%); ONCOLOGY (72%); Pharma & Biotech (%)
Load-Date: July 22, 2021","Sensyne Health PLC (LON:SENS) said it appointed David Ruau to the newly created role of chief scientific officer with effect from September 1.
The ethical artificial intelligence (AI) firm said Ruau will lead the company's research effort aimed at applying ethical AI to improve patient care and discover and develop new medicines.
READ: Sensyne Health mulls second listing on Nasdaq
Ruau joins the company from German pharma giant Bayer, where he served as head of global data assets and decision science and led teams working on the development and application of AI solutions in oncology, radiology and women's health.
Before joining Bayer, Ruau worked as head of scientific computing solutions at AstraZeneca PLC (LON:AZN) where he focused on data science solutions in the late-phase development pipeline to improve efficiency in designing and analysing clinical trials.
""I am thrilled to be joining Sensyne at such an exciting time for the business and for the pharmaceutical AI sector. The ethical application of AI to improve healthcare for patients and life sciences research is one of my greatest passions. Sensyne has a unique model that offers the potential to transform pharmaceutical development and I am greatly looking forward to joining the team"", Ruau said in a statement.
""With his wealth of experience in bioinformatics, pharma and data science, I am delighted to welcome David to Sensyne. His experience will be invaluable as we continue to innovate and develop our work applying ethical AI to improve patient care and to accelerate the development of new medicines"", added Sensyne chief executive Paul Drayson.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: DRUG DESIGN & DISCOVERY (91%); APPOINTMENTS (90%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ETHICS (90%); EXECUTIVE MOVES (90%); RESEARCH & DEVELOPMENT (90%); EXECUTIVES (89%); EXPERIMENTATION & RESEARCH (78%); BIOINFORMATICS (73%); COMPUTATIONAL RESEARCH (73%); WOMEN'S HEALTH (73%); ONCOLOGY (72%); Small caps (%)
Company:  BAYER AG (84%);  ASTRAZENECA PLC (57%); Sensyne Health PLC
Ticker: BAYN (FRA) (84%); BAY (MCE) (84%); BAY (BIT) (84%); AZN (STO) (57%); AZN (NYSE) (57%); AZN (LSE) (57%)
Industry: NAICS325412 PHARMACEUTICAL PREPARATION MANUFACTURING (84%); NAICS325320 PESTICIDE & OTHER AGRICULTURAL CHEMICAL MANUFACTURING (84%); NAICS325211 PLASTICS MATERIAL & RESIN MANUFACTURING (84%); SIC2834 PHARMACEUTICAL PREPARATIONS (57%); DRUG DESIGN & DISCOVERY (91%); PHARMACEUTICALS & BIOTECHNOLOGY (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); PHARMACEUTICAL PREPARATION MFG (90%); PHARMACEUTICALS INDUSTRY (90%); PHARMACEUTICALS PRODUCT DEVELOPMENT (90%); COMPUTATIONAL RESEARCH (73%); ONCOLOGY (72%); Pharma & Biotech (%)
Load-Date: July 22, 2021",neutral,0.6258342862129211,balanced/neutral,[],[],[],[],0,0,0,0
2021,Unknown Title,"Byline: Thomas Fox
Body
Sep 24, 2021( JD Supra: http://www.jdsupra.com Delivered by Newstex)  
 A successful whistleblowing program doesn't start with installing a helpline-it starts with fostering an environment that protects whistleblowers, makes them feel supported, and makes clear the value they bring to the business. So how do you build that ""speak-up culture?"" Join this session to hear from a panel of practitioners who manage whistleblowing programs and whistleblower advocates who'll share their insights, experiences, and challenges they've Seemore+ A successful whistleblowing program doesn't start with installing a helpline-it starts with fostering an environment that protects whistleblowers, makes them feel supported, and makes clear the value they bring to the business. So how do you build that ""speak-up culture?"" Join this session to hear from a panel of practitioners who manage whistleblowing programs and whistleblower advocates who'll share their insights, experiences, and challenges they've faced. 
 In this episode of The Ethics Movement, I visit Philip Winterburn, Convercent by OneTrust Chief Strategy Officer and talk about his presentation at Converge21 on Digital Ethics: AI, Privacy, and More. We discuss why it's high time to consider the ethics behind practices, including AI, machine learning, and consumer data collection, and how companies can embrace new technologies, scale quickly, and continue evolving while maintaining their values. Seeless- 
Classification
Language: English
Publication-Type: Web Blog
Journal Code: JSUP-5263
Subject: ETHICS (90%); PSYCHOLOGICAL SAFETY (90%); WHISTLEBLOWER PROTECTION (90%); WHISTLEBLOWERS (90%); ARTIFICIAL INTELLIGENCE (77%); MACHINE LEARNING (72%); COMPANY STRATEGY (71%); EXECUTIVES (71%)
Industry: PSYCHOLOGICAL SAFETY (90%); ARTIFICIAL INTELLIGENCE (77%); MACHINE LEARNING (72%)
Load-Date: September 24, 2021","Sep 24, 2021( JD Supra: http://www.jdsupra.com Delivered by Newstex)  
 A successful whistleblowing program doesn't start with installing a helpline-it starts with fostering an environment that protects whistleblowers, makes them feel supported, and makes clear the value they bring to the business. So how do you build that ""speak-up culture?"" Join this session to hear from a panel of practitioners who manage whistleblowing programs and whistleblower advocates who'll share their insights, experiences, and challenges they've Seemore+ A successful whistleblowing program doesn't start with installing a helpline-it starts with fostering an environment that protects whistleblowers, makes them feel supported, and makes clear the value they bring to the business. So how do you build that ""speak-up culture?"" Join this session to hear from a panel of practitioners who manage whistleblowing programs and whistleblower advocates who'll share their insights, experiences, and challenges they've faced. 
 In this episode of The Ethics Movement, I visit Philip Winterburn, Convercent by OneTrust Chief Strategy Officer and talk about his presentation at Converge21 on Digital Ethics: AI, Privacy, and More. We discuss why it's high time to consider the ethics behind practices, including AI, machine learning, and consumer data collection, and how companies can embrace new technologies, scale quickly, and continue evolving while maintaining their values. Seeless- 
Classification
Language: English
Publication-Type: Web Blog
Journal Code: JSUP-5263
Subject: ETHICS (90%); PSYCHOLOGICAL SAFETY (90%); WHISTLEBLOWER PROTECTION (90%); WHISTLEBLOWERS (90%); ARTIFICIAL INTELLIGENCE (77%); MACHINE LEARNING (72%); COMPANY STRATEGY (71%); EXECUTIVES (71%)
Industry: PSYCHOLOGICAL SAFETY (90%); ARTIFICIAL INTELLIGENCE (77%); MACHINE LEARNING (72%)
Load-Date: September 24, 2021",neutral,0.7870039939880371,balanced/neutral,"['privacy', 'safety']",[],[],['machine learning'],2,0,0,1
2021,Unknown Title,"Body
November 24, 2021
The ""Ethical AI is Pivotal to the Maximization of the Future Growth Potential of the Global AI Market"" report has been added to ResearchAndMarkets.com's offering.
Artificial intelligence (AI) is transforming organizations, industries, and the technology landscape. The world is moving to the increased adoption of AI-powered smart applications/systems, and this trend will increase exponentially over the next few years. AI technologies are maturing, and the need to leverage their capabilities is becoming a CXO priority.As businesses make AI part of their core strategy, the transformation of business functions, measures, and controls to ensure ethical best practices will gain importance. The implementation and the governance of ethical AI practices will become a priority and a board-level concern.
The deployment of AI solutions that are ethical (from a regulatory and a legal standpoint), transparent, and without bias will become essential. As governments and industry bodies across the world articulate AI regulations, AI companies must establish their ethical frameworks until roadmaps are clearly defined.The operationalization of ethical AI principles is challenging for enterprises, given the large volumes of user-centric data that need to be processed, the breadth of use-cases, the regulatory variations in operating markets, and the diverse stakeholder priorities.
This also opens up opportunities for technology vendors and service providers. To effectively partner with enterprises and monetize these opportunities, ICT providers need to assess potential areas impacting AI ethics and evaluate opportunities across the people-process-technology spectrum.Forward-thinking technology and service companies, including large ICT providers and start-ups, are working with enterprises and industry stakeholders to leverage potential opportunities. Ethical challenges will continue to be discovered and remediated to create sustained growth in potential advisory services.
As enterprises define goals, values, strategic outcomes, and key performance metrics, the time is right for technology companies to strategically partner with enterprises in the detection and the mitigation of ethical AI concerns.
Key Topics Covered:
1. Strategic Imperatives
Why is it Increasingly Difficult to Grow?
The Strategic Imperative 
The Impact of the Top Three Strategic Imperatives on Ethical AI
Growth Opportunities Fuel the Growth Pipeline Engine
2. Growth Environment
AI Growth Drivers
AI Key Use-Cases
AI Ethics and Challenges to Adoption
3. Growth Opportunity Analysis
Increasing Ecosystem Focus on Ethical AI
Ethical AI Principles Require the Addressal of Multiple Elements/Dimensions
Growth Opportunity Segments
People
Process
Technology
The Way Forward
4. Growth Opportunity Universe
Growth Opportunity 1: Consulting and Advisory Services for AI Roadmaps and Ethical Considerations
For more information about this report visit https://www.researchandmarkets.com/r/2nklb1
CONTACT: 
  ResearchAndMarkets.com
 Laura Wood, Senior Press Manager
  press@researchandmarkets.com
  For E.S.T Office Hours Call 1-917-300-0470
  For U.S./CAN Toll Free Call 1-800-526-8630
  For GMT Office Hours Call +353-1-416-8900          
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: M2PW
Subject: ARTIFICIAL INTELLIGENCE ETHICS (94%); MARKET RESEARCH REPORTS (92%); ARTIFICIAL INTELLIGENCE (90%); BUSINESS REPORTS & FORECASTS (90%); ETHICS (90%); FINANCIAL MARKET UPDATES (90%); PRESS RELEASES (90%); REPORTS, REVIEWS & SECTIONS (90%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (78%); BEST PRACTICES (78%); TRENDS (78%); BUSINESS & PROFESSIONAL ASSOCIATIONS (77%); RESEARCH REPORTS (77%); TECHNOLOGY MATURITY (77%); BUSINESS METRICS (75%); ASSOCIATIONS & ORGANIZATIONS (72%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (94%); MARKET RESEARCH REPORTS (92%); ARTIFICIAL INTELLIGENCE (90%); FINANCIAL MARKET UPDATES (90%); MARKET RESEARCH (90%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (78%); INFORMATION TECHNOLOGY INDUSTRY (78%); MARKET RESEARCH & ANALYSIS (77%)
Load-Date: November 24, 2021","November 24, 2021
The ""Ethical AI is Pivotal to the Maximization of the Future Growth Potential of the Global AI Market"" report has been added to ResearchAndMarkets.com's offering.
Artificial intelligence (AI) is transforming organizations, industries, and the technology landscape. The world is moving to the increased adoption of AI-powered smart applications/systems, and this trend will increase exponentially over the next few years. AI technologies are maturing, and the need to leverage their capabilities is becoming a CXO priority.As businesses make AI part of their core strategy, the transformation of business functions, measures, and controls to ensure ethical best practices will gain importance. The implementation and the governance of ethical AI practices will become a priority and a board-level concern.
The deployment of AI solutions that are ethical (from a regulatory and a legal standpoint), transparent, and without bias will become essential. As governments and industry bodies across the world articulate AI regulations, AI companies must establish their ethical frameworks until roadmaps are clearly defined.The operationalization of ethical AI principles is challenging for enterprises, given the large volumes of user-centric data that need to be processed, the breadth of use-cases, the regulatory variations in operating markets, and the diverse stakeholder priorities.
This also opens up opportunities for technology vendors and service providers. To effectively partner with enterprises and monetize these opportunities, ICT providers need to assess potential areas impacting AI ethics and evaluate opportunities across the people-process-technology spectrum.Forward-thinking technology and service companies, including large ICT providers and start-ups, are working with enterprises and industry stakeholders to leverage potential opportunities. Ethical challenges will continue to be discovered and remediated to create sustained growth in potential advisory services.
As enterprises define goals, values, strategic outcomes, and key performance metrics, the time is right for technology companies to strategically partner with enterprises in the detection and the mitigation of ethical AI concerns.
Key Topics Covered:
1. Strategic Imperatives
Why is it Increasingly Difficult to Grow?
The Strategic Imperative 
The Impact of the Top Three Strategic Imperatives on Ethical AI
Growth Opportunities Fuel the Growth Pipeline Engine
2. Growth Environment
AI Growth Drivers
AI Key Use-Cases
AI Ethics and Challenges to Adoption
3. Growth Opportunity Analysis
Increasing Ecosystem Focus on Ethical AI
Ethical AI Principles Require the Addressal of Multiple Elements/Dimensions
Growth Opportunity Segments
People
Process
Technology
The Way Forward
4. Growth Opportunity Universe
Growth Opportunity 1: Consulting and Advisory Services for AI Roadmaps and Ethical Considerations
For more information about this report visit https://www.researchandmarkets.com/r/2nklb1
CONTACT: 
  ResearchAndMarkets.com
 Laura Wood, Senior Press Manager
  press@researchandmarkets.com
  For E.S.T Office Hours Call 1-917-300-0470
  For U.S./CAN Toll Free Call 1-800-526-8630
  For GMT Office Hours Call +353-1-416-8900          
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: M2PW
Subject: ARTIFICIAL INTELLIGENCE ETHICS (94%); MARKET RESEARCH REPORTS (92%); ARTIFICIAL INTELLIGENCE (90%); BUSINESS REPORTS & FORECASTS (90%); ETHICS (90%); FINANCIAL MARKET UPDATES (90%); PRESS RELEASES (90%); REPORTS, REVIEWS & SECTIONS (90%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (78%); BEST PRACTICES (78%); TRENDS (78%); BUSINESS & PROFESSIONAL ASSOCIATIONS (77%); RESEARCH REPORTS (77%); TECHNOLOGY MATURITY (77%); BUSINESS METRICS (75%); ASSOCIATIONS & ORGANIZATIONS (72%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (94%); MARKET RESEARCH REPORTS (92%); ARTIFICIAL INTELLIGENCE (90%); FINANCIAL MARKET UPDATES (90%); MARKET RESEARCH (90%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (78%); INFORMATION TECHNOLOGY INDUSTRY (78%); MARKET RESEARCH & ANALYSIS (77%)
Load-Date: November 24, 2021",positive,0.5119508504867554,balanced/neutral,['bias'],[],"['regulation', 'policy', 'governance', 'must', 'need to']",[],1,0,5,0
2021,Unknown Title,"Body
Link to Story
TORONTO and LONDON, Aug. 20, 2021 (GLOBE NEWSWIRE) -- ACCA (the Association of Chartered Certified Accountants) and Chartered Accountants Australia and New Zealand (CA ANZ) reveal in a new report the pressing need for the accountancy profession to make the necessary connections between Artificial Intelligence (AI) and its relationship to environmental, social and governance (ESG) dimensions.
Polling over 5,700 respondents globally, including an expert panel of ACCA members across North America, the research reveals a cautious tone, with fewer than half (43%) believing that the impact of AI on their rights as an individual is positive - such as safety and personal security, levels of fairness, levels of choice, levels of transparency. In North America, 30% of respondents believe this to be the case.
Feedback from North American respondents also reveals:
A considerable 82% believe that their leaders prioritize ethics as highly as profits - compared with 66% globally.
70% believe that the impact of AI on overall standard of living in society is positive - compared to 64% globally.
However, just 13% of their organization has implemented an ethical framework for AI use.
And 44% of respondents have a basic understanding of what an algorithm is.
ACCA and CA ANZ say in Ethics for sustainable AI adoption: Connecting AI and ESG that accountants, with their explicit and long-standing commitment to ethical practices, are well placed to guide organizations along a responsible path for AI adoption.
Jillian Couse, Head of ACCA North America says: 'Our findings present a wake-up call for the accountancy profession to lead the way and become the super connectors needed to ensure an ethical approach. Their management of the transition to mass usage of AI in an ethical, responsible manner is essential if sustainable long-term value is to be secured from it.'
The report's nine recommendations include the need to set tone at the top on AI adoption by prioritizing an approach that is consistent with organizational values such as diversity and inclusion in considering the impact of AI on under-represented groups, or fairness when it comes to recruitment or surveillance of employees; and transparency such as appropriately disclosing AI use to customers.
Another recommendation for the profession is to challenge greenwashing and seek insights from AI tools to help with professional skepticism in examining whether the organization's claims about sustainability, such as on achieving net zero targets, are matched by its performance. Suspect claims need to be challenged.
Speaking of the global picture, Helen Brand, Chief Executive of ACCA says: 'AI adoption must consider the needs of all, especially the under-represented and vulnerable in society. That's why one of our recommendations is to ensure the profession exercises its professional judgement, because AI may create previously unseen situations. We recommend that accountants need to avoid over-reliance on simplistic checklist-based approaches which don't give the full picture or leave space for unintended consequences.'
Also commenting on the global findings, Ainslie van Onselen, Chief Executive of CA ANZ adds: 'Our report found that in order to ethically and sustainably adopt AI, organizations need effective governance mechanisms. This starts with setting the right tone and culture at the top and covers a range of areas from oversight and delivery procedures, to regulation and data governance. AI is a strategic endeavour that should be spearheaded by leaders who know and execute on the difference between what we have a right to do and what is the right thing to do. It's important to build knowledge and skills at the intersection of AI, ethics and sustainable development. This aligns well to the accountancy profession which can play a key role in driving responsible adoption.'
The global accountancy profession is bound by the Code of Ethics (the 'Code' ) and its five fundamental principles as set out by the International Ethics Standards Board for Accountants (IESBA). These are integrity, objectivity, professional competence and due care, confidentiality, and professional behaviour.
Ethics for sustainable AI adoption: Connecting AI and ESG can be downloaded here:
For media enquiries, contact:
Jeff Simmons,
About ACCA
ACCA (the Association of Chartered Certified Accountants) is the global body for finance professionals.
We're a thriving global community of 233,000 members and 536,000 future members based in 178 countries and regions, who work across a wide range of sectors and industries. We uphold the highest professional and ethical values.
We offer everyone everywhere the opportunity to experience a rewarding career in accountancy, finance and management. Our qualifications and learning opportunities develop strategic business leaders, forward-thinking professionals with the financial, business and digital expertise essential for the creation of sustainable organizations and flourishing societies.
Since 1904, being a force for public good has been embedded in our purpose . We believe that accountancy is a cornerstone profession of society and is vital helping economies, organizations and individuals to grow and prosper. It does this by creating robust trusted financial and business management, combating corruption, ensuring organizations are managed ethically, driving sustainability, and providing rewarding career opportunities.
And through our cutting-edge research, we lead the profession by answering today's questions and preparing for the future. We're a not-for-profit organization. Find out more at accaglobal.com
ACCA is not affiliated with any Chartered Accountant (CA) organization or Chartered Professional Accountant (CPA) organization.
About CA ANZ
Chartered Accountants Australia and New Zealand (CA ANZ) represents more than 128,000 financial professionals, supporting them to make a difference to the businesses, organizations and communities in which they work and live. Chartered Accountants are known as Difference Makers. The depth and breadth of their expertise helps them to see the big picture and chart the best course of action. Find out more:
Tags accounting finance ESG AI Related Links MENAFN20082021004107003653ID1102652681
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (91%); ARTIFICIAL INTELLIGENCE (90%); ASSOCIATIONS & ORGANIZATIONS (90%); ESG FACTORS (90%); ETHICS (90%); PROFESSIONAL WORKERS (90%); GREENWASHING (76%); SAFETY (75%); SURVEILLANCE (72%); EXECUTIVES (67%); LIVING STANDARDS (52%); DIVERSITY & INCLUSION (50%)
Company:  AUSTRALIA & NEW ZEALAND BANKING GROUP LTD (92%)
Organization: ASSOCIATION OF CHARTERED CERTIFIED ACCOUNTANTS (93%)
Ticker: ANZ (NZX) (92%); ANZ (ASX) (92%)
Industry: NAICS524210 INSURANCE AGENCIES & BROKERAGES (92%); NAICS523920 PORTFOLIO MANAGEMENT (92%); NAICS522110 COMMERCIAL BANKING (92%); SIC6411 INSURANCE AGENTS, BROKERS, & SERVICE (92%); SIC6282 INVESTMENT ADVICE (92%); ACCOUNTING (90%); ACCOUNTING & AUDITING FIRMS (90%); ARTIFICIAL INTELLIGENCE (90%); GREENWASHING (76%)
Geographic: TORONTO, ON, CANADA (58%); NORTH AMERICA (92%); AUSTRALIA & NEW ZEALAND (79%); NEW ZEALAND (79%); AUSTRALIA (78%)
Load-Date: September 1, 2021","Link to Story
TORONTO and LONDON, Aug. 20, 2021 (GLOBE NEWSWIRE) -- ACCA (the Association of Chartered Certified Accountants) and Chartered Accountants Australia and New Zealand (CA ANZ) reveal in a new report the pressing need for the accountancy profession to make the necessary connections between Artificial Intelligence (AI) and its relationship to environmental, social and governance (ESG) dimensions.
Polling over 5,700 respondents globally, including an expert panel of ACCA members across North America, the research reveals a cautious tone, with fewer than half (43%) believing that the impact of AI on their rights as an individual is positive - such as safety and personal security, levels of fairness, levels of choice, levels of transparency. In North America, 30% of respondents believe this to be the case.
Feedback from North American respondents also reveals:
A considerable 82% believe that their leaders prioritize ethics as highly as profits - compared with 66% globally.
70% believe that the impact of AI on overall standard of living in society is positive - compared to 64% globally.
However, just 13% of their organization has implemented an ethical framework for AI use.
And 44% of respondents have a basic understanding of what an algorithm is.
ACCA and CA ANZ say in Ethics for sustainable AI adoption: Connecting AI and ESG that accountants, with their explicit and long-standing commitment to ethical practices, are well placed to guide organizations along a responsible path for AI adoption.
Jillian Couse, Head of ACCA North America says: 'Our findings present a wake-up call for the accountancy profession to lead the way and become the super connectors needed to ensure an ethical approach. Their management of the transition to mass usage of AI in an ethical, responsible manner is essential if sustainable long-term value is to be secured from it.'
The report's nine recommendations include the need to set tone at the top on AI adoption by prioritizing an approach that is consistent with organizational values such as diversity and inclusion in considering the impact of AI on under-represented groups, or fairness when it comes to recruitment or surveillance of employees; and transparency such as appropriately disclosing AI use to customers.
Another recommendation for the profession is to challenge greenwashing and seek insights from AI tools to help with professional skepticism in examining whether the organization's claims about sustainability, such as on achieving net zero targets, are matched by its performance. Suspect claims need to be challenged.
Speaking of the global picture, Helen Brand, Chief Executive of ACCA says: 'AI adoption must consider the needs of all, especially the under-represented and vulnerable in society. That's why one of our recommendations is to ensure the profession exercises its professional judgement, because AI may create previously unseen situations. We recommend that accountants need to avoid over-reliance on simplistic checklist-based approaches which don't give the full picture or leave space for unintended consequences.'
Also commenting on the global findings, Ainslie van Onselen, Chief Executive of CA ANZ adds: 'Our report found that in order to ethically and sustainably adopt AI, organizations need effective governance mechanisms. This starts with setting the right tone and culture at the top and covers a range of areas from oversight and delivery procedures, to regulation and data governance. AI is a strategic endeavour that should be spearheaded by leaders who know and execute on the difference between what we have a right to do and what is the right thing to do. It's important to build knowledge and skills at the intersection of AI, ethics and sustainable development. This aligns well to the accountancy profession which can play a key role in driving responsible adoption.'
The global accountancy profession is bound by the Code of Ethics (the 'Code' ) and its five fundamental principles as set out by the International Ethics Standards Board for Accountants (IESBA). These are integrity, objectivity, professional competence and due care, confidentiality, and professional behaviour.
Ethics for sustainable AI adoption: Connecting AI and ESG can be downloaded here:
For media enquiries, contact:
Jeff Simmons,
About ACCA
ACCA (the Association of Chartered Certified Accountants) is the global body for finance professionals.
We're a thriving global community of 233,000 members and 536,000 future members based in 178 countries and regions, who work across a wide range of sectors and industries. We uphold the highest professional and ethical values.
We offer everyone everywhere the opportunity to experience a rewarding career in accountancy, finance and management. Our qualifications and learning opportunities develop strategic business leaders, forward-thinking professionals with the financial, business and digital expertise essential for the creation of sustainable organizations and flourishing societies.
Since 1904, being a force for public good has been embedded in our purpose . We believe that accountancy is a cornerstone profession of society and is vital helping economies, organizations and individuals to grow and prosper. It does this by creating robust trusted financial and business management, combating corruption, ensuring organizations are managed ethically, driving sustainability, and providing rewarding career opportunities.
And through our cutting-edge research, we lead the profession by answering today's questions and preparing for the future. We're a not-for-profit organization. Find out more at accaglobal.com
ACCA is not affiliated with any Chartered Accountant (CA) organization or Chartered Professional Accountant (CPA) organization.
About CA ANZ
Chartered Accountants Australia and New Zealand (CA ANZ) represents more than 128,000 financial professionals, supporting them to make a difference to the businesses, organizations and communities in which they work and live. Chartered Accountants are known as Difference Makers. The depth and breadth of their expertise helps them to see the big picture and chart the best course of action. Find out more:
Tags accounting finance ESG AI Related Links MENAFN20082021004107003653ID1102652681
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (91%); ARTIFICIAL INTELLIGENCE (90%); ASSOCIATIONS & ORGANIZATIONS (90%); ESG FACTORS (90%); ETHICS (90%); PROFESSIONAL WORKERS (90%); GREENWASHING (76%); SAFETY (75%); SURVEILLANCE (72%); EXECUTIVES (67%); LIVING STANDARDS (52%); DIVERSITY & INCLUSION (50%)
Company:  AUSTRALIA & NEW ZEALAND BANKING GROUP LTD (92%)
Organization: ASSOCIATION OF CHARTERED CERTIFIED ACCOUNTANTS (93%)
Ticker: ANZ (NZX) (92%); ANZ (ASX) (92%)
Industry: NAICS524210 INSURANCE AGENCIES & BROKERAGES (92%); NAICS523920 PORTFOLIO MANAGEMENT (92%); NAICS522110 COMMERCIAL BANKING (92%); SIC6411 INSURANCE AGENTS, BROKERS, & SERVICE (92%); SIC6282 INVESTMENT ADVICE (92%); ACCOUNTING (90%); ACCOUNTING & AUDITING FIRMS (90%); ARTIFICIAL INTELLIGENCE (90%); GREENWASHING (76%)
Geographic: TORONTO, ON, CANADA (58%); NORTH AMERICA (92%); AUSTRALIA & NEW ZEALAND (79%); NEW ZEALAND (79%); AUSTRALIA (78%)
Load-Date: September 1, 2021",neutral,0.909469485282898,balanced/neutral,"['surveillance', 'fairness', 'transparency', 'safety', 'security']",['fairness'],"['regulation', 'governance', 'oversight', 'standards', 'framework', 'should', 'must', 'need to', 'recommend']",['algorithm'],5,1,9,1
2021,Unknown Title,"Body
Moscow: Sberbank has issued the following press release:
First Deputy Chairman of the Sberbank Executive Board Alexander Vedyakhin spoke about where Sber stands on ethical AI regulation issues during a plenary session of the International Forum called AI Ethics: the Beginning of Trust.
Other discussion participants were Deputy Prime Minister of Russia Dmitry Chernyshenko, Chief of the Presidential Directorate for the Development of Information and Communication Technology and Communication Infrastructure Tatyana Matveeva, CEO of Yandex Russia Elena Bunina, MTS President Vyacheslav Nikolaev, film director Fyodor Bondarchuk, and InfoWatch Group President Natalya Kaspersky. The session was moderated by TV host Sergey Brilev.
Like any other technology, AI entails opportunities, but also risks. Therefore, Sber was one of the first in Russia to approve the principles of AI ethics complying with the best global practices and with the National AI Development Strategy. The principles provide Sber employees with guidance on how to deal with ethical dilemmas that may arise when interacting with AI. There are five of these principles: controllability and manageability of AI systems, transparency and predictability of their work, stability and reliability of systems, responsible use, and impartiality.
Alexander Vedyakhin noted that Sber had also been one of the first in the country to create a special permanent working group to address the ethical aspects of using AI in the work of all structural divisions of the bank and the Sber ecosystem.
AI ethical principles are important not only for one company, but for the entire country and the well-being of humanity. This requires international coordination. Alexander Vedyakhin emphasized that Sber was an active participant in a dialogue, providing its AI expertise for key international organizations like the Council of Europe, UNESCO, OSCE, OECD, BRICS, and ISO.
Alexander Vedyakhin, First Deputy Chairman of the Executive Board, Sberbank:
“We have been working on AI ethics for years. The principles for AI development and use that we adopted have enhanced the trust of Sber clients and Russians in artificial intelligence, which is used in most of our processes, products, and services. And we are happy that our principles are at the core of the AI ethics code that has been approved today. ”
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: IFN
Subject: ARTIFICIAL INTELLIGENCE ETHICS (94%); ETHICS (94%); BOARDS OF DIRECTORS (90%); BUSINESS NEWS (90%); ARTIFICIAL INTELLIGENCE (89%); APPROVALS (78%); ECONOMIC DEVELOPMENT (78%); EXECUTIVES (78%); PRESS RELEASES (78%); HEADS OF STATE & GOVERNMENT (77%); REGULATORY COMPLIANCE (77%); COMPANY STRATEGY (74%); INTERNATIONAL ECONOMIC ORGANIZATIONS (74%); ASSOCIATIONS & ORGANIZATIONS (73%); PRIME MINISTERS (57%); FILM DIRECTORS (55%)
Company:  SBERBANK ROSSII OAO (90%);  YANDEX NV (57%);  AI SYSTEMS (55%)
Ticker: SBER (RTS) (90%); SBER (LSE) (90%); YNDX (NASDAQ) (57%)
Industry: NAICS522293 INTERNATIONAL TRADE FINANCING (90%); NAICS522110 COMMERCIAL BANKING (90%); SIC6082 FOREIGN TRADE & INTERNATIONAL BANKING INSTITUTIONS (90%); SIC6021 NATIONAL COMMERCIAL BANKS (90%); NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (57%); SIC7372 PREPACKAGED SOFTWARE (55%); ARTIFICIAL INTELLIGENCE ETHICS (94%); BANKING & FINANCE (90%); ARTIFICIAL INTELLIGENCE (89%); COMPUTING & INFORMATION TECHNOLOGY (71%); FILM DIRECTORS (55%)
Geographic: MOSCOW, RUSSIAN FEDERATION (74%); RUSSIAN FEDERATION (90%); BRICS MEMBER STATES (79%); EUROPE (79%)
Load-Date: October 28, 2021","Moscow: Sberbank has issued the following press release:
First Deputy Chairman of the Sberbank Executive Board Alexander Vedyakhin spoke about where Sber stands on ethical AI regulation issues during a plenary session of the International Forum called AI Ethics: the Beginning of Trust.
Other discussion participants were Deputy Prime Minister of Russia Dmitry Chernyshenko, Chief of the Presidential Directorate for the Development of Information and Communication Technology and Communication Infrastructure Tatyana Matveeva, CEO of Yandex Russia Elena Bunina, MTS President Vyacheslav Nikolaev, film director Fyodor Bondarchuk, and InfoWatch Group President Natalya Kaspersky. The session was moderated by TV host Sergey Brilev.
Like any other technology, AI entails opportunities, but also risks. Therefore, Sber was one of the first in Russia to approve the principles of AI ethics complying with the best global practices and with the National AI Development Strategy. The principles provide Sber employees with guidance on how to deal with ethical dilemmas that may arise when interacting with AI. There are five of these principles: controllability and manageability of AI systems, transparency and predictability of their work, stability and reliability of systems, responsible use, and impartiality.
Alexander Vedyakhin noted that Sber had also been one of the first in the country to create a special permanent working group to address the ethical aspects of using AI in the work of all structural divisions of the bank and the Sber ecosystem.
AI ethical principles are important not only for one company, but for the entire country and the well-being of humanity. This requires international coordination. Alexander Vedyakhin emphasized that Sber was an active participant in a dialogue, providing its AI expertise for key international organizations like the Council of Europe, UNESCO, OSCE, OECD, BRICS, and ISO.
Alexander Vedyakhin, First Deputy Chairman of the Executive Board, Sberbank:
“We have been working on AI ethics for years. The principles for AI development and use that we adopted have enhanced the trust of Sber clients and Russians in artificial intelligence, which is used in most of our processes, products, and services. And we are happy that our principles are at the core of the AI ethics code that has been approved today. ”
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: IFN
Subject: ARTIFICIAL INTELLIGENCE ETHICS (94%); ETHICS (94%); BOARDS OF DIRECTORS (90%); BUSINESS NEWS (90%); ARTIFICIAL INTELLIGENCE (89%); APPROVALS (78%); ECONOMIC DEVELOPMENT (78%); EXECUTIVES (78%); PRESS RELEASES (78%); HEADS OF STATE & GOVERNMENT (77%); REGULATORY COMPLIANCE (77%); COMPANY STRATEGY (74%); INTERNATIONAL ECONOMIC ORGANIZATIONS (74%); ASSOCIATIONS & ORGANIZATIONS (73%); PRIME MINISTERS (57%); FILM DIRECTORS (55%)
Company:  SBERBANK ROSSII OAO (90%);  YANDEX NV (57%);  AI SYSTEMS (55%)
Ticker: SBER (RTS) (90%); SBER (LSE) (90%); YNDX (NASDAQ) (57%)
Industry: NAICS522293 INTERNATIONAL TRADE FINANCING (90%); NAICS522110 COMMERCIAL BANKING (90%); SIC6082 FOREIGN TRADE & INTERNATIONAL BANKING INSTITUTIONS (90%); SIC6021 NATIONAL COMMERCIAL BANKS (90%); NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (57%); SIC7372 PREPACKAGED SOFTWARE (55%); ARTIFICIAL INTELLIGENCE ETHICS (94%); BANKING & FINANCE (90%); ARTIFICIAL INTELLIGENCE (89%); COMPUTING & INFORMATION TECHNOLOGY (71%); FILM DIRECTORS (55%)
Geographic: MOSCOW, RUSSIAN FEDERATION (74%); RUSSIAN FEDERATION (90%); BRICS MEMBER STATES (79%); EUROPE (79%)
Load-Date: October 28, 2021",neutral,0.8953462243080139,balanced/neutral,['transparency'],[],"['regulation', 'compliance']",[],1,0,2,0
2021,Unknown Title,"Body
London: techUK has issued the following press release:
Sue Daley, techUK Director of Technology and Innovation, shares her reflections on the progress made since our first Digital Ethics Summit in 2017.
Back in 2017, the year of our first Digital Ethics Summit, it was becoming very clear that the diffusion of powerful new data driven technologies, such as AI, would rapidly change the way we work and live. We believed, and still do, that applied with responsibility and purpose these technologies could achieve great things for people and the planet. But applied casually and unthinkingly we could risk missing this opportunity. There was a broad consensus then that the best way forward was through the development and deep application of digital ethics to ensure that the process of innovation was reflective, and foresight was applied as we moved forward. But many questions remained; what does this mean in practice? Do we have the capability and capacity we need? What sort of institutions do we need to stay on the right course?
These questions and others were the reason techUK held its first ever Digital Ethics Summit in December 2017. The aim was to bring together business, academia, government and the third sector to push forward the discussion on how we could build a culture and reputation for digital ethics in the UK that could be world leading. What became really clear very quickly was there was significant interest in this important conversation. A conference room booked for seventy delegates had to be upgraded to a much bigger room as we had over three hundred people registering to attend. In hindsight a great problem to have but it didn ’ t help my blood pressure around what was already the first event of its kind techUK had ever held in collaboration with partner organisations including as the British Academy, the Royal Society, the Wellcome Trust, the Open Data Institute, the Royal Statistical Society and the Nuffield Foundation.
The first Summit turned out to be a landmark event. DCMS spoke about the launch of the Government ’ s new Centre for Data Ethics and Innovation. The Nuffield Foundation announced the creation of the Ada Lovelace Institute and industry spoke about how an ethics by design approach was being taken forward. Panels discussed how to ensure ethical foresight as the use of AI continues to increase, how everyone can embed an ethics by design approach and how we position the UK for ethical leadership. It became clear there were more issues and questions that needed to be unpacked and that this was a conversation that needed more voices, input and discussion. So what began in 2017 has continued to this day with our fifth annual Summit being held this Wednesday.
As part of marking the fifth techUK ’ s Digital Ethics Summit we wanted to take a moment to reflect on what has happened in the years since the first Summit and explore how the digital ethics conversation has developed and matured. We are therefore releasing insights this week by leaders in the UK ’ s digital ethics community and by our members, sharing examples of digital ethics in practice ,thoughts and reflections on where they think we are today, the progress that has been made and what the future of digital ethics in the UK may hold. Many of these pieces are written by people who have been involved in the digital ethics conversation in the UK since the very first Summit and we hope that their own insights, views and perspectives can help to, once again, push forward the discussion on what we need to do over the next five years to continue to be a world leader in digital ethics and show the way by operationalising ethics and demonstrating our ability to innovate for good.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: IFN
Subject: BUSINESS NEWS (90%); ETHICS (90%); TALKS & MEETINGS (78%); TECHNOLOGY MATURITY (78%); ASSOCIATIONS & ORGANIZATIONS (73%); PRESS RELEASES (73%); TRENDS & EVENTS (64%)
Industry: OPEN DATA (78%)
Geographic: LONDON, ENGLAND (79%); UNITED KINGDOM (90%)
Load-Date: December 8, 2021","London: techUK has issued the following press release:
Sue Daley, techUK Director of Technology and Innovation, shares her reflections on the progress made since our first Digital Ethics Summit in 2017.
Back in 2017, the year of our first Digital Ethics Summit, it was becoming very clear that the diffusion of powerful new data driven technologies, such as AI, would rapidly change the way we work and live. We believed, and still do, that applied with responsibility and purpose these technologies could achieve great things for people and the planet. But applied casually and unthinkingly we could risk missing this opportunity. There was a broad consensus then that the best way forward was through the development and deep application of digital ethics to ensure that the process of innovation was reflective, and foresight was applied as we moved forward. But many questions remained; what does this mean in practice? Do we have the capability and capacity we need? What sort of institutions do we need to stay on the right course?
These questions and others were the reason techUK held its first ever Digital Ethics Summit in December 2017. The aim was to bring together business, academia, government and the third sector to push forward the discussion on how we could build a culture and reputation for digital ethics in the UK that could be world leading. What became really clear very quickly was there was significant interest in this important conversation. A conference room booked for seventy delegates had to be upgraded to a much bigger room as we had over three hundred people registering to attend. In hindsight a great problem to have but it didn ’ t help my blood pressure around what was already the first event of its kind techUK had ever held in collaboration with partner organisations including as the British Academy, the Royal Society, the Wellcome Trust, the Open Data Institute, the Royal Statistical Society and the Nuffield Foundation.
The first Summit turned out to be a landmark event. DCMS spoke about the launch of the Government ’ s new Centre for Data Ethics and Innovation. The Nuffield Foundation announced the creation of the Ada Lovelace Institute and industry spoke about how an ethics by design approach was being taken forward. Panels discussed how to ensure ethical foresight as the use of AI continues to increase, how everyone can embed an ethics by design approach and how we position the UK for ethical leadership. It became clear there were more issues and questions that needed to be unpacked and that this was a conversation that needed more voices, input and discussion. So what began in 2017 has continued to this day with our fifth annual Summit being held this Wednesday.
As part of marking the fifth techUK ’ s Digital Ethics Summit we wanted to take a moment to reflect on what has happened in the years since the first Summit and explore how the digital ethics conversation has developed and matured. We are therefore releasing insights this week by leaders in the UK ’ s digital ethics community and by our members, sharing examples of digital ethics in practice ,thoughts and reflections on where they think we are today, the progress that has been made and what the future of digital ethics in the UK may hold. Many of these pieces are written by people who have been involved in the digital ethics conversation in the UK since the very first Summit and we hope that their own insights, views and perspectives can help to, once again, push forward the discussion on what we need to do over the next five years to continue to be a world leader in digital ethics and show the way by operationalising ethics and demonstrating our ability to innovate for good.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: IFN
Subject: BUSINESS NEWS (90%); ETHICS (90%); TALKS & MEETINGS (78%); TECHNOLOGY MATURITY (78%); ASSOCIATIONS & ORGANIZATIONS (73%); PRESS RELEASES (73%); TRENDS & EVENTS (64%)
Industry: OPEN DATA (78%)
Geographic: LONDON, ENGLAND (79%); UNITED KINGDOM (90%)
Load-Date: December 8, 2021",neutral,0.550482988357544,balanced/neutral,[],[],['need to'],[],0,0,1,0
2021,Unknown Title,"Body
ATLANTA, March 23 -- OneTrust issued the following news release:
- Acquisition builds on OneTrust's category-defining software platform of trust
OneTrust, the #1 fastest-growing company on the Inc. 500 and category-defining enterprise platform to operationalize trust, today announced it signed a definitive agreement to acquire Convercent, a leading global provider of ethics and compliance cloud software. The acquisition will build on OneTrust's longstanding investments in creating the technology fabric of trust within an organization, bringing together privacy, security, data governance, ethics and compliance, GRC, third-party risk, and ESG into a single operational workflow.
Convercent's technology, 750 customers, 150 employees, and global CONVERGE user community, will see an acceleration in investment while integrating into OneTrust's platform, global customer base of 8,000 organizations, and more than 1,500 employees across 12 offices.
* Learn more: Defining a New Software Category: Trust
* Register for the webinar: OneTrust to Acquire Convercent: Building the Technology Platform of Trust
Defining the New Software Category of Trust
Trust and transparency are driving the next generation of business innovation and market differentiation. Rising amounts of personal data, advancements in AI and personalization, and customer demand for transparency have created increasing pressure for organizations to operate and differentiate based on trust. To successfully make trust a competitive differentiator, organizations must manage the complex myriad of privacy regulations, security requirements, ethics and compliance frameworks, and sustainability initiatives.
By helping organizations build centralized, agile workflows across privacy, security, data governance, ethics and compliance, GRC, third-party risk, and ESG, OneTrust is pioneering this new software market of trust and building the trust fabric of an organization.
Ethics & Compliance at the Foundation of Trust
Integrating Convercent into OneTrust further enhances the platform's foundation of trust through advanced ethics and compliance capabilities. The Convercent Ethics Cloud Platform, including whistleblowing, policy management, disclosure management, analytics and benchmarking, and learning, are used by more than 750 enterprise customers. With a team of 150 ethics and compliance experts, Convercent brings unparalleled experience in managing the world's most complex and advanced global ethics programs, including Airbnb, Under Armour, Kimberly-Clark, and TimeWarner.
""Kimberly-Clark is continuously focused on improving our integrity culture, because building trust with our employees, consumers, and stakeholders provides a competitive advantage,"" said Kurt Drake, Chief Ethics & Compliance Officer, Kimberly-Clark. ""A key component of that focus is having centralized programs and platforms that provide metrics and insights in delivering better results and value for our customers and stakeholders. As customers of both Convercent and OneTrust, there is strategic value in bringing the two platforms together, allowing integration of metrics and reporting, which drives further insights and value for Kimberly-Clark.""
""Convercent brings innovative ethics and compliance customer programs into the OneTrust platform,"" said Kabir Barday, OneTrust CEO and Fellow of Information Privacy (FIP). ""The technology fits seamlessly into the OneTrust platform, and our vision to build trust as the technology fabric within an organization. We're excited to welcome Convercent's team of experts, customers, and CONVERGE community into the OneTrust family and work together to make trust a competitive advantage.""
""At Convercent, we passionately believe that driving ethics to the center of business can build a better world,"" said Patrick Quinlan, CEO, Convercent. ""Becoming a part of OneTrust is a natural fit. Just as ethics programs must exist within all functions of a business, ethics and compliance programs too are more effective as a part of a broader trust framework. We're delivering on this greater vision with OneTrust. We're grateful to the Convercent community of employees, customers, partners, and families, who have worked alongside us, and now as a part of OneTrust we can deliver for our community faster and bolder than ever before.""
Financial details of the acquisition were not disclosed, and the transaction is expected to close in April.
To learn more about how Convercent will integrate into OneTrust, register for our webinar. To learn more about the new software market category of trust, visit OneTrust.com.
About OneTrust
OneTrust is the #1 fastest-growing company on Inc. 500 and the category-defining enterprise platform to operationalize trust. More than 8,000 customers, including half of the Fortune 500, use OneTrust to make trust a competitive differentiator, implementing central agile workflows across privacy, security, data governance, GRC, third-party risk, ethics and compliance, and ESG programs.
The OneTrust platform is backed by 140 patents and powered by the OneTrust Athena(TM) AI and robotic automation engine, and capabilities include:
* OneTrust Privacy - Privacy Management Software
* OneTrust DataDiscovery(TM) - AI-Powered Discovery and Classification
* OneTrust DataGovernance(TM) -Data Intelligence Software
* OneTrust Vendorpedia(TM) - Third-Party Risk Exchange
* OneTrust GRC - Integrated Risk Management Software
* OneTrust Ethics - Ethics and Compliance Software
* OneTrust PreferenceChoice(TM) - Consent and Preference Management Software
* OneTrust ESG - Environmental, Social & Governance Software
In December 2020, OneTrust raised a $300 million Series C funding from TCV, Insight Partners, and Coatue, bringing total funds raised to $710 million at a $5.1 billion valuation. OneTrust's fast-growing team of 1,500 employees is co-headquartered in Atlanta and London with additional offices in Bangalore, Melbourne, Seattle, San Francisco, New York, So Paulo, Munich, Paris, Hong Kong, and Bangkok.
To learn more, visit OneTrust.com or connect on LinkedIn, Twitter, and YouTube.
About Convercent
As a leading global provider of ethics and compliance software, Convercent empowers the world's largest and most admired companies to understand organizational risk, protect their brand, and engage employees with their ethics and compliance program. Its Ethics Cloud Platform, which includes the Ethics and Compliance Portal; Helpline and Case Manager; Policy and Learning; and Disclosures and Conflict of Interest, leverages a global dataset to deliver business leaders the insights required to make proactive, informed decisions about their company's ethical health through real-time dashboards and analytics. Convercent has over 750 global customers including Microsoft, Four Seasons Hotels and Resorts, Capgemini and Under Armour, representing almost seven million employees in 150 countries. In 2021, Convercent was awarded ""Company of the Year"" from the Colorado Technology Association (CTA) at the Apex Awards. Convercent is based in Denver, CO, with an international office in London and is backed by Sapphire Ventures, Tola Capital, and Azure Capital Partners.
Source: OneTrust
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ASSOCIATIONS & ORGANIZATIONS (90%); BUSINESS ETHICS (90%); COMPANY LISTS & RANKINGS (90%); ETHICS (90%); ESG FACTORS - GOVERNANCE (89%); AGREEMENTS (78%); DATA ANALYTICS (78%); DATA PROTECTION LAWS (78%); REGULATORY COMPLIANCE (78%); WHISTLEBLOWERS (78%); BENCHMARKING (77%); BUSINESS ANALYTICS (77%); OUTPUT & DEMAND (77%); INTERNET PRIVACY (75%); CONSUMERS (74%); ACQUISITIONS (73%); SUSTAINABILITY (50%)
Company:  UNDER ARMOUR INC (52%);  KIMBERLY-CLARK CORP (52%)
Ticker: UA (NYSE) (52%); KMB (NYSE) (52%)
Industry: NAICS315240 WOMEN'S, GIRLS' & INFANTS' CUT & SEW APPAREL MANUFACTURING (52%); NAICS315220 MEN'S & BOYS' CUT & SEW APPAREL MANUFACTURING (52%); NAICS339113 SURGICAL APPLIANCE & SUPPLIES MANUFACTURING (52%); NAICS322291 SANITARY PAPER PRODUCT MANUFACTURING (52%); SIC3842 ORTHOPEDIC, PROSTHETIC, & SURGICAL APPLIANCES & SUPPLIES (52%); SIC2844 PERFUMES, COSMETICS, & OTHER TOILET PREPARATIONS (52%); SIC2676 SANITARY PAPER PRODUCTS (52%); COMPUTER SOFTWARE (90%); SOFTWARE SERVICES & APPLICATIONS (90%); CLOUD COMPUTING (89%); DATA GOVERNANCE & STEWARDSHIP (89%); DATA ANALYTICS (78%); DATA PROTECTION LAWS (78%); ON DEMAND SERVICES (78%); BUSINESS ANALYTICS (77%); INFORMATION SECURITY & PRIVACY (75%); INTERNET PRIVACY (75%)
Load-Date: March 23, 2021","ATLANTA, March 23 -- OneTrust issued the following news release:
- Acquisition builds on OneTrust's category-defining software platform of trust
OneTrust, the #1 fastest-growing company on the Inc. 500 and category-defining enterprise platform to operationalize trust, today announced it signed a definitive agreement to acquire Convercent, a leading global provider of ethics and compliance cloud software. The acquisition will build on OneTrust's longstanding investments in creating the technology fabric of trust within an organization, bringing together privacy, security, data governance, ethics and compliance, GRC, third-party risk, and ESG into a single operational workflow.
Convercent's technology, 750 customers, 150 employees, and global CONVERGE user community, will see an acceleration in investment while integrating into OneTrust's platform, global customer base of 8,000 organizations, and more than 1,500 employees across 12 offices.
* Learn more: Defining a New Software Category: Trust
* Register for the webinar: OneTrust to Acquire Convercent: Building the Technology Platform of Trust
Defining the New Software Category of Trust
Trust and transparency are driving the next generation of business innovation and market differentiation. Rising amounts of personal data, advancements in AI and personalization, and customer demand for transparency have created increasing pressure for organizations to operate and differentiate based on trust. To successfully make trust a competitive differentiator, organizations must manage the complex myriad of privacy regulations, security requirements, ethics and compliance frameworks, and sustainability initiatives.
By helping organizations build centralized, agile workflows across privacy, security, data governance, ethics and compliance, GRC, third-party risk, and ESG, OneTrust is pioneering this new software market of trust and building the trust fabric of an organization.
Ethics & Compliance at the Foundation of Trust
Integrating Convercent into OneTrust further enhances the platform's foundation of trust through advanced ethics and compliance capabilities. The Convercent Ethics Cloud Platform, including whistleblowing, policy management, disclosure management, analytics and benchmarking, and learning, are used by more than 750 enterprise customers. With a team of 150 ethics and compliance experts, Convercent brings unparalleled experience in managing the world's most complex and advanced global ethics programs, including Airbnb, Under Armour, Kimberly-Clark, and TimeWarner.
""Kimberly-Clark is continuously focused on improving our integrity culture, because building trust with our employees, consumers, and stakeholders provides a competitive advantage,"" said Kurt Drake, Chief Ethics & Compliance Officer, Kimberly-Clark. ""A key component of that focus is having centralized programs and platforms that provide metrics and insights in delivering better results and value for our customers and stakeholders. As customers of both Convercent and OneTrust, there is strategic value in bringing the two platforms together, allowing integration of metrics and reporting, which drives further insights and value for Kimberly-Clark.""
""Convercent brings innovative ethics and compliance customer programs into the OneTrust platform,"" said Kabir Barday, OneTrust CEO and Fellow of Information Privacy (FIP). ""The technology fits seamlessly into the OneTrust platform, and our vision to build trust as the technology fabric within an organization. We're excited to welcome Convercent's team of experts, customers, and CONVERGE community into the OneTrust family and work together to make trust a competitive advantage.""
""At Convercent, we passionately believe that driving ethics to the center of business can build a better world,"" said Patrick Quinlan, CEO, Convercent. ""Becoming a part of OneTrust is a natural fit. Just as ethics programs must exist within all functions of a business, ethics and compliance programs too are more effective as a part of a broader trust framework. We're delivering on this greater vision with OneTrust. We're grateful to the Convercent community of employees, customers, partners, and families, who have worked alongside us, and now as a part of OneTrust we can deliver for our community faster and bolder than ever before.""
Financial details of the acquisition were not disclosed, and the transaction is expected to close in April.
To learn more about how Convercent will integrate into OneTrust, register for our webinar. To learn more about the new software market category of trust, visit OneTrust.com.
About OneTrust
OneTrust is the #1 fastest-growing company on Inc. 500 and the category-defining enterprise platform to operationalize trust. More than 8,000 customers, including half of the Fortune 500, use OneTrust to make trust a competitive differentiator, implementing central agile workflows across privacy, security, data governance, GRC, third-party risk, ethics and compliance, and ESG programs.
The OneTrust platform is backed by 140 patents and powered by the OneTrust Athena(TM) AI and robotic automation engine, and capabilities include:
* OneTrust Privacy - Privacy Management Software
* OneTrust DataDiscovery(TM) - AI-Powered Discovery and Classification
* OneTrust DataGovernance(TM) -Data Intelligence Software
* OneTrust Vendorpedia(TM) - Third-Party Risk Exchange
* OneTrust GRC - Integrated Risk Management Software
* OneTrust Ethics - Ethics and Compliance Software
* OneTrust PreferenceChoice(TM) - Consent and Preference Management Software
* OneTrust ESG - Environmental, Social & Governance Software
In December 2020, OneTrust raised a $300 million Series C funding from TCV, Insight Partners, and Coatue, bringing total funds raised to $710 million at a $5.1 billion valuation. OneTrust's fast-growing team of 1,500 employees is co-headquartered in Atlanta and London with additional offices in Bangalore, Melbourne, Seattle, San Francisco, New York, So Paulo, Munich, Paris, Hong Kong, and Bangkok.
To learn more, visit OneTrust.com or connect on LinkedIn, Twitter, and YouTube.
About Convercent
As a leading global provider of ethics and compliance software, Convercent empowers the world's largest and most admired companies to understand organizational risk, protect their brand, and engage employees with their ethics and compliance program. Its Ethics Cloud Platform, which includes the Ethics and Compliance Portal; Helpline and Case Manager; Policy and Learning; and Disclosures and Conflict of Interest, leverages a global dataset to deliver business leaders the insights required to make proactive, informed decisions about their company's ethical health through real-time dashboards and analytics. Convercent has over 750 global customers including Microsoft, Four Seasons Hotels and Resorts, Capgemini and Under Armour, representing almost seven million employees in 150 countries. In 2021, Convercent was awarded ""Company of the Year"" from the Colorado Technology Association (CTA) at the Apex Awards. Convercent is based in Denver, CO, with an international office in London and is backed by Sapphire Ventures, Tola Capital, and Azure Capital Partners.
Source: OneTrust
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ASSOCIATIONS & ORGANIZATIONS (90%); BUSINESS ETHICS (90%); COMPANY LISTS & RANKINGS (90%); ETHICS (90%); ESG FACTORS - GOVERNANCE (89%); AGREEMENTS (78%); DATA ANALYTICS (78%); DATA PROTECTION LAWS (78%); REGULATORY COMPLIANCE (78%); WHISTLEBLOWERS (78%); BENCHMARKING (77%); BUSINESS ANALYTICS (77%); OUTPUT & DEMAND (77%); INTERNET PRIVACY (75%); CONSUMERS (74%); ACQUISITIONS (73%); SUSTAINABILITY (50%)
Company:  UNDER ARMOUR INC (52%);  KIMBERLY-CLARK CORP (52%)
Ticker: UA (NYSE) (52%); KMB (NYSE) (52%)
Industry: NAICS315240 WOMEN'S, GIRLS' & INFANTS' CUT & SEW APPAREL MANUFACTURING (52%); NAICS315220 MEN'S & BOYS' CUT & SEW APPAREL MANUFACTURING (52%); NAICS339113 SURGICAL APPLIANCE & SUPPLIES MANUFACTURING (52%); NAICS322291 SANITARY PAPER PRODUCT MANUFACTURING (52%); SIC3842 ORTHOPEDIC, PROSTHETIC, & SURGICAL APPLIANCES & SUPPLIES (52%); SIC2844 PERFUMES, COSMETICS, & OTHER TOILET PREPARATIONS (52%); SIC2676 SANITARY PAPER PRODUCTS (52%); COMPUTER SOFTWARE (90%); SOFTWARE SERVICES & APPLICATIONS (90%); CLOUD COMPUTING (89%); DATA GOVERNANCE & STEWARDSHIP (89%); DATA ANALYTICS (78%); DATA PROTECTION LAWS (78%); ON DEMAND SERVICES (78%); BUSINESS ANALYTICS (77%); INFORMATION SECURITY & PRIVACY (75%); INTERNET PRIVACY (75%)
Load-Date: March 23, 2021",neutral,0.5331156253814697,balanced/neutral,"['privacy', 'transparency', 'security', 'consent']",[],"['policy', 'governance', 'framework', 'compliance', 'must']",[],4,0,5,0
2021,Unknown Title,"Dateline: New Delhi, 2021-11-30 10:13:23 
Body
 November 30 -- Artificial intelligence (AI) was once the stuff of science fiction. But it's becoming widespread. It is used in mobile phone technology and motor vehicles. It powers tools for agriculture and healthcare.
But concerns have emerged about the accountability of AI and related technologies like machine learning. In December 2020 a computer scientist, Timnit Gebru, was fired from Google's Ethical AI team. She had previously raised the alarm about the social effects of bias in AI technologies. For instance, in a 2018 paper Gebru and another researcher, Joy Buolamwini, had shown how facial recognition software was less accurate in identifying women and people of color than white men. Biases in training data can have far-reaching and unintended effects.
There is already a substantial body of research about ethics in AI. This highlights the importance of principles to ensure technologies do not simply worsen biases or even introduce new social harms. As the UNESCO draft recommendation on the ethics of AI states:
We need international and national policies and regulatory frameworks to ensure that these emerging technologies benefit humanity as a whole.
Mark your calendar for June 16 & 17!Tickets to TNW 2022 are available now!
In recent years, many frameworks and guidelines have been created that identify objectives and priorities for ethical AI.
This is certainly a step in the right direction. But it's also critical to look beyond technical solutions when addressing issues of bias or inclusivity. Biases can enter at the level of who frames the objectives and balances the priorities.
In a recent paper, we argue that inclusivity and diversity also need to be at the level of identifying values and defining frameworks of what counts as ethical AI in the first place. This is especially pertinent when considering the growth of AI research and machine learning across the African continent.
ContextResearch and development of AI and machine learning technologies are growing in African countries. Programs such as Data Science Africa, Data Science Nigeria, and the Deep Learning Indaba with its satellite IndabaX events, which have so far been held in 27 different African countries, illustrate the interest and human investment in the fields.
The potential of AI and related technologies to promote opportunities for growth, development, and democratization in Africa is a key driver of this research.
Yet very few African voices have so far been involved in the international ethical frameworks that aim to guide the research. This might not be a problem if the principles and values in those frameworks have universal application. But it's not clear that they do.
For instance, the European AI4People framework offers a synthesis of six other ethical frameworks. It identifies respect for autonomy as one of its key principles. This principle has been criticized within the applied ethical field of bioethics. It is seen as failing to do justice to the communitarian values common across Africa. These focus less on the individual and more on community, even requiring that exceptions are made to uphold such a principle to allow for effective interventions.
Challenges like these - or even acknowledgment that there could be such challenges - are largely absent from the discussions and frameworks for ethical AI.
Just like training data can entrench existing inequalities and injustices, so can failing to recognize the possibility of diverse sets of values that can vary across social, cultural, and political contexts.
Unusable resultsIn addition, failing to take into account social, cultural, and political contexts can mean that even a seemingly perfect ethical technical solution can be ineffective or misguided once implemented.
For machine learning to be effective at making useful predictions, any learning system needs access to training data. This involves samples of the data of interest: inputs in the form of multiple features or measurements, and outputs which are the labels scientists want to predict. In most cases, both these features and labels require human knowledge of the problem. But a failure to correctly account for the local context could result in underperforming systems.
For example, mobile phone call records have been used to estimate population sizes before and after disasters. However, vulnerable populations are less likely to have access to mobile devices. So, this kind of approach could yield results that aren't useful.
Similarly, computer vision technologies for identifying different kinds of structures in an area will likely underperform where different construction materials are used. In both of these cases, as we and other colleagues discuss in another recent paper, not accounting for regional differences may have profound effects on anything from the delivery of disaster aid, to the performance of autonomous systems.
Going forwardAI technologies must not simply worsen or incorporate the problematic aspects of current human societies.
Being sensitive to and inclusive of different contexts is vital for designing effective technical solutions. It is equally important not to assume that values are universal. Those developing AI need to start including people of different backgrounds: not just in the technical aspects of designing data sets and the like but also in defining the values that can be called upon to frame and set objectives and priorities.
Classification
Language: ENGLISH
Publication-Type: Magazine
Subject: ARTIFICIAL INTELLIGENCE ETHICS (92%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); DATA SCIENCE (89%); DEEP LEARNING (89%); MACHINE LEARNING (89%); COMPUTER SCIENCE (79%); EMERGING TECHNOLOGY (79%); IDENTIFICATION TECHNOLOGIES (78%); PUBLIC POLICY (77%); RACE & ETHNICITY (77%); DIVERSITY & INCLUSION (75%); BIOETHICS (73%); BIOMETRICS (70%)
Company: GOOGLE LLC (84%)
Industry: NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (84%); ARTIFICIAL INTELLIGENCE ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); DATA SCIENCE (89%); DEEP LEARNING (89%); MACHINE LEARNING (89%); COMPUTER SCIENCE (79%); COMPUTER SOFTWARE (76%); MOBILE & CELLULAR TELEPHONES (73%); PATTERN RECOGNITION (73%)
Geographic: NEW DELHI, INDIA (59%); AFRICA (94%); NIGERIA (78%)
Load-Date: November 30, 2021","November 30 -- Artificial intelligence (AI) was once the stuff of science fiction. But it's becoming widespread. It is used in mobile phone technology and motor vehicles. It powers tools for agriculture and healthcare.
But concerns have emerged about the accountability of AI and related technologies like machine learning. In December 2020 a computer scientist, Timnit Gebru, was fired from Google's Ethical AI team. She had previously raised the alarm about the social effects of bias in AI technologies. For instance, in a 2018 paper Gebru and another researcher, Joy Buolamwini, had shown how facial recognition software was less accurate in identifying women and people of color than white men. Biases in training data can have far-reaching and unintended effects.
There is already a substantial body of research about ethics in AI. This highlights the importance of principles to ensure technologies do not simply worsen biases or even introduce new social harms. As the UNESCO draft recommendation on the ethics of AI states:
We need international and national policies and regulatory frameworks to ensure that these emerging technologies benefit humanity as a whole.
Mark your calendar for June 16 & 17!Tickets to TNW 2022 are available now!
In recent years, many frameworks and guidelines have been created that identify objectives and priorities for ethical AI.
This is certainly a step in the right direction. But it's also critical to look beyond technical solutions when addressing issues of bias or inclusivity. Biases can enter at the level of who frames the objectives and balances the priorities.
In a recent paper, we argue that inclusivity and diversity also need to be at the level of identifying values and defining frameworks of what counts as ethical AI in the first place. This is especially pertinent when considering the growth of AI research and machine learning across the African continent.
ContextResearch and development of AI and machine learning technologies are growing in African countries. Programs such as Data Science Africa, Data Science Nigeria, and the Deep Learning Indaba with its satellite IndabaX events, which have so far been held in 27 different African countries, illustrate the interest and human investment in the fields.
The potential of AI and related technologies to promote opportunities for growth, development, and democratization in Africa is a key driver of this research.
Yet very few African voices have so far been involved in the international ethical frameworks that aim to guide the research. This might not be a problem if the principles and values in those frameworks have universal application. But it's not clear that they do.
For instance, the European AI4People framework offers a synthesis of six other ethical frameworks. It identifies respect for autonomy as one of its key principles. This principle has been criticized within the applied ethical field of bioethics. It is seen as failing to do justice to the communitarian values common across Africa. These focus less on the individual and more on community, even requiring that exceptions are made to uphold such a principle to allow for effective interventions.
Challenges like these - or even acknowledgment that there could be such challenges - are largely absent from the discussions and frameworks for ethical AI.
Just like training data can entrench existing inequalities and injustices, so can failing to recognize the possibility of diverse sets of values that can vary across social, cultural, and political contexts.
Unusable resultsIn addition, failing to take into account social, cultural, and political contexts can mean that even a seemingly perfect ethical technical solution can be ineffective or misguided once implemented.
For machine learning to be effective at making useful predictions, any learning system needs access to training data. This involves samples of the data of interest: inputs in the form of multiple features or measurements, and outputs which are the labels scientists want to predict. In most cases, both these features and labels require human knowledge of the problem. But a failure to correctly account for the local context could result in underperforming systems.
For example, mobile phone call records have been used to estimate population sizes before and after disasters. However, vulnerable populations are less likely to have access to mobile devices. So, this kind of approach could yield results that aren't useful.
Similarly, computer vision technologies for identifying different kinds of structures in an area will likely underperform where different construction materials are used. In both of these cases, as we and other colleagues discuss in another recent paper, not accounting for regional differences may have profound effects on anything from the delivery of disaster aid, to the performance of autonomous systems.
Going forwardAI technologies must not simply worsen or incorporate the problematic aspects of current human societies.
Being sensitive to and inclusive of different contexts is vital for designing effective technical solutions. It is equally important not to assume that values are universal. Those developing AI need to start including people of different backgrounds: not just in the technical aspects of designing data sets and the like but also in defining the values that can be called upon to frame and set objectives and priorities.
Classification
Language: ENGLISH
Publication-Type: Magazine
Subject: ARTIFICIAL INTELLIGENCE ETHICS (92%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); DATA SCIENCE (89%); DEEP LEARNING (89%); MACHINE LEARNING (89%); COMPUTER SCIENCE (79%); EMERGING TECHNOLOGY (79%); IDENTIFICATION TECHNOLOGIES (78%); PUBLIC POLICY (77%); RACE & ETHNICITY (77%); DIVERSITY & INCLUSION (75%); BIOETHICS (73%); BIOMETRICS (70%)
Company: GOOGLE LLC (84%)
Industry: NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (84%); ARTIFICIAL INTELLIGENCE ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); DATA SCIENCE (89%); DEEP LEARNING (89%); MACHINE LEARNING (89%); COMPUTER SCIENCE (79%); COMPUTER SOFTWARE (76%); MOBILE & CELLULAR TELEPHONES (73%); PATTERN RECOGNITION (73%)
Geographic: NEW DELHI, INDIA (59%); AFRICA (94%); NIGERIA (78%)
Load-Date: November 30, 2021",neutral,0.7677332162857056,balanced/neutral,"['bias', 'accountability', 'autonomy', 'inclusivity', 'access']","['justice', 'autonomy', 'justice']","['policy', 'guidelines', 'framework', 'must', 'need to']","['machine learning', 'deep learning', 'computer vision', 'facial recognition']",5,3,5,4
2021,Unknown Title,"Byline: Nina Hendy
Highlight: Investors are prodding companies right in the soft underbelly to ensure ethics play a part in our collective wealth creation process.
Body
Recovering from traumatic events is a lesson in how we can avoid the economic consequences the next time.
Take the GFC, for example. Australia's devastating bushfires. Growing evidence of serious climate change. And now the global pandemic.
Each traumatic event has created a clear shift in our individual and collective conscious thinking. Fund managers also say these and other traumatic events have helped to funnel more money into ethical investments.
But this latest traumatic event has had one of the biggest impacts of all, as we all look for places to stash our wealth that's safe for the long-term after a period of economic turmoil.
Ethical investments have come out trumps as the pandemic turbo-charges our appetite for ESG investments, prompting fund managers to prod companies right in the soft underbelly to ensure ethics play a part in our collective wealth creation process.
It's paying dividends, too. A recent white paper by Fidelity International analysed the market crash during the height of the pandemic, testing the effect of this volatility on companies with different environmental, social and governance characteristics.
It reveals a strong positive correlation between a company's relative market performance and its ESG rating, which held firm across a nine-month timeframe in the face of the pandemic during 2020.
The analysis concluded that companies with high sustainability ratings performed better than their peers when markets fall. This finding bore out Fidelity's initial hypothesis that companies with good sustainability characteristics have more prudent management and will demonstrate greater resilience in a crisis.
Ethics have been rising in importance for many years, but the pandemic turbo-charged the matter, Fidelity's global head of stewardship and sustainable investing Jenn-Hui Tan reveals.
Based in Singapore, Tan says the research suggests that the market does, in fact, discriminate between companies based on their attention to sustainability matters, both in crashes and recoveries, demonstrating why sustainability is at the heart of active portfolio management.
The ESG movement is about firstly acknowledging these non-financial outcomes, and secondly, trying to measure them, he says.
""The purpose of finance is ultimately not just about allocating capital in a way that maximises financial returns, but actually maximises capital in a way that creates strong, positive societal outcomes and minimises the environmental harms like climate change,"" Tan says.
""While on the surface, ESG appears to be a global concern given it relates to climate change and diversity, but at the corporate level, it's extremely local. In Australia, issues such as the Royal Commission and bushfires play out in a local context, directly impacting where investments are channelled. The fact is that you don't make progress by thinking globally about these issues.
""By allocating capital in favourable sustainable businesses, you can actually generate better financial performance for your clients. And you can actually live up to the purpose of the financial industry,"" Tan says.
Australian Ethical started forging this space 35 years ago. Five years ago, the fund had $300 million of under management, which has ballooned to $5.05 billion today. The growth has been underpinned by $0.42 billion in new inflows and $0.57 billion in market movements. Meanwhile, new members grew 22 per cent over the past 12 months.
Growing demand has prompted the fund to set an ambitious target of $25 billion in assets under management, strengthening its teams in preparation to hit the goal. But whether or not it will take a traumatic event or not to achieve such growth is yet to be seen.
Australian Ethical's chief investment officer David Macri says investors are expecting more from companies, particularly when it comes to accountability of social and environmental issues.
""The demand for ethical investments has always been there, but the pandemic has certainly made people more aware of social issues,"" he says.
Recent research sponsored by Australian Ethical found that 86 per cent of Australians believe it's important their adviser asks about their values in relation to their investments. Nine of out 10 respondents also believed it was important that their adviser provided responsible or ethical investment options.
Fidelity's Tan adds that while it's widely understood that all decisions have non-financial consequences, but finance has been relatively slow to understand that.
""Finance has persisted with the view that you can make a certain financial return, you can make a certain investment, and then you calculate your rate of return and generate your investment rate of return back. So it's become a very closed system. Capital in and return out,"" Tan says.
Ethical companies also have a clear and concise corporate purpose that goes above and beyond benefitting shareholders. This is a dramatic change from the concept of companies existing for the benefit of shareholders to make profit, he says.
""We've got businesses that used to make airbags now make testing equipment, or used to make perfume and now make hand sanitisers. Not to mention the incredible work that's gone into biotech to create the vaccines,"" Tan says.
""Ethical companies take responsibility for their employees, have strong relationships across the corporate landscape, think about their impact on the environment and have a corporate purpose.
""These companies exist because they have a reason to exist. They're fulfilling a broader societal need or they're solving a problem we have in some way. And the profit that a company generates is the successful outcome of having fulfilled its purpose. It's not the reason why the company exists.""
The path to ethical investments varies. While Fidelity is happy to work with companies to lead them onto a more ethical path, Australian Ethical favours investments that demonstrate their ethical allegiance from the outset.
Tan says: ""Our philosophy is that if you want to reduce the world's carbon emissions, you can't do that by only owning low carbon emission companies. At some point, you have to work with people who are today emitting carbon emissions, because that's how you're going to make change.""
But the challenge lies in formalising the assessment process to measure ESG.
While investors look for the best tools to measure outcomes in a consistent way, Australian Ethical is bedding down the AI technology to give a clearer picture of the risks, which will be switched on later this year.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (93%); AUSTRALIAN BUSHFIRES (90%); CLIMATE CHANGE (90%); ESG FACTORS (90%); ESG FACTORS - ENVIRONMENTAL (90%); ESG FACTORS - SOCIAL (90%); ETHICAL INVESTING (90%); MUTUAL FUNDS (90%); SUSTAINABLE INVESTING (90%); GREEN FINANCE (89%); INVESTMENT MANAGEMENT (89%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (89%); SUSTAINABLE DEVELOPMENT (89%); DIVIDENDS (79%); CORPORATE SUSTAINABILITY (78%); ECONOMIC CRISIS (78%); FINANCIAL PERFORMANCE & REPORTS (78%); NEGATIVE ENVIRONMENTAL NEWS (78%); NEGATIVE NEWS (78%); WILDFIRES (78%); MANAGERS & SUPERVISORS (77%); PANDEMICS (77%); POLLUTION & ENVIRONMENTAL IMPACTS (77%); ESG FACTORS RATINGS (74%); INDUSTRY SECTOR PERFORMANCE (73%); Specialist Investments (%)
Company:  FIDELITY INTERNATIONAL LTD (56%)
Industry: NAICS525910 OPEN-END INVESTMENT FUNDS (56%); NAICS523930 INVESTMENT ADVICE (56%); NAICS523920 PORTFOLIO MANAGEMENT (56%); ETHICAL INVESTING (90%); MUTUAL FUNDS (90%); SUSTAINABLE INVESTING (90%); GREEN FINANCE (89%); INVESTMENT MANAGEMENT (89%); SUSTAINABLE DEVELOPMENT (89%); ESG FACTORS RATINGS (74%); MARKET RESEARCH (72%)
Geographic: AUSTRALIA (91%); SINGAPORE (79%)
Load-Date: March 19, 2021","Recovering from traumatic events is a lesson in how we can avoid the economic consequences the next time.
Take the GFC, for example. Australia's devastating bushfires. Growing evidence of serious climate change. And now the global pandemic.
Each traumatic event has created a clear shift in our individual and collective conscious thinking. Fund managers also say these and other traumatic events have helped to funnel more money into ethical investments.
But this latest traumatic event has had one of the biggest impacts of all, as we all look for places to stash our wealth that's safe for the long-term after a period of economic turmoil.
Ethical investments have come out trumps as the pandemic turbo-charges our appetite for ESG investments, prompting fund managers to prod companies right in the soft underbelly to ensure ethics play a part in our collective wealth creation process.
It's paying dividends, too. A recent white paper by Fidelity International analysed the market crash during the height of the pandemic, testing the effect of this volatility on companies with different environmental, social and governance characteristics.
It reveals a strong positive correlation between a company's relative market performance and its ESG rating, which held firm across a nine-month timeframe in the face of the pandemic during 2020.
The analysis concluded that companies with high sustainability ratings performed better than their peers when markets fall. This finding bore out Fidelity's initial hypothesis that companies with good sustainability characteristics have more prudent management and will demonstrate greater resilience in a crisis.
Ethics have been rising in importance for many years, but the pandemic turbo-charged the matter, Fidelity's global head of stewardship and sustainable investing Jenn-Hui Tan reveals.
Based in Singapore, Tan says the research suggests that the market does, in fact, discriminate between companies based on their attention to sustainability matters, both in crashes and recoveries, demonstrating why sustainability is at the heart of active portfolio management.
The ESG movement is about firstly acknowledging these non-financial outcomes, and secondly, trying to measure them, he says.
""The purpose of finance is ultimately not just about allocating capital in a way that maximises financial returns, but actually maximises capital in a way that creates strong, positive societal outcomes and minimises the environmental harms like climate change,"" Tan says.
""While on the surface, ESG appears to be a global concern given it relates to climate change and diversity, but at the corporate level, it's extremely local. In Australia, issues such as the Royal Commission and bushfires play out in a local context, directly impacting where investments are channelled. The fact is that you don't make progress by thinking globally about these issues.
""By allocating capital in favourable sustainable businesses, you can actually generate better financial performance for your clients. And you can actually live up to the purpose of the financial industry,"" Tan says.
Australian Ethical started forging this space 35 years ago. Five years ago, the fund had $300 million of under management, which has ballooned to $5.05 billion today. The growth has been underpinned by $0.42 billion in new inflows and $0.57 billion in market movements. Meanwhile, new members grew 22 per cent over the past 12 months.
Growing demand has prompted the fund to set an ambitious target of $25 billion in assets under management, strengthening its teams in preparation to hit the goal. But whether or not it will take a traumatic event or not to achieve such growth is yet to be seen.
Australian Ethical's chief investment officer David Macri says investors are expecting more from companies, particularly when it comes to accountability of social and environmental issues.
""The demand for ethical investments has always been there, but the pandemic has certainly made people more aware of social issues,"" he says.
Recent research sponsored by Australian Ethical found that 86 per cent of Australians believe it's important their adviser asks about their values in relation to their investments. Nine of out 10 respondents also believed it was important that their adviser provided responsible or ethical investment options.
Fidelity's Tan adds that while it's widely understood that all decisions have non-financial consequences, but finance has been relatively slow to understand that.
""Finance has persisted with the view that you can make a certain financial return, you can make a certain investment, and then you calculate your rate of return and generate your investment rate of return back. So it's become a very closed system. Capital in and return out,"" Tan says.
Ethical companies also have a clear and concise corporate purpose that goes above and beyond benefitting shareholders. This is a dramatic change from the concept of companies existing for the benefit of shareholders to make profit, he says.
""We've got businesses that used to make airbags now make testing equipment, or used to make perfume and now make hand sanitisers. Not to mention the incredible work that's gone into biotech to create the vaccines,"" Tan says.
""Ethical companies take responsibility for their employees, have strong relationships across the corporate landscape, think about their impact on the environment and have a corporate purpose.
""These companies exist because they have a reason to exist. They're fulfilling a broader societal need or they're solving a problem we have in some way. And the profit that a company generates is the successful outcome of having fulfilled its purpose. It's not the reason why the company exists.""
The path to ethical investments varies. While Fidelity is happy to work with companies to lead them onto a more ethical path, Australian Ethical favours investments that demonstrate their ethical allegiance from the outset.
Tan says: ""Our philosophy is that if you want to reduce the world's carbon emissions, you can't do that by only owning low carbon emission companies. At some point, you have to work with people who are today emitting carbon emissions, because that's how you're going to make change.""
But the challenge lies in formalising the assessment process to measure ESG.
While investors look for the best tools to measure outcomes in a consistent way, Australian Ethical is bedding down the AI technology to give a clearer picture of the risks, which will be switched on later this year.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (93%); AUSTRALIAN BUSHFIRES (90%); CLIMATE CHANGE (90%); ESG FACTORS (90%); ESG FACTORS - ENVIRONMENTAL (90%); ESG FACTORS - SOCIAL (90%); ETHICAL INVESTING (90%); MUTUAL FUNDS (90%); SUSTAINABLE INVESTING (90%); GREEN FINANCE (89%); INVESTMENT MANAGEMENT (89%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (89%); SUSTAINABLE DEVELOPMENT (89%); DIVIDENDS (79%); CORPORATE SUSTAINABILITY (78%); ECONOMIC CRISIS (78%); FINANCIAL PERFORMANCE & REPORTS (78%); NEGATIVE ENVIRONMENTAL NEWS (78%); NEGATIVE NEWS (78%); WILDFIRES (78%); MANAGERS & SUPERVISORS (77%); PANDEMICS (77%); POLLUTION & ENVIRONMENTAL IMPACTS (77%); ESG FACTORS RATINGS (74%); INDUSTRY SECTOR PERFORMANCE (73%); Specialist Investments (%)
Company:  FIDELITY INTERNATIONAL LTD (56%)
Industry: NAICS525910 OPEN-END INVESTMENT FUNDS (56%); NAICS523930 INVESTMENT ADVICE (56%); NAICS523920 PORTFOLIO MANAGEMENT (56%); ETHICAL INVESTING (90%); MUTUAL FUNDS (90%); SUSTAINABLE INVESTING (90%); GREEN FINANCE (89%); INVESTMENT MANAGEMENT (89%); SUSTAINABLE DEVELOPMENT (89%); ESG FACTORS RATINGS (74%); MARKET RESEARCH (72%)
Geographic: AUSTRALIA (91%); SINGAPORE (79%)
Load-Date: March 19, 2021",neutral,0.6275767683982849,balanced/neutral,['accountability'],[],['governance'],[],1,0,1,0
2021,Unknown Title,"Dateline: DUBLIN 
Body
The ""Ethical AI is Pivotal to the Maximization of the Future Growth Potential of the Global AI Market"" report has been added to ResearchAndMarkets.com's offering.
Artificial intelligence (AI) is transforming organizations, industries, and the technology landscape. The world is moving to the increased adoption of AI-powered smart applications/systems, and this trend will increase exponentially over the next few years. AI technologies are maturing, and the need to leverage their capabilities is becoming a CXO priority.
As businesses make AI part of their core strategy, the transformation of business functions, measures, and controls to ensure ethical best practices will gain importance. The implementation and the governance of ethical AI practices will become a priority and a board-level concern.
The deployment of AI solutions that are ethical (from a regulatory and a legal standpoint), transparent, and without bias will become essential. As governments and industry bodies across the world articulate AI regulations, AI companies must establish their ethical frameworks until roadmaps are clearly defined.
The operationalization of ethical AI principles is challenging for enterprises, given the large volumes of user-centric data that need to be processed, the breadth of use-cases, the regulatory variations in operating markets, and the diverse stakeholder priorities.
This also opens up opportunities for technology vendors and service providers. To effectively partner with enterprises and monetize these opportunities, ICT providers need to assess potential areas impacting AI ethics and evaluate opportunities across the people-process-technology spectrum.
Forward-thinking technology and service companies, including large ICT providers and start-ups, are working with enterprises and industry stakeholders to leverage potential opportunities. Ethical challenges will continue to be discovered and remediated to create sustained growth in potential advisory services.
As enterprises define goals, values, strategic outcomes, and key performance metrics, the time is right for technology companies to strategically partner with enterprises in the detection and the mitigation of ethical AI concerns.
Key Topics Covered:
1. Strategic Imperatives
Why is it Increasingly Difficult to Grow?
The Strategic Imperative
The Impact of the Top Three Strategic Imperatives on Ethical AI
Growth Opportunities Fuel the Growth Pipeline Engine
2. Growth Environment
AI Growth Drivers
AI Key Use-Cases
AI Ethics and Challenges to Adoption
3. Growth Opportunity Analysis
Increasing Ecosystem Focus on Ethical AI
Ethical AI Principles Require the Addressal of Multiple Elements/Dimensions
Growth Opportunity Segments
People
Process
Technology
The Way Forward
4. Growth Opportunity Universe
Growth Opportunity 1: Consulting and Advisory Services for AI Roadmaps and Ethical Considerations
For more information about this report visit https://www.researchandmarkets.com/r/jm6eup
View source version on businesswire.com: https://www.businesswire.com/news/home/20211124006013/en/
CONTACT: ResearchAndMarkets.com
Laura Wood, Senior Press Manager
press@researchandmarkets.com
For E.S.T Office Hours Call 1-917-300-0470
For U.S./CAN Toll Free Call 1-800-526-8630
For GMT Office Hours Call +353-1-416-8900
http://www.businesswire.com
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ARTIFICIAL INTELLIGENCE ETHICS (94%); MARKET RESEARCH REPORTS (92%); BUSINESS REPORTS & FORECASTS (91%); ARTIFICIAL INTELLIGENCE (90%); ETHICS (90%); PRESS RELEASES (90%); REPORTS, REVIEWS & SECTIONS (90%); BUSINESS & PROFESSIONAL ASSOCIATIONS (79%); BUSINESS METRICS (79%); RESEARCH REPORTS (79%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (78%); BEST PRACTICES (78%); TECHNOLOGY MATURITY (78%); TRENDS (78%); ASSOCIATIONS & ORGANIZATIONS (72%)
Company: RESEARCH-AND-MARKETS
Industry: ARTIFICIAL INTELLIGENCE ETHICS (94%); MARKET RESEARCH REPORTS (92%); ARTIFICIAL INTELLIGENCE (90%); MARKET RESEARCH (90%); INFORMATION TECHNOLOGY INDUSTRY (79%); MARKET RESEARCH & ANALYSIS (79%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (78%); Software (%); Technology (%); Data Management (%)
Load-Date: November 24, 2021","The ""Ethical AI is Pivotal to the Maximization of the Future Growth Potential of the Global AI Market"" report has been added to ResearchAndMarkets.com's offering.
Artificial intelligence (AI) is transforming organizations, industries, and the technology landscape. The world is moving to the increased adoption of AI-powered smart applications/systems, and this trend will increase exponentially over the next few years. AI technologies are maturing, and the need to leverage their capabilities is becoming a CXO priority.
As businesses make AI part of their core strategy, the transformation of business functions, measures, and controls to ensure ethical best practices will gain importance. The implementation and the governance of ethical AI practices will become a priority and a board-level concern.
The deployment of AI solutions that are ethical (from a regulatory and a legal standpoint), transparent, and without bias will become essential. As governments and industry bodies across the world articulate AI regulations, AI companies must establish their ethical frameworks until roadmaps are clearly defined.
The operationalization of ethical AI principles is challenging for enterprises, given the large volumes of user-centric data that need to be processed, the breadth of use-cases, the regulatory variations in operating markets, and the diverse stakeholder priorities.
This also opens up opportunities for technology vendors and service providers. To effectively partner with enterprises and monetize these opportunities, ICT providers need to assess potential areas impacting AI ethics and evaluate opportunities across the people-process-technology spectrum.
Forward-thinking technology and service companies, including large ICT providers and start-ups, are working with enterprises and industry stakeholders to leverage potential opportunities. Ethical challenges will continue to be discovered and remediated to create sustained growth in potential advisory services.
As enterprises define goals, values, strategic outcomes, and key performance metrics, the time is right for technology companies to strategically partner with enterprises in the detection and the mitigation of ethical AI concerns.
Key Topics Covered:
1. Strategic Imperatives
Why is it Increasingly Difficult to Grow?
The Strategic Imperative
The Impact of the Top Three Strategic Imperatives on Ethical AI
Growth Opportunities Fuel the Growth Pipeline Engine
2. Growth Environment
AI Growth Drivers
AI Key Use-Cases
AI Ethics and Challenges to Adoption
3. Growth Opportunity Analysis
Increasing Ecosystem Focus on Ethical AI
Ethical AI Principles Require the Addressal of Multiple Elements/Dimensions
Growth Opportunity Segments
People
Process
Technology
The Way Forward
4. Growth Opportunity Universe
Growth Opportunity 1: Consulting and Advisory Services for AI Roadmaps and Ethical Considerations
For more information about this report visit https://www.researchandmarkets.com/r/jm6eup
View source version on businesswire.com: https://www.businesswire.com/news/home/20211124006013/en/
CONTACT: ResearchAndMarkets.com
Laura Wood, Senior Press Manager
press@researchandmarkets.com
For E.S.T Office Hours Call 1-917-300-0470
For U.S./CAN Toll Free Call 1-800-526-8630
For GMT Office Hours Call +353-1-416-8900
http://www.businesswire.com
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ARTIFICIAL INTELLIGENCE ETHICS (94%); MARKET RESEARCH REPORTS (92%); BUSINESS REPORTS & FORECASTS (91%); ARTIFICIAL INTELLIGENCE (90%); ETHICS (90%); PRESS RELEASES (90%); REPORTS, REVIEWS & SECTIONS (90%); BUSINESS & PROFESSIONAL ASSOCIATIONS (79%); BUSINESS METRICS (79%); RESEARCH REPORTS (79%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (78%); BEST PRACTICES (78%); TECHNOLOGY MATURITY (78%); TRENDS (78%); ASSOCIATIONS & ORGANIZATIONS (72%)
Company: RESEARCH-AND-MARKETS
Industry: ARTIFICIAL INTELLIGENCE ETHICS (94%); MARKET RESEARCH REPORTS (92%); ARTIFICIAL INTELLIGENCE (90%); MARKET RESEARCH (90%); INFORMATION TECHNOLOGY INDUSTRY (79%); MARKET RESEARCH & ANALYSIS (79%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (78%); Software (%); Technology (%); Data Management (%)
Load-Date: November 24, 2021",positive,0.5468617081642151,balanced/neutral,['bias'],[],"['regulation', 'policy', 'governance', 'must', 'need to']",[],1,0,5,0
2021,Unknown Title,"Dateline: New Delhi, 2021-05-13 09:43:58 
Body
 May 13 -- While more companies are realizing the value of artificial intelligence (AI) and are levering the technology to improve efficiency and bring down costs, ethical concerns continue to rise as AI takes bigger decision-making role in more industries. According to the O'Reilly 2021 AI Adoption in the Enterprise report, less than half of companies surveyed said they haven't thought through the consequences AI products. Only organizations with matured AI practice bother to check the fairness, bias and ethics of their AI platforms. In that sense one can say that AI ethics is directly proportional to AI maturity of an organization. 
The problem however is that companies often overestimate their level of maturity when it comes to responsible AI implementation. As another survey conducted by BCG GAMMA finds that 55% of all respondents overestimated the maturity of their AI program. While 26% of companies say they've hit scale in their AI deployment, only 12% include a responsible AI program as part of their work. This clearly shows that organizations are overly optimistic about the maturity of their AI implementation. As Steven Mills, BCG GAMMA's chief ethics officer and coauthor mentions, ""While many organizations are making progress, it's clear the depth and breadth of most efforts fall behind what is needed to truly ensure responsible AI - creating a major ethical dilemma in AI."" The O'Reilly report also notes that AI implementation won't hit maturity until ethics, safety, privacy, and security are primary rather than secondary concerns. To support AI maturity, teams can benefit from reviewing case studies in how other organizations have managed AI implementation, believes Rachel Roumeliotis, VP of Content Strategy, O'Reilly Media. For some companies, the ethical dimension of AI implementation hasn't been fully thought through because they have yet to deploy at scale and reach full maturity, explains Roumeliotis. He adds that bias can seep into AI products at multiple points in the creation process from the data fueling decisions to the algorithmic training and final review stage. Viral Thakker, Partner, Deloitte India, says, ""AI ethics deal with managing ethical complexities in the age of vast amounts of data and extensive use of automation. The big issues driving this are privacy considerations, lack of transparency of ""black box"" AI models, bias and discrimination that may be embedded in the data, which AI algorithms learn from, as well as lack of governance and accountability. Worldwide business spending on AI is expected to hit $50 billion this year and $110 billion annually by 2024, even after the global economic slump caused by the COVID-19 pandemic, according to a forecast by research firm IDC. Retail, healthcare and banking industries are slated to spend the maximum, said the analyst firm. For all the good that AI can bring, policy makers and responsible tech companies must recognize, execute, and mitigate its potential unintended, harmful effects. Praveen Kumar, Vice President, Digital & Innovations, JK Technosoft states, ""Organizations should start thinking of educating and upskilling employees as AI will incorporate a great deal of work alongside human workforces. At the same time, it is of utmost importance to consider the necessary and adequate governance mechanisms carefully and proactively for ensuring ethical considerations in the deployment of responsible AI tools."" Already several organizations are offering programs to help employees put ethics at the core of their respective workflows. These programs are designed to empower the entire organization to think critically about every step of the process of building AI solutions, and perhaps most importantly continue advancing AI that is safe and inclusive. A big sign in AI maturity is to grow the company's knowledge on the subject, and nurture an organization-wide 'ethical-first' approach, very similar to the 'security-first' philosophy. As we move forward, our reliability on AI will deepen which will inevitably cause many ethical issues, especially in industries where personal and business data is at stake Companies should therefore start thinking about how they will retrain and educate employees with regard to AI's ethical implication. Shreeranganath Kulkarni, Chief Delivery Officer, Birlasoft suggests that for enterprise AI to reach maturity, they should work towards an AI playbook built on the pillars of AI strategy, data, talent, technology, execution, and culture. Organizations that are able to adapt and upgrade themselves and emerge stronger will be the ones to benefit from any kind of disruptions. In the process, they must consider carefully and proactively the necessary governance mechanisms to be used to ensure ethical considerations in the deployment of AI tools. Building trust in AI solutions and tools is vital so that businesses and individuals can benefit from its use. 
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ARTIFICIAL INTELLIGENCE ETHICS (95%); ETHICS (95%); ARTIFICIAL INTELLIGENCE (90%); BUSINESS ETHICS (78%); CORPORATE GOVERNANCE (78%); POLLS & SURVEYS (77%); DISCRIMINATION (75%); EXECUTIVES (71%); SAFETY (69%); EPIDEMICS (65%); INFECTIOUS DISEASE (65%); CASE STUDIES (63%); COVID CORONAVIRUS (63%); COVID-19 CORONAVIRUS (60%)
Company:  DELOITTE LLP (52%)
Industry: NAICS541211 OFFICES OF CERTIFIED PUBLIC ACCOUNTANTS (52%); SIC8721 ACCOUNTING, AUDITING, & BOOKKEEPING SERVICES (52%); ARTIFICIAL INTELLIGENCE ETHICS (95%); ARTIFICIAL INTELLIGENCE (90%); INFORMATION TECHNOLOGY INDUSTRY (78%); MEDIA BIAS (75%); INFORMATION SECURITY & PRIVACY (69%); BANKING & FINANCE (62%)
Person: BILL O'REILLY (76%)
Geographic: NEW DELHI, INDIA (59%)
Load-Date: May 13, 2021","May 13 -- While more companies are realizing the value of artificial intelligence (AI) and are levering the technology to improve efficiency and bring down costs, ethical concerns continue to rise as AI takes bigger decision-making role in more industries. According to the O'Reilly 2021 AI Adoption in the Enterprise report, less than half of companies surveyed said they haven't thought through the consequences AI products. Only organizations with matured AI practice bother to check the fairness, bias and ethics of their AI platforms. In that sense one can say that AI ethics is directly proportional to AI maturity of an organization. 
The problem however is that companies often overestimate their level of maturity when it comes to responsible AI implementation. As another survey conducted by BCG GAMMA finds that 55% of all respondents overestimated the maturity of their AI program. While 26% of companies say they've hit scale in their AI deployment, only 12% include a responsible AI program as part of their work. This clearly shows that organizations are overly optimistic about the maturity of their AI implementation. As Steven Mills, BCG GAMMA's chief ethics officer and coauthor mentions, ""While many organizations are making progress, it's clear the depth and breadth of most efforts fall behind what is needed to truly ensure responsible AI - creating a major ethical dilemma in AI."" The O'Reilly report also notes that AI implementation won't hit maturity until ethics, safety, privacy, and security are primary rather than secondary concerns. To support AI maturity, teams can benefit from reviewing case studies in how other organizations have managed AI implementation, believes Rachel Roumeliotis, VP of Content Strategy, O'Reilly Media. For some companies, the ethical dimension of AI implementation hasn't been fully thought through because they have yet to deploy at scale and reach full maturity, explains Roumeliotis. He adds that bias can seep into AI products at multiple points in the creation process from the data fueling decisions to the algorithmic training and final review stage. Viral Thakker, Partner, Deloitte India, says, ""AI ethics deal with managing ethical complexities in the age of vast amounts of data and extensive use of automation. The big issues driving this are privacy considerations, lack of transparency of ""black box"" AI models, bias and discrimination that may be embedded in the data, which AI algorithms learn from, as well as lack of governance and accountability. Worldwide business spending on AI is expected to hit $50 billion this year and $110 billion annually by 2024, even after the global economic slump caused by the COVID-19 pandemic, according to a forecast by research firm IDC. Retail, healthcare and banking industries are slated to spend the maximum, said the analyst firm. For all the good that AI can bring, policy makers and responsible tech companies must recognize, execute, and mitigate its potential unintended, harmful effects. Praveen Kumar, Vice President, Digital & Innovations, JK Technosoft states, ""Organizations should start thinking of educating and upskilling employees as AI will incorporate a great deal of work alongside human workforces. At the same time, it is of utmost importance to consider the necessary and adequate governance mechanisms carefully and proactively for ensuring ethical considerations in the deployment of responsible AI tools."" Already several organizations are offering programs to help employees put ethics at the core of their respective workflows. These programs are designed to empower the entire organization to think critically about every step of the process of building AI solutions, and perhaps most importantly continue advancing AI that is safe and inclusive. A big sign in AI maturity is to grow the company's knowledge on the subject, and nurture an organization-wide 'ethical-first' approach, very similar to the 'security-first' philosophy. As we move forward, our reliability on AI will deepen which will inevitably cause many ethical issues, especially in industries where personal and business data is at stake Companies should therefore start thinking about how they will retrain and educate employees with regard to AI's ethical implication. Shreeranganath Kulkarni, Chief Delivery Officer, Birlasoft suggests that for enterprise AI to reach maturity, they should work towards an AI playbook built on the pillars of AI strategy, data, talent, technology, execution, and culture. Organizations that are able to adapt and upgrade themselves and emerge stronger will be the ones to benefit from any kind of disruptions. In the process, they must consider carefully and proactively the necessary governance mechanisms to be used to ensure ethical considerations in the deployment of AI tools. Building trust in AI solutions and tools is vital so that businesses and individuals can benefit from its use. 
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ARTIFICIAL INTELLIGENCE ETHICS (95%); ETHICS (95%); ARTIFICIAL INTELLIGENCE (90%); BUSINESS ETHICS (78%); CORPORATE GOVERNANCE (78%); POLLS & SURVEYS (77%); DISCRIMINATION (75%); EXECUTIVES (71%); SAFETY (69%); EPIDEMICS (65%); INFECTIOUS DISEASE (65%); CASE STUDIES (63%); COVID CORONAVIRUS (63%); COVID-19 CORONAVIRUS (60%)
Company:  DELOITTE LLP (52%)
Industry: NAICS541211 OFFICES OF CERTIFIED PUBLIC ACCOUNTANTS (52%); SIC8721 ACCOUNTING, AUDITING, & BOOKKEEPING SERVICES (52%); ARTIFICIAL INTELLIGENCE ETHICS (95%); ARTIFICIAL INTELLIGENCE (90%); INFORMATION TECHNOLOGY INDUSTRY (78%); MEDIA BIAS (75%); INFORMATION SECURITY & PRIVACY (69%); BANKING & FINANCE (62%)
Person: BILL O'REILLY (76%)
Geographic: NEW DELHI, INDIA (59%)
Load-Date: May 13, 2021",neutral,0.7194255590438843,balanced/neutral,"['privacy', 'bias', 'discrimination', 'fairness', 'transparency', 'accountability', 'safety', 'security']",['fairness'],"['policy', 'governance', 'should', 'must']",[],8,1,4,0
2021,Unknown Title,"Body
London: Bristows LLP has issued the following press release:
Following his commentary on the European Commission’s report on the ethics of data and artificial intelligencein connected and autonomous vehicles (CAVs), which was published in October 2020, Jamie Witton was interviewed by journalist Graham Jarvis, from TU Automotive, on the topic.
The report asks questions aboutthe existing dilemmas in the field that current ethics can’t solve; about the requirements in terms of safety, human dignity, personal freedom of choice and thendata protection; and about distribution of responsibility.
Jamie observed that technical discussions are constantly ahead of where the legislation is, so it’s important that legislators look at the ethical issues emerging even at the current -limited- level of automation. There is a need for legislation and tech “to grow together. ”
One of the main issues is that self driving technology generates a lot of data, often related to one or more identifiable people, so the collection and processing of that personal data must be lawful, fair and, importantly, transparent. Data protection law requires manufacturers to explain how their AI works, both at a technical and a layman’s level.
Given the novelty of the technology, and the ‘black box’ nature of the algorithms controlling the artificial intelligence of CAVs, explaining how the product works to users presents an obvious challenge. The AI must be capable of being audited, too. “Without accountability, there is no trust”, he says.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: IFN
Subject: ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); COMPANY PRESS RELEASES (78%); DATA PROTECTION LAWS (78%); JOURNALISM (78%); AUDITS (73%); PRESS RELEASES (73%); WRITERS (72%); LEGISLATIVE BODIES (70%)
Organization: EUROPEAN COMMISSION (84%)
Industry: ARTIFICIAL INTELLIGENCE (90%); AUTOMOTIVE (90%); AUTONOMOUS MOTOR VEHICLES (90%); AUTONOMOUS VEHICLES (90%); DATA PROTECTION LAWS (78%); DATA SECURITY (78%); INFORMATION SECURITY & PRIVACY (78%); MANUFACTURING (75%); WRITERS (72%)
Load-Date: August 10, 2021","London: Bristows LLP has issued the following press release:
Following his commentary on the European Commission’s report on the ethics of data and artificial intelligencein connected and autonomous vehicles (CAVs), which was published in October 2020, Jamie Witton was interviewed by journalist Graham Jarvis, from TU Automotive, on the topic.
The report asks questions aboutthe existing dilemmas in the field that current ethics can’t solve; about the requirements in terms of safety, human dignity, personal freedom of choice and thendata protection; and about distribution of responsibility.
Jamie observed that technical discussions are constantly ahead of where the legislation is, so it’s important that legislators look at the ethical issues emerging even at the current -limited- level of automation. There is a need for legislation and tech “to grow together. ”
One of the main issues is that self driving technology generates a lot of data, often related to one or more identifiable people, so the collection and processing of that personal data must be lawful, fair and, importantly, transparent. Data protection law requires manufacturers to explain how their AI works, both at a technical and a layman’s level.
Given the novelty of the technology, and the ‘black box’ nature of the algorithms controlling the artificial intelligence of CAVs, explaining how the product works to users presents an obvious challenge. The AI must be capable of being audited, too. “Without accountability, there is no trust”, he says.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: IFN
Subject: ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); COMPANY PRESS RELEASES (78%); DATA PROTECTION LAWS (78%); JOURNALISM (78%); AUDITS (73%); PRESS RELEASES (73%); WRITERS (72%); LEGISLATIVE BODIES (70%)
Organization: EUROPEAN COMMISSION (84%)
Industry: ARTIFICIAL INTELLIGENCE (90%); AUTOMOTIVE (90%); AUTONOMOUS MOTOR VEHICLES (90%); AUTONOMOUS VEHICLES (90%); DATA PROTECTION LAWS (78%); DATA SECURITY (78%); INFORMATION SECURITY & PRIVACY (78%); MANUFACTURING (75%); WRITERS (72%)
Load-Date: August 10, 2021",neutral,0.9071946144104004,balanced/neutral,"['privacy', 'accountability', 'safety', 'security']",['dignity'],"['legislation', 'law', 'must']",[],4,1,3,0
2021,Unknown Title,"Body
IEEE 7000T-2021 integrates ethical and functional requirements to mitigate risk and increase innovation in systems engineering product design
PISCATAWAY, N.J. - IEEE, the world's largest technical professional organization dedicated to advancing technology for humanity, and the IEEE Standards Association (IEEE SA)( https://cts.businesswire.com/ct/CT?id=smartlink&;url=https%3A%2F%2Fstandards.ieee.org%2F%3Futm_source%3Dbusinesswire%26utm_medium%3Dpr%26utm_campaign%3Dais-2021&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=IEEE+Standards+Association+%28IEEE+SA%29&;index=1&;md5=3ce6f79cd4ebcb7143f540d92af5ef2c ) today announced the launch of IEEE 7000T-2021 - IEEE Standard Model Process for Addressing Ethical Concerns During System Design( https://cts.businesswire.com/ct/CT?id=smartlink&;url=https%3A%2F%2Fengagestandards.ieee.org%2Fieee-7000-2021-for-systems-design-ethical-concerns.html%3Futm_source%3Dbusinesswire%26utm_medium%3Dpr%26utm_campaign%3Dais-2021&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=IEEE+Standard+Model+Process+for+Addressing+Ethical+Concerns+During+System+Design&;index=2&;md5=20a9846460f256332cbf6c313b86ce5d ) that provides a clear methodology to analyze human and social values relevant for an ethical system engineering effort.
IEEE 7000-2021 is recommended for use by organizations that seek to apply broader ethical value criteria and minimize risk, thereby helping to strengthen relationships with their end users and customers. Organizations may apply this first of its kind standard across multiple levels including concept exploration, system requirements definition, or development of new or revised products or services.
""Engineers, their managers, and other stakeholders benefit from well-defined processes for considering ethical issues along with the usual concerns of system performance and functionality early in the system life cycle,"" said Konstantinos Karachalios, Managing Director of IEEE SA. ""End users can be unaware of the ethical considerations regarding the products and services they use; this is why IEEE supported the launch of the IEEE 7000 series of standards including this standard that also complements our Ethics Certification Program for Autonomous and Intelligent Systems (ECPAIS)( https://cts.businesswire.com/ct/CT?id=smartlink&;url=https%3A%2F%2Fstandards.ieee.org%2Findustry-connections%2Fecpais.html%3Futm_source%3Dbusinesswire%26utm_medium%3Dpr%26utm_campaign%3Dais-2021&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=Ethics+Certification+Program+for+Autonomous+and+Intelligent+Systems+%28ECPAIS%29&;index=3&;md5=d8b463943bcda5fb3fdccb392f935af9 ) certification criteria offering. It is only by rigorously prioritizing ethical concerns at the outset of design that manufacturers, engineers, and technologists can responsibly align products and services with results honoring the contextual values of customers, citizens, and society at large.""
This standard provides:
a system engineering standard approach integrating human and social values into traditional systems engineering and design.
processes for engineers to translate stakeholder values and ethical considerations into system requirements and design practices.
a systematic, transparent, and traceable approach to address ethically-oriented regulatory obligations in the design of autonomous intelligent systems.
""Value-based Engineering (VbE), a methodology providing ways to elicit, conceptualize, prioritize and respect end user values in system design, is at the heart of IEEE 7000-2021 and provides companies with a highly practical approach to master the values based challenges of their digital transformation,"" said Dr. Sarah Spiekermann, Chair of The Institute for Information Systems &; Society at Vienna University of Economics and Business (WU Vienna) and Vice-Chair of IEEE 7000-2021. ""IEEE 7000-2021 test users identified ten issues per person involved in the project, demonstrating that the utilization of value-based Engineering can lead to fewer project risks and exponential innovation. This is a massive improvement relative to current technical roadmap processes.""
A key part of digital transformation provided by IEEE 7000-2021 comes in addressing risk. Where traditional evaluations of technological risk may focus largely on areas of physical harm, the VbE methodology provides a broader lens to consider also potential value harms associated with product or systems design. This makes the standard unique and deeply important in terms of ease of adoption of applied ethics methodologies in emerging technologies such as AI.
The use of this standard could help organizations better earn and keep the trust of end-users and stakeholders by directly addressing ethical concerns upfront, leading to greater market acceptance of their products, services, or systems.
For more information, to purchase, or to view a read-only version of the IEEE 7000-2021 Standard, click here( https://cts.businesswire.com/ct/CT?id=smartlink&;url=https%3A%2F%2Fengagestandards.ieee.org%2Fieee-7000-2021-for-systems-design-ethical-concerns.html%3Futm_source%3Dbusinesswire%26utm_medium%3Dpr%26utm_campaign%3Dais-2021&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=click+here&;index=4&;md5=be117a357c282c032e51c9cf18610b3d ).
To learn more about IEEE SA or about any of its many market-driven initiatives, visit us on Facebook( https://cts.businesswire.com/ct/CT?id=smartlink&;url=http%3A%2F%2Fwww.facebook.com%2Fieeesa&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=Facebook&;index=5&;md5=f57f429eb51947c1807f03bcb1e0c5b3 ), follow us on Twitter( https://cts.businesswire.com/ct/CT?id=smartlink&;url=http%3A%2F%2Fwww.twitter.com%2Fieeesa&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=Twitter&;index=6&;md5=1af46b5e6f2c3683c4d16e16d9131b80 ), connect with us on LinkedIn( https://cts.businesswire.com/ct/CT?id=smartlink&;url=https%3A%2F%2Fwww.linkedin.com%2Fcompany%2Fieee-sa-ieee-standards-association&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=LinkedIn&;index=7&;md5=dc9d012a544455b14b42f5222f49d802 ), or on the Beyond Standards Blog( https://cts.businesswire.com/ct/CT?id=smartlink&;url=https%3A%2F%2Fbeyondstandards.ieee.org%2F%3Futm_source%3Dbusinesswire%26utm_medium%3Dpr%26utm_campaign%3Dais-2021&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=Beyond+Standards+Blog&;index=8&;md5=671943f2e036cd6a6c873b5dd7c88382 ).
About the IEEE Standards Association
IEEE Standards Association (IEEE SA) is a collaborative organization where innovators raise the world's standards for technology. IEEE SA provides a globally open, consensus-building environment and platform that empowers people to work together in the development of leading-edge, market-relevant technology standards, and industry solutions shaping a better, safer, and sustainable world. For more information, visit https://standards.ieee.org( https://cts.businesswire.com/ct/CT?id=smartlink&;url=https%3A%2F%2Fstandards.ieee.org&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=https%3A%2F%2Fstandards.ieee.org&;index=9&;md5=7c98b83f11f0b3ee5d810a8184f7ea91 ).
About IEEE
IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. Through its highly cited publications, conferences, technology standards, and professional and educational activities, IEEE is the trusted voice in a wide variety of areas ranging from aerospace systems, computers, and telecommunications to biomedical engineering, electric power, and consumer electronics. Learn more at https://www.ieee.org( https://cts.businesswire.com/ct/CT?id=smartlink&;url=http%3A%2F%2Fwww.ieee.org&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=https%3A%2F%2Fwww.ieee.org&;index=10&;md5=232d8d4e49cf8c27318df9a4687ab82c ).
View source version on businesswire.com: https://www.businesswire.com/news/home/20210915005012/en/( https://www.businesswire.com/news/home/20210915005012/en/ )
Contacts
Lloyd Green, Strategic Market Engagement &; Operations Director
John C. Havens, Director of Emerging Technology &; Strategic Development
standards-pr@ieee.org( standards-pr@ieee.org )
#distro !@COPYRIGHT=© 2021 Postmedia Network Inc. All rights reserved.
Classification
Language: ENGLISH
Publication-Type: Newspaper
Subject: ETHICS (92%); ASSOCIATIONS & ORGANIZATIONS (90%); ENGINEERING (90%); PRESS RELEASES (90%); PRODUCT DEVELOPMENT (89%); STANDARDS & MEASUREMENTS (79%); NEW PRODUCTS (78%); PRODUCT INNOVATION (78%); PROFESSIONAL WORKERS (78%); MANAGERS & SUPERVISORS (63%); integrates,ethical,functional,requirements,mitigate,increase !@PERMALINK= https://financialpost.com/pmn/press-releases-pmn/business-wire-news-releases-pmn/ieee-launches-new-standard-to-address-ethical-concerns-during-systems-design (%)
Industry: ENGINEERING (90%); NEW PRODUCTS (78%)
Geographic: NEW JERSEY, USA (78%)
Load-Date: September 15, 2021","IEEE 7000T-2021 integrates ethical and functional requirements to mitigate risk and increase innovation in systems engineering product design
PISCATAWAY, N.J. - IEEE, the world's largest technical professional organization dedicated to advancing technology for humanity, and the IEEE Standards Association (IEEE SA)( https://cts.businesswire.com/ct/CT?id=smartlink&;url=https%3A%2F%2Fstandards.ieee.org%2F%3Futm_source%3Dbusinesswire%26utm_medium%3Dpr%26utm_campaign%3Dais-2021&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=IEEE+Standards+Association+%28IEEE+SA%29&;index=1&;md5=3ce6f79cd4ebcb7143f540d92af5ef2c ) today announced the launch of IEEE 7000T-2021 - IEEE Standard Model Process for Addressing Ethical Concerns During System Design( https://cts.businesswire.com/ct/CT?id=smartlink&;url=https%3A%2F%2Fengagestandards.ieee.org%2Fieee-7000-2021-for-systems-design-ethical-concerns.html%3Futm_source%3Dbusinesswire%26utm_medium%3Dpr%26utm_campaign%3Dais-2021&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=IEEE+Standard+Model+Process+for+Addressing+Ethical+Concerns+During+System+Design&;index=2&;md5=20a9846460f256332cbf6c313b86ce5d ) that provides a clear methodology to analyze human and social values relevant for an ethical system engineering effort.
IEEE 7000-2021 is recommended for use by organizations that seek to apply broader ethical value criteria and minimize risk, thereby helping to strengthen relationships with their end users and customers. Organizations may apply this first of its kind standard across multiple levels including concept exploration, system requirements definition, or development of new or revised products or services.
""Engineers, their managers, and other stakeholders benefit from well-defined processes for considering ethical issues along with the usual concerns of system performance and functionality early in the system life cycle,"" said Konstantinos Karachalios, Managing Director of IEEE SA. ""End users can be unaware of the ethical considerations regarding the products and services they use; this is why IEEE supported the launch of the IEEE 7000 series of standards including this standard that also complements our Ethics Certification Program for Autonomous and Intelligent Systems (ECPAIS)( https://cts.businesswire.com/ct/CT?id=smartlink&;url=https%3A%2F%2Fstandards.ieee.org%2Findustry-connections%2Fecpais.html%3Futm_source%3Dbusinesswire%26utm_medium%3Dpr%26utm_campaign%3Dais-2021&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=Ethics+Certification+Program+for+Autonomous+and+Intelligent+Systems+%28ECPAIS%29&;index=3&;md5=d8b463943bcda5fb3fdccb392f935af9 ) certification criteria offering. It is only by rigorously prioritizing ethical concerns at the outset of design that manufacturers, engineers, and technologists can responsibly align products and services with results honoring the contextual values of customers, citizens, and society at large.""
This standard provides:
a system engineering standard approach integrating human and social values into traditional systems engineering and design.
processes for engineers to translate stakeholder values and ethical considerations into system requirements and design practices.
a systematic, transparent, and traceable approach to address ethically-oriented regulatory obligations in the design of autonomous intelligent systems.
""Value-based Engineering (VbE), a methodology providing ways to elicit, conceptualize, prioritize and respect end user values in system design, is at the heart of IEEE 7000-2021 and provides companies with a highly practical approach to master the values based challenges of their digital transformation,"" said Dr. Sarah Spiekermann, Chair of The Institute for Information Systems &; Society at Vienna University of Economics and Business (WU Vienna) and Vice-Chair of IEEE 7000-2021. ""IEEE 7000-2021 test users identified ten issues per person involved in the project, demonstrating that the utilization of value-based Engineering can lead to fewer project risks and exponential innovation. This is a massive improvement relative to current technical roadmap processes.""
A key part of digital transformation provided by IEEE 7000-2021 comes in addressing risk. Where traditional evaluations of technological risk may focus largely on areas of physical harm, the VbE methodology provides a broader lens to consider also potential value harms associated with product or systems design. This makes the standard unique and deeply important in terms of ease of adoption of applied ethics methodologies in emerging technologies such as AI.
The use of this standard could help organizations better earn and keep the trust of end-users and stakeholders by directly addressing ethical concerns upfront, leading to greater market acceptance of their products, services, or systems.
For more information, to purchase, or to view a read-only version of the IEEE 7000-2021 Standard, click here( https://cts.businesswire.com/ct/CT?id=smartlink&;url=https%3A%2F%2Fengagestandards.ieee.org%2Fieee-7000-2021-for-systems-design-ethical-concerns.html%3Futm_source%3Dbusinesswire%26utm_medium%3Dpr%26utm_campaign%3Dais-2021&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=click+here&;index=4&;md5=be117a357c282c032e51c9cf18610b3d ).
To learn more about IEEE SA or about any of its many market-driven initiatives, visit us on Facebook( https://cts.businesswire.com/ct/CT?id=smartlink&;url=http%3A%2F%2Fwww.facebook.com%2Fieeesa&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=Facebook&;index=5&;md5=f57f429eb51947c1807f03bcb1e0c5b3 ), follow us on Twitter( https://cts.businesswire.com/ct/CT?id=smartlink&;url=http%3A%2F%2Fwww.twitter.com%2Fieeesa&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=Twitter&;index=6&;md5=1af46b5e6f2c3683c4d16e16d9131b80 ), connect with us on LinkedIn( https://cts.businesswire.com/ct/CT?id=smartlink&;url=https%3A%2F%2Fwww.linkedin.com%2Fcompany%2Fieee-sa-ieee-standards-association&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=LinkedIn&;index=7&;md5=dc9d012a544455b14b42f5222f49d802 ), or on the Beyond Standards Blog( https://cts.businesswire.com/ct/CT?id=smartlink&;url=https%3A%2F%2Fbeyondstandards.ieee.org%2F%3Futm_source%3Dbusinesswire%26utm_medium%3Dpr%26utm_campaign%3Dais-2021&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=Beyond+Standards+Blog&;index=8&;md5=671943f2e036cd6a6c873b5dd7c88382 ).
About the IEEE Standards Association
IEEE Standards Association (IEEE SA) is a collaborative organization where innovators raise the world's standards for technology. IEEE SA provides a globally open, consensus-building environment and platform that empowers people to work together in the development of leading-edge, market-relevant technology standards, and industry solutions shaping a better, safer, and sustainable world. For more information, visit https://standards.ieee.org( https://cts.businesswire.com/ct/CT?id=smartlink&;url=https%3A%2F%2Fstandards.ieee.org&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=https%3A%2F%2Fstandards.ieee.org&;index=9&;md5=7c98b83f11f0b3ee5d810a8184f7ea91 ).
About IEEE
IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. Through its highly cited publications, conferences, technology standards, and professional and educational activities, IEEE is the trusted voice in a wide variety of areas ranging from aerospace systems, computers, and telecommunications to biomedical engineering, electric power, and consumer electronics. Learn more at https://www.ieee.org( https://cts.businesswire.com/ct/CT?id=smartlink&;url=http%3A%2F%2Fwww.ieee.org&;esheet=52489151&;newsitemid=20210915005012&;lan=en-US&;anchor=https%3A%2F%2Fwww.ieee.org&;index=10&;md5=232d8d4e49cf8c27318df9a4687ab82c ).
View source version on businesswire.com: https://www.businesswire.com/news/home/20210915005012/en/( https://www.businesswire.com/news/home/20210915005012/en/ )
Contacts
Lloyd Green, Strategic Market Engagement &; Operations Director
John C. Havens, Director of Emerging Technology &; Strategic Development
standards-pr@ieee.org( standards-pr@ieee.org )
#distro !@COPYRIGHT=© 2021 Postmedia Network Inc. All rights reserved.
Classification
Language: ENGLISH
Publication-Type: Newspaper
Subject: ETHICS (92%); ASSOCIATIONS & ORGANIZATIONS (90%); ENGINEERING (90%); PRESS RELEASES (90%); PRODUCT DEVELOPMENT (89%); STANDARDS & MEASUREMENTS (79%); NEW PRODUCTS (78%); PRODUCT INNOVATION (78%); PROFESSIONAL WORKERS (78%); MANAGERS & SUPERVISORS (63%); integrates,ethical,functional,requirements,mitigate,increase !@PERMALINK= https://financialpost.com/pmn/press-releases-pmn/business-wire-news-releases-pmn/ieee-launches-new-standard-to-address-ethical-concerns-during-systems-design (%)
Industry: ENGINEERING (90%); NEW PRODUCTS (78%)
Geographic: NEW JERSEY, USA (78%)
Load-Date: September 15, 2021",neutral,0.5462672114372253,balanced/neutral,[],[],"['standards', 'certification']",[],0,0,2,0
2021,Unknown Title,"Body
Experts from FAO and the Philippines Department of Agriculture using drones to gather visual data on damaged rice crops.
The Pontifical Academy for Life, together with the first co-signatories of the Rome Call for Artificial Intelligence (AI) Ethics: Microsoft, IBM, and the Food and Agriculture Organization of the United Nations (FAO), today marked the first anniversary of the document which has been endorsed by Pope Francis and seeks a commitment towards developing AI technologies in ways that are transparent, inclusive, socially beneficial and accountable.
""Progress can make a better world possible if it goes together with the common good"". This was reiterated by Archbishop Vincenzo Paglia, President of the Pontifical Academy for Life, recalling that on February 28, 2020, Microsoft, IBM, FAO, the Minister for Technological Innovation of the Italian government, signed the Rome Call for AI Ethics, promoted by the Pontifical Academy for Life.
""After 12 months the 'family' of signatories has grown and we are working to make the document more and more known, in view of further accessions by strategic actors for an ethical approach to the themes of Artificial Intelligence,"" Archbishop Paglia noted. ""A channel of dialogue with monotheistic religions is open, in order to converge on a common vision of technology at the service of all humanity. The depth and acceleration of the transformations in the digital age raise global and constantly evolving issues. A year after the Call, the Pontifical Academy for Life is increasingly convinced and determined on the importance of placing itself at the service of each person in his/her entirety and of all people, without discrimination or exclusion. The complexity of the technological world requires a more articulated ethical reflection, to make our commitment truly incisive. We need a new alliance between research, science and ethics, because we stand at a crucial crossroads, in order to build a world where technology is actually used for the development of peoples. That is a request coming from faith and reason. Without equitable and widespread development there will be no justice, there will be no peace, there will be no universal brotherhood.""
The Rome Call for AI Ethics is a document created to support an ethical approach to Artificial Intelligence and promote a sense of shared responsibility among organizations, governments and institutions with the aim of guaranteeing a future in which digital innovation and technological progress are at the service of human ingenuity and creativity.
""By 2050, the world will have to feed about 10 billion people. This will only be possible with transformed agri-food systems that are inclusive, resilient and sustainable. Artificial Intelligence in Food and Agriculture plays a key role in this transformation and in achieving Food for All,"" said FAO Director-General QU Dongyu. ""At FAO, we use ethical AI in our work for Better Production, Better Nutrition, a Better Environment and a Better Life. All people have the right to benefit from an ethical AI and reap the digital dividend. FAO has been fully committed with the Rome Call for AI ethics,"" he added.
""A year ago, we joined Archbishop Vincenzo Paglia and the Pontifical Academy for Life to honor the six fundamental principles in the Rome Call for AI and Ethics and ensure that technology continues to serve humanity,"" said Microsoft President Brad Smith. ""As we recover from the COVID-19 pandemic, the Rome Call will be even more important as we think more broadly and ethically about the future of technology. The Rome Call helps put us on this path to promote a thoughtful, respectful and inclusive conversation about the intersection of artificial intelligence technology and society.""
""At IBM we believe that AI has the ability to transform and improve our lives and our society in many important ways,"" said Dario Gil, Senior Vice President and Director of IBM Research. ""For all of us to benefit from AI, it requires a commitment to actively develop, deploy, and use it responsibly in order to prevent adverse outcomes. This is why IBM was so proud to join the Pontifical Academy for Life as one of the initial signers of the Rome Call for an AI Ethics, supporting an ethical approach to Artificial Intelligence. Our work in support of AI Ethics permeates our entire company, which includes a centralized governance framework, risk assessment protocols, trustworthy AI development methodologies, education and training initiatives, research innovations, and open source toolkits to help others bolster their AI ethics efforts,"" Gil added.
The Rome Call for AI Ethics stresses that ""AI systems must be conceived, designed and implemented to serve and protect human beings and the environment in which they live,"" a concept that many participants reiterated one year ago.
(The Food and Agriculture Organization)
Classification
Language: ENGLISH
Publication-Type: Wire
Subject: ARTIFICIAL INTELLIGENCE ETHICS (91%); ETHICS (91%); AGRICULTURE DEPARTMENTS (90%); ANNIVERSARIES (90%); ARTIFICIAL INTELLIGENCE (90%); CATHOLIC POPES (90%); CHRISTIANS & CHRISTIANITY (90%); CLERGY & RELIGIOUS VOCATIONS (90%); RELIGION (90%); THIS DAY IN HISTORY (90%); UNITED NATIONS (78%); ASSOCIATIONS & ORGANIZATIONS (77%); EMERGING TECHNOLOGY (77%); DISCRIMINATION (75%); UNITED NATIONS INSTITUTIONS (73%); PRODUCT INNOVATION (68%)
Company:  MICROSOFT CORP (90%)
Ticker: MSFT (NASDAQ) (90%)
Industry: NAICS511210 SOFTWARE PUBLISHERS (90%); SIC7372 PREPACKAGED SOFTWARE (90%); ARTIFICIAL INTELLIGENCE ETHICS (91%); AGRICULTURE (90%); AGRICULTURE DEPARTMENTS (90%); ARTIFICIAL INTELLIGENCE (90%); RICE FARMING (78%)
Person: POPE FRANCIS I (72%)
Geographic: ROME, ITALY (90%); PHILIPPINES (79%); ITALY (58%)
Load-Date: March 1, 2021","Experts from FAO and the Philippines Department of Agriculture using drones to gather visual data on damaged rice crops.
The Pontifical Academy for Life, together with the first co-signatories of the Rome Call for Artificial Intelligence (AI) Ethics: Microsoft, IBM, and the Food and Agriculture Organization of the United Nations (FAO), today marked the first anniversary of the document which has been endorsed by Pope Francis and seeks a commitment towards developing AI technologies in ways that are transparent, inclusive, socially beneficial and accountable.
""Progress can make a better world possible if it goes together with the common good"". This was reiterated by Archbishop Vincenzo Paglia, President of the Pontifical Academy for Life, recalling that on February 28, 2020, Microsoft, IBM, FAO, the Minister for Technological Innovation of the Italian government, signed the Rome Call for AI Ethics, promoted by the Pontifical Academy for Life.
""After 12 months the 'family' of signatories has grown and we are working to make the document more and more known, in view of further accessions by strategic actors for an ethical approach to the themes of Artificial Intelligence,"" Archbishop Paglia noted. ""A channel of dialogue with monotheistic religions is open, in order to converge on a common vision of technology at the service of all humanity. The depth and acceleration of the transformations in the digital age raise global and constantly evolving issues. A year after the Call, the Pontifical Academy for Life is increasingly convinced and determined on the importance of placing itself at the service of each person in his/her entirety and of all people, without discrimination or exclusion. The complexity of the technological world requires a more articulated ethical reflection, to make our commitment truly incisive. We need a new alliance between research, science and ethics, because we stand at a crucial crossroads, in order to build a world where technology is actually used for the development of peoples. That is a request coming from faith and reason. Without equitable and widespread development there will be no justice, there will be no peace, there will be no universal brotherhood.""
The Rome Call for AI Ethics is a document created to support an ethical approach to Artificial Intelligence and promote a sense of shared responsibility among organizations, governments and institutions with the aim of guaranteeing a future in which digital innovation and technological progress are at the service of human ingenuity and creativity.
""By 2050, the world will have to feed about 10 billion people. This will only be possible with transformed agri-food systems that are inclusive, resilient and sustainable. Artificial Intelligence in Food and Agriculture plays a key role in this transformation and in achieving Food for All,"" said FAO Director-General QU Dongyu. ""At FAO, we use ethical AI in our work for Better Production, Better Nutrition, a Better Environment and a Better Life. All people have the right to benefit from an ethical AI and reap the digital dividend. FAO has been fully committed with the Rome Call for AI ethics,"" he added.
""A year ago, we joined Archbishop Vincenzo Paglia and the Pontifical Academy for Life to honor the six fundamental principles in the Rome Call for AI and Ethics and ensure that technology continues to serve humanity,"" said Microsoft President Brad Smith. ""As we recover from the COVID-19 pandemic, the Rome Call will be even more important as we think more broadly and ethically about the future of technology. The Rome Call helps put us on this path to promote a thoughtful, respectful and inclusive conversation about the intersection of artificial intelligence technology and society.""
""At IBM we believe that AI has the ability to transform and improve our lives and our society in many important ways,"" said Dario Gil, Senior Vice President and Director of IBM Research. ""For all of us to benefit from AI, it requires a commitment to actively develop, deploy, and use it responsibly in order to prevent adverse outcomes. This is why IBM was so proud to join the Pontifical Academy for Life as one of the initial signers of the Rome Call for an AI Ethics, supporting an ethical approach to Artificial Intelligence. Our work in support of AI Ethics permeates our entire company, which includes a centralized governance framework, risk assessment protocols, trustworthy AI development methodologies, education and training initiatives, research innovations, and open source toolkits to help others bolster their AI ethics efforts,"" Gil added.
The Rome Call for AI Ethics stresses that ""AI systems must be conceived, designed and implemented to serve and protect human beings and the environment in which they live,"" a concept that many participants reiterated one year ago.
(The Food and Agriculture Organization)
Classification
Language: ENGLISH
Publication-Type: Wire
Subject: ARTIFICIAL INTELLIGENCE ETHICS (91%); ETHICS (91%); AGRICULTURE DEPARTMENTS (90%); ANNIVERSARIES (90%); ARTIFICIAL INTELLIGENCE (90%); CATHOLIC POPES (90%); CHRISTIANS & CHRISTIANITY (90%); CLERGY & RELIGIOUS VOCATIONS (90%); RELIGION (90%); THIS DAY IN HISTORY (90%); UNITED NATIONS (78%); ASSOCIATIONS & ORGANIZATIONS (77%); EMERGING TECHNOLOGY (77%); DISCRIMINATION (75%); UNITED NATIONS INSTITUTIONS (73%); PRODUCT INNOVATION (68%)
Company:  MICROSOFT CORP (90%)
Ticker: MSFT (NASDAQ) (90%)
Industry: NAICS511210 SOFTWARE PUBLISHERS (90%); SIC7372 PREPACKAGED SOFTWARE (90%); ARTIFICIAL INTELLIGENCE ETHICS (91%); AGRICULTURE (90%); AGRICULTURE DEPARTMENTS (90%); ARTIFICIAL INTELLIGENCE (90%); RICE FARMING (78%)
Person: POPE FRANCIS I (72%)
Geographic: ROME, ITALY (90%); PHILIPPINES (79%); ITALY (58%)
Load-Date: March 1, 2021",neutral,0.7694018483161926,balanced/neutral,['discrimination'],"['justice', 'justice']","['governance', 'framework', 'must']",[],1,2,3,0
2021,Unknown Title,"Byline: Claire Stowe
Body
The McCourt Institute appointed Shéhérazade Semsar-de Boisséson (SFS '90) Nov. 10 as its first executive director.
Georgetown University's McCourt School of Public Policy joined with the Paris Institute of Political Studies (Sciences Po) in June 2021 to found the McCourt Institute, which was created to promote ethical technological innovation. The Institute builds on Georgetown's Tech & Society initiative, an initiative between all of Georgetown's nine schools focused on research, driving policy change and improving education around technology.
Kirk Zieser/The Hoya | Shéhérazade Semsar-de Boisséson (SFS '90) has been appointed the first executive director to lead the McCourt Institute, which was created in June 2021 to promote ethical technological innovation and governance.
Semsar's experience in technological development and infrastructure will allow her to make significant contributions in the field of technological ethics to the McCourt Institute, according to Frank McCourt (CAS '75), who funded the Institute's partnership with Sciences Po.
""We are thrilled to welcome Shéhérazade to the McCourt Institute and grateful for the expertise, enthusiasm, and leadership skills that she brings to her role as our first executive director,"" McCourt wrote in a statement. ""She understands that, to solve the urgent and systemic problems that today's tech infrastructure is fueling, we need interdisciplinary thinkers working hand-in-hand with technologists to infuse ethics into technology and creating a workable plan for good digital governance.""
Semsar previously served as the founding CEO of Politico Europe, and will now further the institute's goal of enhancing digital governance, online tools establishing authority and accountability in an organization, and technological ethics through research and engagement with policymakers and academics.
Ad:
Frank McCourt's March 2021 $100 million donation granted funding to scholarships, financial aid and research. Of these funds, $25 million were directly distributed to support the university's initiatives with the McCourt Institute. This marks Frank McCourt's second donation to Georgetown University; in 2013, Frank McCourt donated the same amount to found the McCourt school.
The Institute will work to improve ethics in technology and development and form a framework for the use of technology in government, according to Semsar.
""Despite technology's many benefits, it's clear that today's tech structure is driving negative outcomes; this is a global problem, not just a U.S. or European problem,"" Semsar wrote in the press release. ""I look forward to working with great minds across countries and disciplines to create a much-needed governance framework and help put us on this better path.""
During her work at Politico, Semsar helped launch the organization's annual Artificial Intelligence Innovation and Governance Summit, which gathers leaders from around the world to discuss AI standards. Semsar continues to serve on the Georgetown School of Foreign Service Advisory Board, and also served on Georgetown's Board of Directors from 2013 to 2019.
The partnership with Sciences Po was made possible by McCourt's $100 million donation to the McCourt Institute, according to McCourt School Dean Maria Cancian.
""One of the key elements of Frank McCourt's extraordinary second $100 Million investment in the McCourt School was support for our partnership with the McCourt Institute,"" Cancian wrote in an email to The Hoya.
Semsar's extensive background in technology and digital advancement and experience with leadership at Politico will allow her to think interdisciplinarily about the institute's work, according to Frank McCourt.
""Shéhérazade's deep international experience leading industry shifts and tackling big challenges makes her the perfect person to advance the goals of Project Liberty and spearhead the critical work of the institute,"" Frank McCourt wrote.
With Semsar's appointment, the McCourt School will continue to engage with the institute's projects and research, according to Cancian.
""The appointment of Shéhérazade Semsar-de-Boisséson as the inaugural executive director of the Institute is a great step forward,"" Cancian wrote. ""Semsar is a Georgetown alum and former board member, with a deep appreciation for public policy and a keen understanding of both US and international dynamics. We look forward to engaging with her in the Institute's important work.""
Classification
Language: ENGLISH
Publication-Type: Newspaper
Subject: APPOINTMENTS (93%); ETHICS (91%); EXECUTIVES (91%); RESEARCH INSTITUTES (90%); EMERGING TECHNOLOGY (89%); PUBLIC POLICY (89%); ASSOCIATIONS & ORGANIZATIONS (78%); CORPORATE GOVERNANCE (78%); EMPLOYMENT HISTORY (78%); POLITICAL SCIENCE (78%); SCHOLARSHIPS & GRANTS (78%); STUDENT EXPENSES & FINANCING (78%); PRODUCT INNOVATION (76%); STUDENT FINANCIAL AID (73%); ARTIFICIAL INTELLIGENCE (60%)
Organization: GEORGETOWN UNIVERSITY (93%)
Industry: ARTIFICIAL INTELLIGENCE (60%)
Geographic: DISTRICT OF COLUMBIA, USA (79%); EUROPE (92%); UNITED STATES (79%); Washington; DC
Load-Date: November 19, 2021","The McCourt Institute appointed Shéhérazade Semsar-de Boisséson (SFS '90) Nov. 10 as its first executive director.
Georgetown University's McCourt School of Public Policy joined with the Paris Institute of Political Studies (Sciences Po) in June 2021 to found the McCourt Institute, which was created to promote ethical technological innovation. The Institute builds on Georgetown's Tech & Society initiative, an initiative between all of Georgetown's nine schools focused on research, driving policy change and improving education around technology.
Kirk Zieser/The Hoya | Shéhérazade Semsar-de Boisséson (SFS '90) has been appointed the first executive director to lead the McCourt Institute, which was created in June 2021 to promote ethical technological innovation and governance.
Semsar's experience in technological development and infrastructure will allow her to make significant contributions in the field of technological ethics to the McCourt Institute, according to Frank McCourt (CAS '75), who funded the Institute's partnership with Sciences Po.
""We are thrilled to welcome Shéhérazade to the McCourt Institute and grateful for the expertise, enthusiasm, and leadership skills that she brings to her role as our first executive director,"" McCourt wrote in a statement. ""She understands that, to solve the urgent and systemic problems that today's tech infrastructure is fueling, we need interdisciplinary thinkers working hand-in-hand with technologists to infuse ethics into technology and creating a workable plan for good digital governance.""
Semsar previously served as the founding CEO of Politico Europe, and will now further the institute's goal of enhancing digital governance, online tools establishing authority and accountability in an organization, and technological ethics through research and engagement with policymakers and academics.
Ad:
Frank McCourt's March 2021 $100 million donation granted funding to scholarships, financial aid and research. Of these funds, $25 million were directly distributed to support the university's initiatives with the McCourt Institute. This marks Frank McCourt's second donation to Georgetown University; in 2013, Frank McCourt donated the same amount to found the McCourt school.
The Institute will work to improve ethics in technology and development and form a framework for the use of technology in government, according to Semsar.
""Despite technology's many benefits, it's clear that today's tech structure is driving negative outcomes; this is a global problem, not just a U.S. or European problem,"" Semsar wrote in the press release. ""I look forward to working with great minds across countries and disciplines to create a much-needed governance framework and help put us on this better path.""
During her work at Politico, Semsar helped launch the organization's annual Artificial Intelligence Innovation and Governance Summit, which gathers leaders from around the world to discuss AI standards. Semsar continues to serve on the Georgetown School of Foreign Service Advisory Board, and also served on Georgetown's Board of Directors from 2013 to 2019.
The partnership with Sciences Po was made possible by McCourt's $100 million donation to the McCourt Institute, according to McCourt School Dean Maria Cancian.
""One of the key elements of Frank McCourt's extraordinary second $100 Million investment in the McCourt School was support for our partnership with the McCourt Institute,"" Cancian wrote in an email to The Hoya.
Semsar's extensive background in technology and digital advancement and experience with leadership at Politico will allow her to think interdisciplinarily about the institute's work, according to Frank McCourt.
""Shéhérazade's deep international experience leading industry shifts and tackling big challenges makes her the perfect person to advance the goals of Project Liberty and spearhead the critical work of the institute,"" Frank McCourt wrote.
With Semsar's appointment, the McCourt School will continue to engage with the institute's projects and research, according to Cancian.
""The appointment of Shéhérazade Semsar-de-Boisséson as the inaugural executive director of the Institute is a great step forward,"" Cancian wrote. ""Semsar is a Georgetown alum and former board member, with a deep appreciation for public policy and a keen understanding of both US and international dynamics. We look forward to engaging with her in the Institute's important work.""
Classification
Language: ENGLISH
Publication-Type: Newspaper
Subject: APPOINTMENTS (93%); ETHICS (91%); EXECUTIVES (91%); RESEARCH INSTITUTES (90%); EMERGING TECHNOLOGY (89%); PUBLIC POLICY (89%); ASSOCIATIONS & ORGANIZATIONS (78%); CORPORATE GOVERNANCE (78%); EMPLOYMENT HISTORY (78%); POLITICAL SCIENCE (78%); SCHOLARSHIPS & GRANTS (78%); STUDENT EXPENSES & FINANCING (78%); PRODUCT INNOVATION (76%); STUDENT FINANCIAL AID (73%); ARTIFICIAL INTELLIGENCE (60%)
Organization: GEORGETOWN UNIVERSITY (93%)
Industry: ARTIFICIAL INTELLIGENCE (60%)
Geographic: DISTRICT OF COLUMBIA, USA (79%); EUROPE (92%); UNITED STATES (79%); Washington; DC
Load-Date: November 19, 2021",neutral,0.7873988747596741,balanced/neutral,['accountability'],[],"['policy', 'governance', 'standards', 'framework']",[],1,0,4,0
2021,Unknown Title,"Dateline: PISCATAWAY, N.J. 
Body
IEEE, the world's largest technical professional organization dedicated to advancing technology for humanity, and the IEEE Standards Association (IEEE SA) today announced that the City of Vienna has become the first city worldwide to earn the IEEE CertifiAIEd AI Ethics (AIE) Certification Mark to advance the city's Digital Humanism strategy.
""With artificial intelligence, we can make many tasks in administration more efficient and faster. As the City of Vienna, we are taking on a pioneering role here,"" says Peter Hanke, City Councillor for Economic Affairs. ""Particularly in direct contact with people and especially in dealing with data, caution is required. People are at the center of our considerations.""
The IEEE CertifAIEd mark recognizes that a product, service, or system has been verified to meet relevant ethical criteria, contributing towards a greater level of confidence and demonstrating a proactive approach to building public trust in AI systems. There are currently four sets of criteria available for IEEE CertifiAIEd certification:
Transparency criteria relate to values embedded in a system design, and the openness and disclosure of choices made for development and operation.
Accountability criteria recognize that the system/service autonomy and learning capacities are the results of algorithms and computational processes designed by humans and organizations that remain responsible for their outcomes.
Algorithmic bias criteria relate to the prevention of systematic errors and repeatable undesirable behaviors that create unfair outcomes.
Privacy criteria are aimed at respecting the private sphere of life and public identity of an individual, group, or community, upholding dignity.
""IEEE has laid the groundwork for AI Ethics based on principles and standards created by hundreds of our volunteers over the past five years, which are already having a global impact,"" said Konstantinos Karachalios, Managing Director of IEEE SA. ""IEEE CertifAIEd represents our continued evolution of the AI Ethics ecosystem by establishing a program to inspire trust and a means towards responsible implementation of AI systems that demonstrates an organization's commitment to upholding human values, dignity, and well-being, and to respecting, protecting and preserving fundamental human rights. We are honored to work with the City of Vienna to support their Digital Humanism platform and to provide the mark for responsible innovation required in today's world to inspire certified trust for their AI Systems.""
IEEE's CertifAIEd program aims to enhance confidence in public and private enterprises that realize the benefits of AI ethics by earning certification in the absence of or as a complement to broadly accepted and enforced regulations for AI. As proposed legislation in the EU would require conformity certification for any AI based systems to mitigate any unintended risks, the City of Vienna is addressing these risks and demonstrating the benefits of responsible government innovation by earning the IEEE CertifAIEd mark. In doing so, it sets a precedent for other cities and entities to achieve their aims in a responsible and human-centric manner.
""Data security and data protection must be at the forefront when using AI from the very beginning. That's why we relied on international expertise during the development of the software and had our program ethically certified,"" said Deputy Director General, Peter Weinelt.
Cities, entities, and individuals interested in learning more about the criteria or getting involved in the program can visit the IEEE CertifAIEd website .
To learn more about IEEE SA or about any of its many market-driven initiatives, visit us on Facebook , follow us on Twitter , connect with us on LinkedIn , or on the Beyond Standards Blog .
About the IEEE Standards Association
IEEE Standards Association (IEEE SA) is a collaborative organization where innovators raise the world's standards for technology. IEEE SA provides a globally open, consensus-building environment and platform that empowers people to work together in the development of leading-edge, market-relevant technology standards, and industry solutions shaping a better, safer and sustainable world. For more information, visit https://standards.ieee.org .
About IEEE
IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. Through its highly cited publications, conferences, technology standards, and professional and educational activities, IEEE is the trusted voice in a wide variety of areas ranging from aerospace systems, computers, and telecommunications to biomedical engineering, electric power, and consumer electronics. Learn more at https://www.ieee.org .
View source version on businesswire.com: https://www.businesswire.com/news/home/20211115005200/en/
CONTACT: Tania Olabi-Colon, Director, Brand Marketing & CommunicationsOlivia Wang, Marketing & Communications Manager
standards-pr@ieee.org
http://www.businesswire.com
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ARTIFICIAL INTELLIGENCE ETHICS (92%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); ASSOCIATIONS & ORGANIZATIONS (90%); PRESS RELEASES (90%); STANDARDS & MEASUREMENTS (79%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (77%); HUMAN RIGHTS (73%); LEGISLATION (50%); Award (%); Product/Service (%)
Company: NJ-IEEE
Organization: INSTITUTE OF ELECTRICAL & ELECTRONICS ENGINEERS (94%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (77%); Data Management (%); Security (%); Technology (%); Software (%); Networks (%); Hardware (%)
Geographic: VIENNA, AUSTRIA (92%); NEW JERSEY, USA (91%); EUROPE (79%); NORTH AMERICA (79%); UNITED STATES (79%); EUROPEAN UNION MEMBER STATES (73%); New Jersey; Austria; United States; Europe; North America
Load-Date: November 15, 2021","IEEE, the world's largest technical professional organization dedicated to advancing technology for humanity, and the IEEE Standards Association (IEEE SA) today announced that the City of Vienna has become the first city worldwide to earn the IEEE CertifiAIEd AI Ethics (AIE) Certification Mark to advance the city's Digital Humanism strategy.
""With artificial intelligence, we can make many tasks in administration more efficient and faster. As the City of Vienna, we are taking on a pioneering role here,"" says Peter Hanke, City Councillor for Economic Affairs. ""Particularly in direct contact with people and especially in dealing with data, caution is required. People are at the center of our considerations.""
The IEEE CertifAIEd mark recognizes that a product, service, or system has been verified to meet relevant ethical criteria, contributing towards a greater level of confidence and demonstrating a proactive approach to building public trust in AI systems. There are currently four sets of criteria available for IEEE CertifiAIEd certification:
Transparency criteria relate to values embedded in a system design, and the openness and disclosure of choices made for development and operation.
Accountability criteria recognize that the system/service autonomy and learning capacities are the results of algorithms and computational processes designed by humans and organizations that remain responsible for their outcomes.
Algorithmic bias criteria relate to the prevention of systematic errors and repeatable undesirable behaviors that create unfair outcomes.
Privacy criteria are aimed at respecting the private sphere of life and public identity of an individual, group, or community, upholding dignity.
""IEEE has laid the groundwork for AI Ethics based on principles and standards created by hundreds of our volunteers over the past five years, which are already having a global impact,"" said Konstantinos Karachalios, Managing Director of IEEE SA. ""IEEE CertifAIEd represents our continued evolution of the AI Ethics ecosystem by establishing a program to inspire trust and a means towards responsible implementation of AI systems that demonstrates an organization's commitment to upholding human values, dignity, and well-being, and to respecting, protecting and preserving fundamental human rights. We are honored to work with the City of Vienna to support their Digital Humanism platform and to provide the mark for responsible innovation required in today's world to inspire certified trust for their AI Systems.""
IEEE's CertifAIEd program aims to enhance confidence in public and private enterprises that realize the benefits of AI ethics by earning certification in the absence of or as a complement to broadly accepted and enforced regulations for AI. As proposed legislation in the EU would require conformity certification for any AI based systems to mitigate any unintended risks, the City of Vienna is addressing these risks and demonstrating the benefits of responsible government innovation by earning the IEEE CertifAIEd mark. In doing so, it sets a precedent for other cities and entities to achieve their aims in a responsible and human-centric manner.
""Data security and data protection must be at the forefront when using AI from the very beginning. That's why we relied on international expertise during the development of the software and had our program ethically certified,"" said Deputy Director General, Peter Weinelt.
Cities, entities, and individuals interested in learning more about the criteria or getting involved in the program can visit the IEEE CertifAIEd website .
To learn more about IEEE SA or about any of its many market-driven initiatives, visit us on Facebook , follow us on Twitter , connect with us on LinkedIn , or on the Beyond Standards Blog .
About the IEEE Standards Association
IEEE Standards Association (IEEE SA) is a collaborative organization where innovators raise the world's standards for technology. IEEE SA provides a globally open, consensus-building environment and platform that empowers people to work together in the development of leading-edge, market-relevant technology standards, and industry solutions shaping a better, safer and sustainable world. For more information, visit https://standards.ieee.org .
About IEEE
IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. Through its highly cited publications, conferences, technology standards, and professional and educational activities, IEEE is the trusted voice in a wide variety of areas ranging from aerospace systems, computers, and telecommunications to biomedical engineering, electric power, and consumer electronics. Learn more at https://www.ieee.org .
View source version on businesswire.com: https://www.businesswire.com/news/home/20211115005200/en/
CONTACT: Tania Olabi-Colon, Director, Brand Marketing & CommunicationsOlivia Wang, Marketing & Communications Manager
standards-pr@ieee.org
http://www.businesswire.com
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ARTIFICIAL INTELLIGENCE ETHICS (92%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); ASSOCIATIONS & ORGANIZATIONS (90%); PRESS RELEASES (90%); STANDARDS & MEASUREMENTS (79%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (77%); HUMAN RIGHTS (73%); LEGISLATION (50%); Award (%); Product/Service (%)
Company: NJ-IEEE
Organization: INSTITUTE OF ELECTRICAL & ELECTRONICS ENGINEERS (94%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (77%); Data Management (%); Security (%); Technology (%); Software (%); Networks (%); Hardware (%)
Geographic: VIENNA, AUSTRIA (92%); NEW JERSEY, USA (91%); EUROPE (79%); NORTH AMERICA (79%); UNITED STATES (79%); EUROPEAN UNION MEMBER STATES (73%); New Jersey; Austria; United States; Europe; North America
Load-Date: November 15, 2021",positive,0.7116027474403381,balanced/neutral,"['privacy', 'bias', 'transparency', 'accountability', 'security', 'human rights', 'autonomy']","['autonomy', 'dignity']","['regulation', 'policy', 'standards', 'legislation', 'certification', 'must']",[],7,2,6,0
2021,Unknown Title,"Dateline: NEW YORK, March 15, 2021 
Body
PR Newswire
 TheDeloitte AI Institute, a Center that focuses on applied Artificial Intelligence (AI) research, eminence and innovation across industries, today announced it is collaborating withChatterbox Labsto develop Model Insights technology for Trustworthy AI, a Deloitte-branded solution that helps organizations address AI ethics by monitoring, updating and validating their AI models.
Deloitte's recent ""State of AI in the Enterprise"" third edition study of enterprise AI adopters, found that 95% of respondents expressed concerns around ethical risks for their AI initiatives. To help companies proactively address AI ethics, the Deloitte AI Institute developed itsTrustworthy AI™ framework, which provides a guide for organizations on how to apply AI responsibly within their businesses and manage common risks and challenges related to AI ethics. Now, through its exclusive collaboration with Chatterbox Labs, Deloitte is operationalizing the Trustworthy AI framework with a new technology solution, Model Insights for Trustworthy AI. By continuously monitoring enterprise AI models, Deloitte's Model Insights delivers immediate insights that can uncover biases and vulnerabilities and allow organizations to validate that their AI models are ethical, trustworthy and fair.
""Rapid developments in AI have unlocked incredible opportunities for organizations globally. At the same time, more work is needed to ensure the ethical use of AI technology,"" saidBeena Ammanath, Executive Director of the Deloitte AI Institute, Deloitte Consulting LLP. ""Our Trustworthy AI framework guides organizations in developing appropriate safeguards and using AI in an ethical manner. Through our collaboration with Chatterbox Labs, our Model Insights technology solution can help our clients put the Trustworthy AI framework into action and mitigate the ethical risks associated with AI.""
Deloitte's Model Insights solution is built on Artificial Intelligence Model Insights (AIMI) from Chatterbox Labs, a patented platform that delivers data and insights into enterprise AI models, enabling organizations to validate and understand their AI initiatives and help ensure they are operating fairly and ethically. Chatterbox Labs was named a 2020 Cool Vendor in Enterprise AI Governance by Gartner. Model Insights for Trustworthy AI will be available to Deloitte clients starting in April 2021.
""As organizations accelerate the adoption of AI technologies, it's critical to have controls and procedures in place to address ethical concerns,"" said Danny Coleman, CEO of Chatterbox Labs. ""Our collaboration with the Deloitte AI Institute will provide Deloitte clients with deep insights into how their AI models are operating, so that they can mitigate ethical risks and validate their systems are trustworthy and fair.""
Organizations across a wide range of industries, including financial services, government and public sector, and life sciences and health care, are accelerating their adoption of AI. Deloitte can provide these organizations with deep AI experience, along with a framework and technology to help ensure their AI initiatives are operating in an ethical and trustworthy manner.
The Deloitte AI Institute's mission is to support the positive growth and development of AI by connecting across all the dimensions of the AI ecosystem to help advance human-machine collaboration in theAge of With™, a world where humans work side-by-side with machines.
To learn more about the Deloitte AI Institute's work, collaborations and research, please visitour website.
About DeloitteDeloitte provides industry-leading audit, consulting, tax and advisory services to many of the world's most admired brands, including nearly 90% of the Fortune 500® and more than 7,000 private companies. Our peoplecome togetherfor the greater good and work across the industry sectors that drive and shape today's marketplace — delivering measurable and lasting results that help reinforce public trust in our capital markets, inspire clients to see challenges as opportunities to transform and thrive, and help lead the way toward a stronger economy and a healthier society. Deloitte is proud to be part of the largest global professional services network serving our clients in the markets that are most important to them. Building on more than 175 years of service, our network of member firms spans more than 150 countries and territories. Learn how Deloitte's more than 330,000 people worldwide connect for impact athttp://www.deloitte.com.
Deloitte refers to one or more of Deloitte Touche Tohmatsu Limited, a UK private company limited by guarantee (""DTTL""), its network of member firms, and their related entities. DTTL and each of its member firms are legally separate and independent entities. DTTL (also referred to as ""Deloitte Global"") does not provide services to clients. In the United States, Deloitte refers to one or more of the US member firms of DTTL, their related entities that operate using the ""Deloitte"" name in the United States and their respective affiliates. Certain services may not be available to attest clients under the rules and regulations of public accounting. Please seehttp://www.deloitte.com/about to learn more about our global network of member firms.
 View original content to download multimedia:http://www.prnewswire.com/news-releases/deloitte-ai-institute-teams-with-chatterbox-labs-to-ensure-ethical-application-of-ai-301246959.html
SOURCE Deloitte Consulting LLP
CONTACT: Melissa Neumann, Public Relations, Deloitte Services LP, +1 408 666 1946, meneumann@deloitte.com 
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ARTIFICIAL INTELLIGENCE ETHICS (93%); ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); ASSOCIATIONS & ORGANIZATIONS (90%); PRESS RELEASES (90%); EXECUTIVES (74%); Deloitte-Chatterbox (%); SVY Surveys, polls & research studies (%)
Company:  DELOITTE LLP (93%); Deloitte Consulting LLP
Industry: NAICS541211 OFFICES OF CERTIFIED PUBLIC ACCOUNTANTS (93%); SIC8721 ACCOUNTING, AUDITING, & BOOKKEEPING SERVICES (93%); ARTIFICIAL INTELLIGENCE ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); BANKING & FINANCE (73%); FIN Banking; Financial Services (%); CPR Computer; Electronics Products (%); FNT Financial Technology (%)
Geographic: New York
Load-Date: March 15, 2021","PR Newswire
 TheDeloitte AI Institute, a Center that focuses on applied Artificial Intelligence (AI) research, eminence and innovation across industries, today announced it is collaborating withChatterbox Labsto develop Model Insights technology for Trustworthy AI, a Deloitte-branded solution that helps organizations address AI ethics by monitoring, updating and validating their AI models.
Deloitte's recent ""State of AI in the Enterprise"" third edition study of enterprise AI adopters, found that 95% of respondents expressed concerns around ethical risks for their AI initiatives. To help companies proactively address AI ethics, the Deloitte AI Institute developed itsTrustworthy AI™ framework, which provides a guide for organizations on how to apply AI responsibly within their businesses and manage common risks and challenges related to AI ethics. Now, through its exclusive collaboration with Chatterbox Labs, Deloitte is operationalizing the Trustworthy AI framework with a new technology solution, Model Insights for Trustworthy AI. By continuously monitoring enterprise AI models, Deloitte's Model Insights delivers immediate insights that can uncover biases and vulnerabilities and allow organizations to validate that their AI models are ethical, trustworthy and fair.
""Rapid developments in AI have unlocked incredible opportunities for organizations globally. At the same time, more work is needed to ensure the ethical use of AI technology,"" saidBeena Ammanath, Executive Director of the Deloitte AI Institute, Deloitte Consulting LLP. ""Our Trustworthy AI framework guides organizations in developing appropriate safeguards and using AI in an ethical manner. Through our collaboration with Chatterbox Labs, our Model Insights technology solution can help our clients put the Trustworthy AI framework into action and mitigate the ethical risks associated with AI.""
Deloitte's Model Insights solution is built on Artificial Intelligence Model Insights (AIMI) from Chatterbox Labs, a patented platform that delivers data and insights into enterprise AI models, enabling organizations to validate and understand their AI initiatives and help ensure they are operating fairly and ethically. Chatterbox Labs was named a 2020 Cool Vendor in Enterprise AI Governance by Gartner. Model Insights for Trustworthy AI will be available to Deloitte clients starting in April 2021.
""As organizations accelerate the adoption of AI technologies, it's critical to have controls and procedures in place to address ethical concerns,"" said Danny Coleman, CEO of Chatterbox Labs. ""Our collaboration with the Deloitte AI Institute will provide Deloitte clients with deep insights into how their AI models are operating, so that they can mitigate ethical risks and validate their systems are trustworthy and fair.""
Organizations across a wide range of industries, including financial services, government and public sector, and life sciences and health care, are accelerating their adoption of AI. Deloitte can provide these organizations with deep AI experience, along with a framework and technology to help ensure their AI initiatives are operating in an ethical and trustworthy manner.
The Deloitte AI Institute's mission is to support the positive growth and development of AI by connecting across all the dimensions of the AI ecosystem to help advance human-machine collaboration in theAge of With™, a world where humans work side-by-side with machines.
To learn more about the Deloitte AI Institute's work, collaborations and research, please visitour website.
About DeloitteDeloitte provides industry-leading audit, consulting, tax and advisory services to many of the world's most admired brands, including nearly 90% of the Fortune 500® and more than 7,000 private companies. Our peoplecome togetherfor the greater good and work across the industry sectors that drive and shape today's marketplace — delivering measurable and lasting results that help reinforce public trust in our capital markets, inspire clients to see challenges as opportunities to transform and thrive, and help lead the way toward a stronger economy and a healthier society. Deloitte is proud to be part of the largest global professional services network serving our clients in the markets that are most important to them. Building on more than 175 years of service, our network of member firms spans more than 150 countries and territories. Learn how Deloitte's more than 330,000 people worldwide connect for impact athttp://www.deloitte.com.
Deloitte refers to one or more of Deloitte Touche Tohmatsu Limited, a UK private company limited by guarantee (""DTTL""), its network of member firms, and their related entities. DTTL and each of its member firms are legally separate and independent entities. DTTL (also referred to as ""Deloitte Global"") does not provide services to clients. In the United States, Deloitte refers to one or more of the US member firms of DTTL, their related entities that operate using the ""Deloitte"" name in the United States and their respective affiliates. Certain services may not be available to attest clients under the rules and regulations of public accounting. Please seehttp://www.deloitte.com/about to learn more about our global network of member firms.
 View original content to download multimedia:http://www.prnewswire.com/news-releases/deloitte-ai-institute-teams-with-chatterbox-labs-to-ensure-ethical-application-of-ai-301246959.html
SOURCE Deloitte Consulting LLP
CONTACT: Melissa Neumann, Public Relations, Deloitte Services LP, +1 408 666 1946, meneumann@deloitte.com 
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ARTIFICIAL INTELLIGENCE ETHICS (93%); ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); ASSOCIATIONS & ORGANIZATIONS (90%); PRESS RELEASES (90%); EXECUTIVES (74%); Deloitte-Chatterbox (%); SVY Surveys, polls & research studies (%)
Company:  DELOITTE LLP (93%); Deloitte Consulting LLP
Industry: NAICS541211 OFFICES OF CERTIFIED PUBLIC ACCOUNTANTS (93%); SIC8721 ACCOUNTING, AUDITING, & BOOKKEEPING SERVICES (93%); ARTIFICIAL INTELLIGENCE ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); BANKING & FINANCE (73%); FIN Banking; Financial Services (%); CPR Computer; Electronics Products (%); FNT Financial Technology (%)
Geographic: New York
Load-Date: March 15, 2021",neutral,0.679011344909668,balanced/neutral,[],[],"['governance', 'framework', 'audit']",[],0,0,3,0
2021,Unknown Title,"Body
Independent dialogue is one of the first on location data
85 members of the public gave their thoughts about location data use across four workshops
Findings outlined will inform government guidance on location data ethics to be published next year
 The Geospatial Commission today publishes the findings of an independent public dialogue on location data ethics . The project was launched in March and co-funded by the Geospatial Commission and UK Research and Innovation’s Sciencewise programme.
The dialogue is one of the UK’s first on location data and was delivered by public engagement specialists Traverse and researchers from the Ada Lovelace Institute. Today’s report provides evidence on public perceptions about location data use, offering valuable insights into what citizens believe are the key benefits and concerns.
The report will be launched at a virtual event today at 3pm hosted by The Alan Turing Institute, the national institute for data science and artificial intelligence. Cosmina Dorobantu, Deputy Programme Director for Public Policy at The Alan Turing Institute, will deliver the keynote speech at the event which will also feature a presentation by Traverse and Ada Lovelace, and a panel discussion with leading data experts.
Minister of State for the Cabinet Office, Lord True CBE said:
The evolution of the UK’s geospatial ecosystem presents great opportunities to realise significant economic, social and environmental value. The public dialogue report underlines that as we seek to capitalise on these benefits we will always aim to do so in a way that carries the confidence of the public and addresses any concerns.
Independent Commissioner of the Geospatial Commission and Interim Chair of the Centre for Data Ethics and Innovation, Edwina Dunn OBE said:
This independent report on public attitudes about location data is one of the first of its kind and I look forward to exploring how it can help inform the Geospatial Commission’s work on location data ethics. The findings will also play an important role in supporting the government’s vital work to enable the trustworthy use of data and AI.
Tom Saunders, Head of Public Engagement, UKRI, said:
This timely independent dialogue will help inform the government’s geospatial strategy, allowing the UK to unlock the power of location data for economic, social and environmental benefit while balancing individuals’ concern over important ethical and privacy considerations. UKRI’s Sciencewise programme helps policy-makers understand what the public really thinks, and ensures that experts, government and the public can design a better future together that works for everyone.
Location data powers our everyday lives, telling us about travel disruption, tracking deliveries, and helping us find the place we need to be. The widespread use of new technologies means that data about our lives, including location data, is available in increasing frequency, detail and accuracy, driving innovation and better services. So that we can continue to benefit from widespread use of location data, it is important that data is used in a way that mitigates concerns and retains public confidence.
This public dialogue opened a conversation with 85 members of the public from all four nations to gather evidence on public perceptions about location data use. The dialogue was supported by an independent and expert Oversight Group, which provided expert support and quality assurance from a diversity of perspectives.
The findings outlined in this report will influence the guidance on location data ethics that the Geospatial Commission intends to publish next year and help deliver the UK Geospatial Strategy .
Members of the Oversight Group
John Pullinger (Chair) and previously the UK’s National Statistician
Andy Gregory, Home Office
Ben Lyons, Centre for Data Ethics and Innovation
Charles Kennelly, Esri
Chris Wroe, Telefónica UK
David Leslie, The Alan Turing Institute
Ellis Parry, Information Commissioner’s Office (Later Matthew Rice)
Jagdev Singh Virdee, Independent Consultant
Jeni Tennison, Open Data Institute (Later Lisa Allen)
Josh Berle, Mastercard
Marcus Grazette, Privitar
Mick Ridley, Global
Phil Earl, Department for Digital, Culture, Media and Sport (DCMS)
Philipa Sharma, Department of Business, Energy and Industrial Strategy
Professor Shannon Vallor, Edinburgh Futures Institute (EFI), University of Edinburgh
Professor Yves-Alexandre de Montjoye, Imperial College London
Renate Samson, Which?
Simon Whitworth, UK Statistics Authority
Sue Bateman, Government Digital Service
Toby Wicks, UNICEF
Editor’s note:
The Geospatial Commission is an expert committee that sets the UK’s geospatial strategy and promotes the best use of geospatial data. Providing guidance on how to unlock value from sensitive location data while mitigating security, ethical and privacy risks is a key commitment of the UK Geospatial Strategy.
For further information, please contact us at geospatialcommission@cabinetoffice.gov.uk 
Share this page
 Share on Facebook 
 Share on Twitter 
Published 16 December 2021
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (90%); PUBLIC POLICY (89%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (89%); EMERGING TECHNOLOGY (76%); RESEARCH INSTITUTES (76%); ARTIFICIAL INTELLIGENCE (73%); GOVERNMENT ADVISORS & MINISTERS (73%); LICENSES & PERMITS (73%); ENVIRONMENT & NATURAL RESOURCES (70%)
Industry: ARTIFICIAL INTELLIGENCE (73%)
Geographic: UNITED KINGDOM (93%)
Load-Date: December 16, 2021","Independent dialogue is one of the first on location data
85 members of the public gave their thoughts about location data use across four workshops
Findings outlined will inform government guidance on location data ethics to be published next year
 The Geospatial Commission today publishes the findings of an independent public dialogue on location data ethics . The project was launched in March and co-funded by the Geospatial Commission and UK Research and Innovation’s Sciencewise programme.
The dialogue is one of the UK’s first on location data and was delivered by public engagement specialists Traverse and researchers from the Ada Lovelace Institute. Today’s report provides evidence on public perceptions about location data use, offering valuable insights into what citizens believe are the key benefits and concerns.
The report will be launched at a virtual event today at 3pm hosted by The Alan Turing Institute, the national institute for data science and artificial intelligence. Cosmina Dorobantu, Deputy Programme Director for Public Policy at The Alan Turing Institute, will deliver the keynote speech at the event which will also feature a presentation by Traverse and Ada Lovelace, and a panel discussion with leading data experts.
Minister of State for the Cabinet Office, Lord True CBE said:
The evolution of the UK’s geospatial ecosystem presents great opportunities to realise significant economic, social and environmental value. The public dialogue report underlines that as we seek to capitalise on these benefits we will always aim to do so in a way that carries the confidence of the public and addresses any concerns.
Independent Commissioner of the Geospatial Commission and Interim Chair of the Centre for Data Ethics and Innovation, Edwina Dunn OBE said:
This independent report on public attitudes about location data is one of the first of its kind and I look forward to exploring how it can help inform the Geospatial Commission’s work on location data ethics. The findings will also play an important role in supporting the government’s vital work to enable the trustworthy use of data and AI.
Tom Saunders, Head of Public Engagement, UKRI, said:
This timely independent dialogue will help inform the government’s geospatial strategy, allowing the UK to unlock the power of location data for economic, social and environmental benefit while balancing individuals’ concern over important ethical and privacy considerations. UKRI’s Sciencewise programme helps policy-makers understand what the public really thinks, and ensures that experts, government and the public can design a better future together that works for everyone.
Location data powers our everyday lives, telling us about travel disruption, tracking deliveries, and helping us find the place we need to be. The widespread use of new technologies means that data about our lives, including location data, is available in increasing frequency, detail and accuracy, driving innovation and better services. So that we can continue to benefit from widespread use of location data, it is important that data is used in a way that mitigates concerns and retains public confidence.
This public dialogue opened a conversation with 85 members of the public from all four nations to gather evidence on public perceptions about location data use. The dialogue was supported by an independent and expert Oversight Group, which provided expert support and quality assurance from a diversity of perspectives.
The findings outlined in this report will influence the guidance on location data ethics that the Geospatial Commission intends to publish next year and help deliver the UK Geospatial Strategy .
Members of the Oversight Group
John Pullinger (Chair) and previously the UK’s National Statistician
Andy Gregory, Home Office
Ben Lyons, Centre for Data Ethics and Innovation
Charles Kennelly, Esri
Chris Wroe, Telefónica UK
David Leslie, The Alan Turing Institute
Ellis Parry, Information Commissioner’s Office (Later Matthew Rice)
Jagdev Singh Virdee, Independent Consultant
Jeni Tennison, Open Data Institute (Later Lisa Allen)
Josh Berle, Mastercard
Marcus Grazette, Privitar
Mick Ridley, Global
Phil Earl, Department for Digital, Culture, Media and Sport (DCMS)
Philipa Sharma, Department of Business, Energy and Industrial Strategy
Professor Shannon Vallor, Edinburgh Futures Institute (EFI), University of Edinburgh
Professor Yves-Alexandre de Montjoye, Imperial College London
Renate Samson, Which?
Simon Whitworth, UK Statistics Authority
Sue Bateman, Government Digital Service
Toby Wicks, UNICEF
Editor’s note:
The Geospatial Commission is an expert committee that sets the UK’s geospatial strategy and promotes the best use of geospatial data. Providing guidance on how to unlock value from sensitive location data while mitigating security, ethical and privacy risks is a key commitment of the UK Geospatial Strategy.
For further information, please contact us at geospatialcommission@cabinetoffice.gov.uk 
Share this page
 Share on Facebook 
 Share on Twitter 
Published 16 December 2021
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (90%); PUBLIC POLICY (89%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (89%); EMERGING TECHNOLOGY (76%); RESEARCH INSTITUTES (76%); ARTIFICIAL INTELLIGENCE (73%); GOVERNMENT ADVISORS & MINISTERS (73%); LICENSES & PERMITS (73%); ENVIRONMENT & NATURAL RESOURCES (70%)
Industry: ARTIFICIAL INTELLIGENCE (73%)
Geographic: UNITED KINGDOM (93%)
Load-Date: December 16, 2021",neutral,0.7849255204200745,balanced/neutral,"['privacy', 'security']",[],"['policy', 'oversight', 'need to']",[],2,0,3,0
2021,Unknown Title,"Byline: exchange4media Staff
Body
Unilever, in a post, talks about its General Counsel for Global Marketing and Media Jamie Barnard creating a guide for brands on data ethics.
FMCG major Unilever has posted an article about how the company is working with the marketing industry to ensure that data is treated responsibly and ethically.
The article talks about an industry-wide code for marketers that Unilever's General Counsel for Global Marketing and Media Jamie Barnard, who is also the Chair of the Data Ethics Board at the World Federation of Advertisers (WFA), has created. As per the article it is the world's first guide for brands on data ethics. The guide is based on four principles: Respect, Fairness, Accountability and Transparency.
Here's the full article:
According to SAP research, when the benefits and value of collecting data are made clear to consumers, more than 70% of us are willing to share our data with brands. But if that trust is broken, if brands are found to have collected data without our knowledge, 79% will never use that brand again.
Over the past few years, high-profile cases such as that of Cambridge Analytica have led regulators to investigate big tech firms and their relationship with consumer data. And we've seen laws passed that mean organisations that collect data for business purposes need to comply with strict rules on privacy - laws that include Europe's General Data Protection Regulation and the Californian Consumer Privacy Act.
For Jamie Barnard, compliance with regulation is not enough. ""Compliance is a baseline, protecting people's fundamental human rights on the one hand and shielding companies from the sharp end of the law on the other,"" he told Google's Vice-President of the Global Client & Agency Solutions, Pedro Pina in an interview for Think with Google.
""But it's of limited value in the court of public opinion: if people think a company's data practices are unethical, then a demonstration of legal compliance will not protect their reputation. This is why brands have to set and follow a code of ethics,"" he says.
It's time for courage, not just compliance
Companies are waking up to the idea that they have a moral responsibility as well as a legal one, that they have a responsibility to respect people's rights and expectations.
As Chair of the Data Ethics Board at the World Federation of Advertisers (WFA), Jamie set about creating the world's first guide for brands on data ethics - an industry-wide code for marketers that looks to ""design a digital future that enhances people's lives and protects them in equal measure"". The guide is based on four key principles.
Respect
When you download a pizza delivery or ride-hailing app and agree to its terms, the app learns a lot about you: the fact that you have the latest iPhone, for example, who you bank with, your network of friends and family, where you live and possibly where you work too.
With every interaction, the algorithm gets to know you a little better; it won't be long before it learns that you like to go out or eat when you get back. When you hail a cab, it knows it's 2:00 am. With access to other data, it could work out that you're in a dangerous part of the city, far from home and in the rain.
It is the company's approach to data ethics that will determine how it chooses to use this information. On the one hand, if the company's priority is safety, it could use this data to protect you (making sure you are picked up first). On the other hand, it could raise its charges instead, since the algorithm knows that, in situations like these, you are statistically more likely to accept them.
With this regard, the guide's first principle is that data usage should respect the people behind the data. It urges companies to strive to understand the interests of all parties and use consumer data to improve people's lives.
Respect
Data has equal capacity for good and evil. It can be used to promote inclusivity, create diversity and eliminate bias but it can also be used to exclude, divide and stigmatise. Segregating people into custom audiences for precision and performance marketing, for example, can avoid showing hamburger ads to vegans or casino games to children.
However, it can be unethical too, such as a landlord restricting certain ethnic groups from seeing its ads, or an employer limiting job opportunities to a male audience.
It's also a double-edged sword - blocking ad targeting by sexual orientation stops people from excluding the LGBT+ community, but it also stops people offering their services directly to the LGBT+ community.
Fairness should see data usage aim to be inclusive, acknowledge diversity and eliminate bias rather than dividing groups. The second of the guide's four principles encourages brands to examine their data sets, mindsets and governance and see avoiding harmful discrimination as a shared responsibility between advertisers, platforms and publishers.
Respect
People are increasingly demanding to be shown how their data is being used and how it is being looked after. They want to know that their personal data is in safe hands, and that organisations have put in place mechanisms to protect their information.
The Consumer Goods Forum Futerra study describes Gen Z as the 'honest generation' who don't expect brands to be perfect but expect them to be truthful. Expectations like this are driving greater demand for openness as companies are held to account not just for their use of data, but for their suppliers' and partners' use too.
""We should think of ethics in the same way we think about sportsmanship. A good sport has scruples - they will do the right thing even if it requires sacrifice. And building trust is also a team sport,"" Jamie says. ""We are all accountable for collecting and using data in a safe, ethical, and transparent manner,"" he told the Think with Google newsletter. ""Embedding data ethics across the industry requires commitment, co-operation, and responsible leadership from advertisers, technology platforms, publishers, developers, and tech vendors alike. If we work together, we all win together.""
Respect
Ensuring what and how data is being collected and processed as well as social, ethical and societal consequences is key to transparency.
""Some years ago, I downloaded a social media app. On registering, a pop-up appeared asking for access to my contacts,"" explains Jamie. ""Ordinarily, alarm bells would ring, but they made their intentions crystal clear - my contacts would be encrypted and only used to connect me to existing friends using the app. Then my data would be deleted permanently. This was all the reassurance I needed. I've never forgotten their respect for privacy and their open, transparent approach,"" he says.
Transparency is also acutely important in the field of AI. In some cases, machine-learnt decisions can have a profound impact on a person's life (diagnosing disease, securing credit or gaining employment), so it is imperative that AI's decision-making rationale can be interrogated.
From data first to people first: ""In a world where privacy has become a byword for the exact opposite, data ethics allows us to breathe the fresh air of higher purpose and think, act and behave in ways that revive trust in data and technology,"" says Jamie. ""The guide's four principles offer a framework to encourage brands and organisations to move from data first to people first, and create a digital age we're all happy to be part of.""
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (92%); FAST MOVING CONSUMER GOODS (90%); INVESTIGATIONS (90%); LAWYERS (90%); CORPORATE COUNSEL (89%); CONSUMERS (78%); DATA PROTECTION LAWS (78%); EU DATA PROTECTION REGULATION (78%); INTERVIEWS (78%); REGULATORY ACTIONS (71%); ASSOCIATIONS & ORGANIZATIONS (70%); CUSTOMER RELATIONS (68%)
Company:  UNILEVER PLC/NV (90%); GOOGLE LLC (85%)
Ticker: UNIA (AMS) (90%); UNA (BIT) (90%); UNA (AMS) (90%); UN (NYSE) (90%); ULVR (LSE) (90%); UL (NYSE) (90%)
Industry: NAICS325620 TOILET PREPARATION MANUFACTURING (90%); NAICS325611 SOAP & OTHER DETERGENT MANUFACTURING (90%); NAICS311412 FROZEN SPECIALTY FOOD MANUFACTURING (90%); NAICS311411 FROZEN FRUIT, JUICE & VEGETABLE MANUFACTURING (90%); NAICS311225 FATS & OILS REFINING & BLENDING (90%); NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (85%); FAST MOVING CONSUMER GOODS (90%); LAWYERS (90%); CORPORATE COUNSEL (89%); BIG TECH (78%); DATA PROTECTION LAWS (78%); DATA SECURITY (78%); EU DATA PROTECTION REGULATION (78%); INFORMATION SECURITY & PRIVACY (78%); INFORMATION TECHNOLOGY INDUSTRY (78%); RIDE SHARING SERVICES (50%)
Geographic: EUROPE (79%)
Load-Date: April 17, 2021","Unilever, in a post, talks about its General Counsel for Global Marketing and Media Jamie Barnard creating a guide for brands on data ethics.
FMCG major Unilever has posted an article about how the company is working with the marketing industry to ensure that data is treated responsibly and ethically.
The article talks about an industry-wide code for marketers that Unilever's General Counsel for Global Marketing and Media Jamie Barnard, who is also the Chair of the Data Ethics Board at the World Federation of Advertisers (WFA), has created. As per the article it is the world's first guide for brands on data ethics. The guide is based on four principles: Respect, Fairness, Accountability and Transparency.
Here's the full article:
According to SAP research, when the benefits and value of collecting data are made clear to consumers, more than 70% of us are willing to share our data with brands. But if that trust is broken, if brands are found to have collected data without our knowledge, 79% will never use that brand again.
Over the past few years, high-profile cases such as that of Cambridge Analytica have led regulators to investigate big tech firms and their relationship with consumer data. And we've seen laws passed that mean organisations that collect data for business purposes need to comply with strict rules on privacy - laws that include Europe's General Data Protection Regulation and the Californian Consumer Privacy Act.
For Jamie Barnard, compliance with regulation is not enough. ""Compliance is a baseline, protecting people's fundamental human rights on the one hand and shielding companies from the sharp end of the law on the other,"" he told Google's Vice-President of the Global Client & Agency Solutions, Pedro Pina in an interview for Think with Google.
""But it's of limited value in the court of public opinion: if people think a company's data practices are unethical, then a demonstration of legal compliance will not protect their reputation. This is why brands have to set and follow a code of ethics,"" he says.
It's time for courage, not just compliance
Companies are waking up to the idea that they have a moral responsibility as well as a legal one, that they have a responsibility to respect people's rights and expectations.
As Chair of the Data Ethics Board at the World Federation of Advertisers (WFA), Jamie set about creating the world's first guide for brands on data ethics - an industry-wide code for marketers that looks to ""design a digital future that enhances people's lives and protects them in equal measure"". The guide is based on four key principles.
Respect
When you download a pizza delivery or ride-hailing app and agree to its terms, the app learns a lot about you: the fact that you have the latest iPhone, for example, who you bank with, your network of friends and family, where you live and possibly where you work too.
With every interaction, the algorithm gets to know you a little better; it won't be long before it learns that you like to go out or eat when you get back. When you hail a cab, it knows it's 2:00 am. With access to other data, it could work out that you're in a dangerous part of the city, far from home and in the rain.
It is the company's approach to data ethics that will determine how it chooses to use this information. On the one hand, if the company's priority is safety, it could use this data to protect you (making sure you are picked up first). On the other hand, it could raise its charges instead, since the algorithm knows that, in situations like these, you are statistically more likely to accept them.
With this regard, the guide's first principle is that data usage should respect the people behind the data. It urges companies to strive to understand the interests of all parties and use consumer data to improve people's lives.
Respect
Data has equal capacity for good and evil. It can be used to promote inclusivity, create diversity and eliminate bias but it can also be used to exclude, divide and stigmatise. Segregating people into custom audiences for precision and performance marketing, for example, can avoid showing hamburger ads to vegans or casino games to children.
However, it can be unethical too, such as a landlord restricting certain ethnic groups from seeing its ads, or an employer limiting job opportunities to a male audience.
It's also a double-edged sword - blocking ad targeting by sexual orientation stops people from excluding the LGBT+ community, but it also stops people offering their services directly to the LGBT+ community.
Fairness should see data usage aim to be inclusive, acknowledge diversity and eliminate bias rather than dividing groups. The second of the guide's four principles encourages brands to examine their data sets, mindsets and governance and see avoiding harmful discrimination as a shared responsibility between advertisers, platforms and publishers.
Respect
People are increasingly demanding to be shown how their data is being used and how it is being looked after. They want to know that their personal data is in safe hands, and that organisations have put in place mechanisms to protect their information.
The Consumer Goods Forum Futerra study describes Gen Z as the 'honest generation' who don't expect brands to be perfect but expect them to be truthful. Expectations like this are driving greater demand for openness as companies are held to account not just for their use of data, but for their suppliers' and partners' use too.
""We should think of ethics in the same way we think about sportsmanship. A good sport has scruples - they will do the right thing even if it requires sacrifice. And building trust is also a team sport,"" Jamie says. ""We are all accountable for collecting and using data in a safe, ethical, and transparent manner,"" he told the Think with Google newsletter. ""Embedding data ethics across the industry requires commitment, co-operation, and responsible leadership from advertisers, technology platforms, publishers, developers, and tech vendors alike. If we work together, we all win together.""
Respect
Ensuring what and how data is being collected and processed as well as social, ethical and societal consequences is key to transparency.
""Some years ago, I downloaded a social media app. On registering, a pop-up appeared asking for access to my contacts,"" explains Jamie. ""Ordinarily, alarm bells would ring, but they made their intentions crystal clear - my contacts would be encrypted and only used to connect me to existing friends using the app. Then my data would be deleted permanently. This was all the reassurance I needed. I've never forgotten their respect for privacy and their open, transparent approach,"" he says.
Transparency is also acutely important in the field of AI. In some cases, machine-learnt decisions can have a profound impact on a person's life (diagnosing disease, securing credit or gaining employment), so it is imperative that AI's decision-making rationale can be interrogated.
From data first to people first: ""In a world where privacy has become a byword for the exact opposite, data ethics allows us to breathe the fresh air of higher purpose and think, act and behave in ways that revive trust in data and technology,"" says Jamie. ""The guide's four principles offer a framework to encourage brands and organisations to move from data first to people first, and create a digital age we're all happy to be part of.""
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (92%); FAST MOVING CONSUMER GOODS (90%); INVESTIGATIONS (90%); LAWYERS (90%); CORPORATE COUNSEL (89%); CONSUMERS (78%); DATA PROTECTION LAWS (78%); EU DATA PROTECTION REGULATION (78%); INTERVIEWS (78%); REGULATORY ACTIONS (71%); ASSOCIATIONS & ORGANIZATIONS (70%); CUSTOMER RELATIONS (68%)
Company:  UNILEVER PLC/NV (90%); GOOGLE LLC (85%)
Ticker: UNIA (AMS) (90%); UNA (BIT) (90%); UNA (AMS) (90%); UN (NYSE) (90%); ULVR (LSE) (90%); UL (NYSE) (90%)
Industry: NAICS325620 TOILET PREPARATION MANUFACTURING (90%); NAICS325611 SOAP & OTHER DETERGENT MANUFACTURING (90%); NAICS311412 FROZEN SPECIALTY FOOD MANUFACTURING (90%); NAICS311411 FROZEN FRUIT, JUICE & VEGETABLE MANUFACTURING (90%); NAICS311225 FATS & OILS REFINING & BLENDING (90%); NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (85%); FAST MOVING CONSUMER GOODS (90%); LAWYERS (90%); CORPORATE COUNSEL (89%); BIG TECH (78%); DATA PROTECTION LAWS (78%); DATA SECURITY (78%); EU DATA PROTECTION REGULATION (78%); INFORMATION SECURITY & PRIVACY (78%); INFORMATION TECHNOLOGY INDUSTRY (78%); RIDE SHARING SERVICES (50%)
Geographic: EUROPE (79%)
Load-Date: April 17, 2021",neutral,0.8457650542259216,balanced/neutral,"['privacy', 'bias', 'discrimination', 'fairness', 'transparency', 'accountability', 'safety', 'security', 'human rights', 'agency', 'inclusivity', 'access']",['fairness'],"['regulation', 'governance', 'framework', 'law', 'compliance', 'should', 'need to', 'urges']",['algorithm'],12,1,8,1
2021,Unknown Title,"Byline: Dr. Norbert Lossau
Body
In the context of the Corona pandemic, there are various conflicts of distribution - for example in the case of vaccination. Can ethical dilemmas be resolved at all? Alena Buyx, who has been a member of the Ethics Council since 2016, explains how ethics scientists work.
   WORLD:
   Can ethics be learned?
   Alena Buyx:
Of course. Ethics is the theory of morality. And morality, that is the individual and collective rules of conduct and norms in a society that we all hopefully adhere to. But they are not immutable. We can and should reflect on whether individual rules are still appropriate or perhaps contradictory, and sometimes need new rules to meet new challenges. This questioning and adding to the arguments and foundations on which our morality is based, that is ethics. It is often misunderstood as some kind of gut feeling or opinion. In fact, ethics is a science. And science can be learned.
   So logic is more important to ethicists than empathy?
Empathy is of course very important in human interaction and thus for our morality. But ethics is first and foremost a science. It's about testing arguments and analysis. Emotional and psychological aspects are definitely involved. So ethics is certainly not just cold logic and analysis. Ethics originated as a field of philosophy; meanwhile ethics is very interdisciplinary. Sociological or psychological studies are also taken into account.
   What about common sense?
Common sense is certainly helpful in moral reasoning, but is sometimes off the mark or distorted, by false information, for example, or prejudice. For ethical analyses, one should have a certain command of scientific tools. Nevertheless, it is not impossible for scientific analysis and common sense to come to the same conclusion. One example is the prioritisation of vaccination against corona recommended by the Ethics Council together with the Standing Committee on Vaccination, the Stiko, and the Leopoldina. We have worked through all possible ethical principles and constitutional aspects and in the end have come to the conclusion that first and foremost those should be vaccinated who have the highest risk of becoming seriously ill with corona and dying from it. In addition, those who are exposed to an increased risk in their work, for example in the health service. If you ask people on the street, they will tell you exactly the same thing with their common sense. The agreement with the recommendation of the Ethics Council is over 90 percent.
   But that's not the case with all the recommendations of the Ethics Council.
True. Sometimes ethics and gut feeling clearly diverge. A classic example of this is the death penalty. Many people intuitively feel that it is the right thing to do. But if you analyze it ethically in detail, the ethical result of the analysis in most Western countries is that the state should not punish crimes with death in this way.
   You once said that the ethical devil is in the details. So you must know a lot of technical details?
For an ethics board, policy advice without empirical knowledge would be problematic. After all, we are talking about practical recommendations for the health care system or at the bedside. It would be absurd if we didn't know what the R-value is and how vaccines work. Nevertheless, the legal-ethical framework we are formulating is coarse-grained. It is not intended for every practical individual situation; it merely provides the ethical guard rails. When it comes to concrete implementation, further ethical questions arise. And that is where the devil is in the detail.
   So these guard rails give politicians room for manoeuvre?
Yes, the ethical guard rails clearly exclude certain things or recommend others, but there is still scope for policy makers to shape them. Even if a prioritisation of Corona vaccinations is given in principle, there may still be many ethical questions of detail when it comes to implementation in the vaccination centres.
   When it comes to vaccination, there will still be many questions of detail. Let us assume that two vector vaccines were licensed and available in sufficient quantities, with one vaccine having a significantly higher efficacy than the other. Wouldn't the Ethics Council then have to recommend that only the more effective vaccine be used?
Such a question is beyond the remit of the Ethics Council. We cannot delve that deeply into individual and detailed questions of application. That is not our mandate. In addition, the work on the Ethics Board is voluntary and most of us, like me, also have a full-time job. So we are not designed to deal with the day-to-day details, but to deal with the more fundamental issues. Of course, you can say something about specific questions in an interview. But answering these specific questions would be a matter for the Stiko anyway.
   Ethical questions are particularly explosive when they lead to an ethical dilemma. How does the Ethics Council deal with this? How do you communicate a dilemma to politicians?
There is a lot of debate about this in ethics theory. Some say that real dilemmas cannot be resolved in principle and that there cannot then be a course of action that is right. The others, and I am one of them, are convinced that dilemmas must be clearly pointed out, but that some can still be solved if one takes the reality of application into account. If one carefully examines the different positions, weighs up all the arguments again and looks for aspects that may not have been taken into account, then one usually finds one or even more ways to make a recommendation. In the rarest of cases, it turns out to be a tough, logical dilemma. The Ethics Board often struggles very hard to reach a consensus statement. Although it has a very plural membership, we usually find a common denominator. But there are also recommendations with a split vote, with a majority and a minority vote.
   How often is there unanimity?
Since May 2020, we've only had a split vote once - on the immunity recommendations. In fact, we always try very hard to reach consensus. Of course, political implementation is much easier when our vote is unanimous. Politicians are less able to deal with ""some say this, others say that "". But sometimes there is no other way, and then we have to be transparent.
   Have you ever been on the side of a minority vote?
No, that hasn't happened yet. In the case of the immunity recommendation, however, there was a 50-50 vote - i.e. no majority and no minority. However, such a stalemate is very rare.
   Does it annoy you when politicians do not follow the recommendations of the Ethics Council?
No, I am not annoyed. However, I always hope that our recommendations will be implemented. Because we really think things through carefully and try to take into account all relevant aspects as far as possible. We do not orient ourselves towards individual social groups or certain political orientations. One thing is clear: we only advise, we do not prescribe and we do not decide. That is a matter for politicians. They alone bear the responsibility for decisions. And it is the right of politicians to decide differently from what we recommend, or to seek advice from other institutions.
   Does the Ethics Council choose the topics to be dealt with itself or are they prescribed by the politicians?
Both. In the current Council period we have received quite a few requests from politicians in a short time. However, there has not yet been a formal request that can only be made by the Bundestag or the Federal Government. Requests come, for example, from individual members of the Federal Government. Both requests and assignments can be rejected by the Ethics Council. No one can give us instructions.
   When was the last time that the Ethics Council rejected a request or an order?
We very regularly refuse requests because many of them come from actors who cannot instruct us. As far as I know, the Council has not yet refused a mandate.
   How does one become a member of the Ethics Board?
Half of the members of the Ethics Council are appointed by the Federal Government and half by the Bundestag. So at some point you get a call from the Secretary of State and receive a letter of appointment from the President of the German Bundestag.
   How long can one be a member?
A council term lasts four years and you can be re-elected once. So you're in for a maximum of eight years.
   In the context of Corona, there are often conflicts over distribution. How can such conflicts be resolved?
We need to incorporate constitutional principles. Justice theory approaches from philosophy and other subjects - a thick board.
   Specific question. How can a fair balance be struck between covid victims and those who die from another disease due to the pandemic, such as a stroke or cancer?
First of all, the principle of equality applies. Corona patients have no special status. At the beginning of the pandemic, one could get the impression that corona patients would automatically be given priority treatment. The Ethics Council already stated in March 2020, still under my predecessor Peter Dabrock, that this is not possible. One must also consider the consequences for other patients. In the first wave of Corona, some clinics were virtually emptied out without using this capacity for Corona patients. On the other hand, patients with other diseases could no longer be cared for as they normally would have been. In the second wave, things have now gone much better. Great attention has also been paid to the care of non-Corona patients. It is in this context that one has to see the constant warning of the medical profession against the intensive care units being overcrowded. Because if there is no more capacity there, then neither the Corona patients nor other patients can be helped. Then there is no more care for emergencies of all kinds - whether one has suffered a heart attack or driven in front of a tree. Critical care is, after all, the common end route for all life-threatening illnesses. I was surprised that many people didn't understand this and thought that ICUs were only needed for Corona patients in pandemic times.
   After all, the lockdown measures were justified in no small part by the need to prevent ICUs from filling up.
Exactly. That is a central justification for the harsh restrictions on our fundamental rights. Proportionality is very important at this point. Because the entire population would be extremely affected by the failure of the medical care structure, this can be used to justify the lockdown measures.
   An example of a distributional conflict is also the prioritisation of vaccinations. There has been criticism that the disabled are not sufficiently taken into account. What do you say to this?
When the Stiko, the Leopoldina and the Ethics Council presented the first recommendation on prioritisation in vaccinations at the end of November 2020, this was, as I said, a legal-ethical framework, but also a rough one. For many disabilities and pre-existing conditions, empirical data were not yet available. However, some groups with disabilities were definitely there from the beginning, such as people with Down syndrome. They have a very high risk of severe covid. I can understand the criticism that was voiced at the beginning. Many disabled people and people with pre-existing conditions were not adequately included at the beginning. Since then, however, the Stiko has continually revised the recommendations and refined them through subgroups. Empirical data and input from professional societies were continuously taken into account. By the end of January 2021, the third version of the recommendations was already available. People with disabilities are now better considered, although it is certainly still not perfect. New evidence continues to be built in. And there is an opening clause for rare diseases where there is foreseeably insufficient empirical data from scientific studies on how much affected people are at risk from Corona. In such cases, case-by-case decisions are possible. Some federal states set up commissions for this purpose.
   Does the Ethics Council also deal with scenarios that lie in the future?
Of course we try to think ahead. In the recommendation that we published at the beginning of February on how to deal with vaccinated people, we refer to the current situation and to how we envisage the situation in the coming months. However, the paper clearly underlines that changing circumstances would also require a new way of thinking. This issue has not been conclusively addressed. This is not unusual in our ad hoc recommendations. At our next meeting, we will reflect on which issues will become relevant in the coming months of the pandemic and where we can contribute substantially. And then, of course, we have other topics that look to the future, such as the relationship between humans and machines.
   What do you think about renaming the Ethics Council the ""Council of Wise Men""?
Wisdom is the highest thing a human being can achieve. I would never presume to claim that for myself. And it would be presumptuous to call the Ethics Council a Council of the Wise. Such an exaggeration would be dangerous and would provoke too high expectations. But it is important for me to emphasize that we stand outside the political day-to-day business. The Ethics Council is not a political body and we have no agenda.
   Are you often asked for advice in your private life?
There is a study that analysed whether philosophers who deal with ethics are themselves more moral than others. The study's answer is a resounding no. Ethicists are not better people, that's for sure (laughs). But it's not surprising that I'm currently being asked for advice a lot in the context of Corona, even in my private life. When it comes to other private issues, I probably get asked as much as others.
   You have a lot of time on your hands with your professorship at the Technical University of Munich, your children and your work on the Ethics Council. But you obviously still have time for Twitter.
I don't do much there. But in my discipline, it's important to be on Twitter. A lot of scientific and professional information is exchanged there, debates are held, publications and conferences are announced. When I became Chair of the Ethics Board, I started using Twitter to publicise the work of the Ethics Board.
   Are you personally a supporter of the No Covid strategy?
There's a lot about the no-covid strategy that makes sense to me - especially the warning against ripping everything open again once the incidence is 49 or higher. That would be irresponsible. I find the vision of reducing incidence faster and more sustainably compelling. However, I see challenges in the implementation and with regard to the proportionality of measures. Of course, this is just my personal opinion and not an opinion of the Ethics Council.
   Once we get past the Covid pandemic, will artificial intelligence become the dominant topic on the Ethics Council?
Artificial intelligence is already a big issue for us. In the previous Council period, there were statements on robotics in care and on Big Data. At the moment, we are working on the major topic of ""Man and Machine "". There are many ethical questions that are of the greatest relevance for society. My chair is called ""Ethics of Medicine and Health Technologies"". That's where I do AI up and down.
Document original
Graphic
Alena Buyx, 43, has been chair of the German Ethics Council since May 2020. The doctor of medicine also holds a master's degree in philosophy and sociology. Since 2018, she has been an ethics professor at the TU Munich
Classification
Language: ENGLISH
Publication-Type: Newspaper
Journal Code: WETR
Subject: ETHICS (96%); CAPITAL PUNISHMENT (89%); PUBLIC HEALTH (89%); TALKS & MEETINGS (89%); VACCINATION & IMMUNIZATION (89%); DEATH & DYING (79%); PHILOSOPHY (79%); PSYCHOLOGY (79%); SOCIOLOGY (79%); AGREEMENTS (78%); DISINFORMATION & MISINFORMATION (78%); NEGATIVE NEWS (78%); EMOTIONS (75%); MEDICINE & HEALTH (71%)
Industry: VACCINATION & IMMUNIZATION (89%); PSYCHOLOGY (79%)
Load-Date: February 20, 2021","In the context of the Corona pandemic, there are various conflicts of distribution - for example in the case of vaccination. Can ethical dilemmas be resolved at all? Alena Buyx, who has been a member of the Ethics Council since 2016, explains how ethics scientists work.
   WORLD:
   Can ethics be learned?
   Alena Buyx:
Of course. Ethics is the theory of morality. And morality, that is the individual and collective rules of conduct and norms in a society that we all hopefully adhere to. But they are not immutable. We can and should reflect on whether individual rules are still appropriate or perhaps contradictory, and sometimes need new rules to meet new challenges. This questioning and adding to the arguments and foundations on which our morality is based, that is ethics. It is often misunderstood as some kind of gut feeling or opinion. In fact, ethics is a science. And science can be learned.
   So logic is more important to ethicists than empathy?
Empathy is of course very important in human interaction and thus for our morality. But ethics is first and foremost a science. It's about testing arguments and analysis. Emotional and psychological aspects are definitely involved. So ethics is certainly not just cold logic and analysis. Ethics originated as a field of philosophy; meanwhile ethics is very interdisciplinary. Sociological or psychological studies are also taken into account.
   What about common sense?
Common sense is certainly helpful in moral reasoning, but is sometimes off the mark or distorted, by false information, for example, or prejudice. For ethical analyses, one should have a certain command of scientific tools. Nevertheless, it is not impossible for scientific analysis and common sense to come to the same conclusion. One example is the prioritisation of vaccination against corona recommended by the Ethics Council together with the Standing Committee on Vaccination, the Stiko, and the Leopoldina. We have worked through all possible ethical principles and constitutional aspects and in the end have come to the conclusion that first and foremost those should be vaccinated who have the highest risk of becoming seriously ill with corona and dying from it. In addition, those who are exposed to an increased risk in their work, for example in the health service. If you ask people on the street, they will tell you exactly the same thing with their common sense. The agreement with the recommendation of the Ethics Council is over 90 percent.
   But that's not the case with all the recommendations of the Ethics Council.
True. Sometimes ethics and gut feeling clearly diverge. A classic example of this is the death penalty. Many people intuitively feel that it is the right thing to do. But if you analyze it ethically in detail, the ethical result of the analysis in most Western countries is that the state should not punish crimes with death in this way.
   You once said that the ethical devil is in the details. So you must know a lot of technical details?
For an ethics board, policy advice without empirical knowledge would be problematic. After all, we are talking about practical recommendations for the health care system or at the bedside. It would be absurd if we didn't know what the R-value is and how vaccines work. Nevertheless, the legal-ethical framework we are formulating is coarse-grained. It is not intended for every practical individual situation; it merely provides the ethical guard rails. When it comes to concrete implementation, further ethical questions arise. And that is where the devil is in the detail.
   So these guard rails give politicians room for manoeuvre?
Yes, the ethical guard rails clearly exclude certain things or recommend others, but there is still scope for policy makers to shape them. Even if a prioritisation of Corona vaccinations is given in principle, there may still be many ethical questions of detail when it comes to implementation in the vaccination centres.
   When it comes to vaccination, there will still be many questions of detail. Let us assume that two vector vaccines were licensed and available in sufficient quantities, with one vaccine having a significantly higher efficacy than the other. Wouldn't the Ethics Council then have to recommend that only the more effective vaccine be used?
Such a question is beyond the remit of the Ethics Council. We cannot delve that deeply into individual and detailed questions of application. That is not our mandate. In addition, the work on the Ethics Board is voluntary and most of us, like me, also have a full-time job. So we are not designed to deal with the day-to-day details, but to deal with the more fundamental issues. Of course, you can say something about specific questions in an interview. But answering these specific questions would be a matter for the Stiko anyway.
   Ethical questions are particularly explosive when they lead to an ethical dilemma. How does the Ethics Council deal with this? How do you communicate a dilemma to politicians?
There is a lot of debate about this in ethics theory. Some say that real dilemmas cannot be resolved in principle and that there cannot then be a course of action that is right. The others, and I am one of them, are convinced that dilemmas must be clearly pointed out, but that some can still be solved if one takes the reality of application into account. If one carefully examines the different positions, weighs up all the arguments again and looks for aspects that may not have been taken into account, then one usually finds one or even more ways to make a recommendation. In the rarest of cases, it turns out to be a tough, logical dilemma. The Ethics Board often struggles very hard to reach a consensus statement. Although it has a very plural membership, we usually find a common denominator. But there are also recommendations with a split vote, with a majority and a minority vote.
   How often is there unanimity?
Since May 2020, we've only had a split vote once - on the immunity recommendations. In fact, we always try very hard to reach consensus. Of course, political implementation is much easier when our vote is unanimous. Politicians are less able to deal with ""some say this, others say that "". But sometimes there is no other way, and then we have to be transparent.
   Have you ever been on the side of a minority vote?
No, that hasn't happened yet. In the case of the immunity recommendation, however, there was a 50-50 vote - i.e. no majority and no minority. However, such a stalemate is very rare.
   Does it annoy you when politicians do not follow the recommendations of the Ethics Council?
No, I am not annoyed. However, I always hope that our recommendations will be implemented. Because we really think things through carefully and try to take into account all relevant aspects as far as possible. We do not orient ourselves towards individual social groups or certain political orientations. One thing is clear: we only advise, we do not prescribe and we do not decide. That is a matter for politicians. They alone bear the responsibility for decisions. And it is the right of politicians to decide differently from what we recommend, or to seek advice from other institutions.
   Does the Ethics Council choose the topics to be dealt with itself or are they prescribed by the politicians?
Both. In the current Council period we have received quite a few requests from politicians in a short time. However, there has not yet been a formal request that can only be made by the Bundestag or the Federal Government. Requests come, for example, from individual members of the Federal Government. Both requests and assignments can be rejected by the Ethics Council. No one can give us instructions.
   When was the last time that the Ethics Council rejected a request or an order?
We very regularly refuse requests because many of them come from actors who cannot instruct us. As far as I know, the Council has not yet refused a mandate.
   How does one become a member of the Ethics Board?
Half of the members of the Ethics Council are appointed by the Federal Government and half by the Bundestag. So at some point you get a call from the Secretary of State and receive a letter of appointment from the President of the German Bundestag.
   How long can one be a member?
A council term lasts four years and you can be re-elected once. So you're in for a maximum of eight years.
   In the context of Corona, there are often conflicts over distribution. How can such conflicts be resolved?
We need to incorporate constitutional principles. Justice theory approaches from philosophy and other subjects - a thick board.
   Specific question. How can a fair balance be struck between covid victims and those who die from another disease due to the pandemic, such as a stroke or cancer?
First of all, the principle of equality applies. Corona patients have no special status. At the beginning of the pandemic, one could get the impression that corona patients would automatically be given priority treatment. The Ethics Council already stated in March 2020, still under my predecessor Peter Dabrock, that this is not possible. One must also consider the consequences for other patients. In the first wave of Corona, some clinics were virtually emptied out without using this capacity for Corona patients. On the other hand, patients with other diseases could no longer be cared for as they normally would have been. In the second wave, things have now gone much better. Great attention has also been paid to the care of non-Corona patients. It is in this context that one has to see the constant warning of the medical profession against the intensive care units being overcrowded. Because if there is no more capacity there, then neither the Corona patients nor other patients can be helped. Then there is no more care for emergencies of all kinds - whether one has suffered a heart attack or driven in front of a tree. Critical care is, after all, the common end route for all life-threatening illnesses. I was surprised that many people didn't understand this and thought that ICUs were only needed for Corona patients in pandemic times.
   After all, the lockdown measures were justified in no small part by the need to prevent ICUs from filling up.
Exactly. That is a central justification for the harsh restrictions on our fundamental rights. Proportionality is very important at this point. Because the entire population would be extremely affected by the failure of the medical care structure, this can be used to justify the lockdown measures.
   An example of a distributional conflict is also the prioritisation of vaccinations. There has been criticism that the disabled are not sufficiently taken into account. What do you say to this?
When the Stiko, the Leopoldina and the Ethics Council presented the first recommendation on prioritisation in vaccinations at the end of November 2020, this was, as I said, a legal-ethical framework, but also a rough one. For many disabilities and pre-existing conditions, empirical data were not yet available. However, some groups with disabilities were definitely there from the beginning, such as people with Down syndrome. They have a very high risk of severe covid. I can understand the criticism that was voiced at the beginning. Many disabled people and people with pre-existing conditions were not adequately included at the beginning. Since then, however, the Stiko has continually revised the recommendations and refined them through subgroups. Empirical data and input from professional societies were continuously taken into account. By the end of January 2021, the third version of the recommendations was already available. People with disabilities are now better considered, although it is certainly still not perfect. New evidence continues to be built in. And there is an opening clause for rare diseases where there is foreseeably insufficient empirical data from scientific studies on how much affected people are at risk from Corona. In such cases, case-by-case decisions are possible. Some federal states set up commissions for this purpose.
   Does the Ethics Council also deal with scenarios that lie in the future?
Of course we try to think ahead. In the recommendation that we published at the beginning of February on how to deal with vaccinated people, we refer to the current situation and to how we envisage the situation in the coming months. However, the paper clearly underlines that changing circumstances would also require a new way of thinking. This issue has not been conclusively addressed. This is not unusual in our ad hoc recommendations. At our next meeting, we will reflect on which issues will become relevant in the coming months of the pandemic and where we can contribute substantially. And then, of course, we have other topics that look to the future, such as the relationship between humans and machines.
   What do you think about renaming the Ethics Council the ""Council of Wise Men""?
Wisdom is the highest thing a human being can achieve. I would never presume to claim that for myself. And it would be presumptuous to call the Ethics Council a Council of the Wise. Such an exaggeration would be dangerous and would provoke too high expectations. But it is important for me to emphasize that we stand outside the political day-to-day business. The Ethics Council is not a political body and we have no agenda.
   Are you often asked for advice in your private life?
There is a study that analysed whether philosophers who deal with ethics are themselves more moral than others. The study's answer is a resounding no. Ethicists are not better people, that's for sure (laughs). But it's not surprising that I'm currently being asked for advice a lot in the context of Corona, even in my private life. When it comes to other private issues, I probably get asked as much as others.
   You have a lot of time on your hands with your professorship at the Technical University of Munich, your children and your work on the Ethics Council. But you obviously still have time for Twitter.
I don't do much there. But in my discipline, it's important to be on Twitter. A lot of scientific and professional information is exchanged there, debates are held, publications and conferences are announced. When I became Chair of the Ethics Board, I started using Twitter to publicise the work of the Ethics Board.
   Are you personally a supporter of the No Covid strategy?
There's a lot about the no-covid strategy that makes sense to me - especially the warning against ripping everything open again once the incidence is 49 or higher. That would be irresponsible. I find the vision of reducing incidence faster and more sustainably compelling. However, I see challenges in the implementation and with regard to the proportionality of measures. Of course, this is just my personal opinion and not an opinion of the Ethics Council.
   Once we get past the Covid pandemic, will artificial intelligence become the dominant topic on the Ethics Council?
Artificial intelligence is already a big issue for us. In the previous Council period, there were statements on robotics in care and on Big Data. At the moment, we are working on the major topic of ""Man and Machine "". There are many ethical questions that are of the greatest relevance for society. My chair is called ""Ethics of Medicine and Health Technologies"". That's where I do AI up and down.
Document original
Graphic
Alena Buyx, 43, has been chair of the German Ethics Council since May 2020. The doctor of medicine also holds a master's degree in philosophy and sociology. Since 2018, she has been an ethics professor at the TU Munich
Classification
Language: ENGLISH
Publication-Type: Newspaper
Journal Code: WETR
Subject: ETHICS (96%); CAPITAL PUNISHMENT (89%); PUBLIC HEALTH (89%); TALKS & MEETINGS (89%); VACCINATION & IMMUNIZATION (89%); DEATH & DYING (79%); PHILOSOPHY (79%); PSYCHOLOGY (79%); SOCIOLOGY (79%); AGREEMENTS (78%); DISINFORMATION & MISINFORMATION (78%); NEGATIVE NEWS (78%); EMOTIONS (75%); MEDICINE & HEALTH (71%)
Industry: VACCINATION & IMMUNIZATION (89%); PSYCHOLOGY (79%)
Load-Date: February 20, 2021",neutral,0.8072177171707153,balanced/neutral,"['misinformation', 'disinformation']","['justice', 'equality', 'justice']","['policy', 'framework', 'should', 'must', 'need to', 'recommend']",['robotics'],2,3,6,1
2021,Unknown Title,"Body
The Code of Ethics of Artificial Intelligence (AI) will help avoid some of the risks associated with the use of AI, Infowatch President Natalya Kasperskaya told TASS on Tuesday. She was speaking on the sidelines of the first international forum ""Ethics of Artificial Intelligence: The Beginning of Trust"", which is being held at TASS.
""As for this code (of AI ethics - TASS), it was developed and signed in order to remove some of these risks,"" she said.
Kasperskaya also believes that everyone interested in the development and effective use of AI should take into account the possible risks in the field of cybersecurity.
About Code
Earlier on Tuesday, the AI Alliance and several other organizations signed the code of ethics of artificial intelligence (AI). The signing took place at TASS as part of the first international forum ""Ethics of Artificial Intelligence: The Beginning of Trust"", which takes place on October 26 in Moscow.
The Code will become part of the Artificial Intelligence federal project and the Strategy for the Development of the Information Society for 2017-2030.
It establishes general ethical principles and standards of conduct to guide those involved in activities using artificial intelligence.
The Code applies to relations involving ethical aspects of the creation (design, construction, piloting), implementation and use of AI technologies at all stages of the life cycle, which are currently not regulated by Russian law or other regulatory acts.
Source: Russian News Agency
Classification
Language: ENGLISH
Publication-Type: Wire
Subject: ETHICS (98%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); NEWS SYNDICATION (78%); PRESS AGENCY RELEASES (78%); INFORMATION SECURITY & PRIVACY (69%)
Geographic: MOSCOW, RUSSIAN FEDERATION (58%); RUSSIAN FEDERATION (90%)
Load-Date: October 27, 2021","The Code of Ethics of Artificial Intelligence (AI) will help avoid some of the risks associated with the use of AI, Infowatch President Natalya Kasperskaya told TASS on Tuesday. She was speaking on the sidelines of the first international forum ""Ethics of Artificial Intelligence: The Beginning of Trust"", which is being held at TASS.
""As for this code (of AI ethics - TASS), it was developed and signed in order to remove some of these risks,"" she said.
Kasperskaya also believes that everyone interested in the development and effective use of AI should take into account the possible risks in the field of cybersecurity.
About Code
Earlier on Tuesday, the AI Alliance and several other organizations signed the code of ethics of artificial intelligence (AI). The signing took place at TASS as part of the first international forum ""Ethics of Artificial Intelligence: The Beginning of Trust"", which takes place on October 26 in Moscow.
The Code will become part of the Artificial Intelligence federal project and the Strategy for the Development of the Information Society for 2017-2030.
It establishes general ethical principles and standards of conduct to guide those involved in activities using artificial intelligence.
The Code applies to relations involving ethical aspects of the creation (design, construction, piloting), implementation and use of AI technologies at all stages of the life cycle, which are currently not regulated by Russian law or other regulatory acts.
Source: Russian News Agency
Classification
Language: ENGLISH
Publication-Type: Wire
Subject: ETHICS (98%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); NEWS SYNDICATION (78%); PRESS AGENCY RELEASES (78%); INFORMATION SECURITY & PRIVACY (69%)
Geographic: MOSCOW, RUSSIAN FEDERATION (58%); RUSSIAN FEDERATION (90%)
Load-Date: October 27, 2021",neutral,0.8300483226776123,balanced/neutral,"['privacy', 'security', 'agency']",[],"['standards', 'law', 'should']",[],3,0,3,0
2021,Unknown Title,"Byline: ACCESSWIRE
Body
85% of advertising executives have shifted their focus towards ethical standards in DOOH advertising
MIAMI BEACH, FL / ACCESSWIRE / October 12, 2021 / The Digital Out of Home (DOOH) market was estimated to be worth around $41.06 billion by the end of 2020, according to new research from Alfi (NASDAQ:ALF), an AI enterprise SaaS advertising platform, however, nearly two out of three (65%) advertising executives predict value will rise to between $50 billion and $55 billion by 2026, while 30% think it will be bigger.
The study also found that 85% of advertising executives are worried about the level of ethical standards DOOH advertising companies are adhering to, with 47% stating they are ""very concerned"" about ethics around DOOH advertising in terms of the use of cookies, the storage of personal data, and facial images.
""The DOOH advertising market has not experienced a technological revolution in decades, so naturally it has work to do in order to address concerns with operations and scalability with modern technology,"" said Paul Pereira, CEO, Alfi. ""Privacy and respect are key factors that not only the industry should be aware of but also the leaders of the pack.""
Alfi's next generation technology can detect behavior without collecting or storing personal data or facial images. It sets new standards by providing precise targeting information to advertisers by collecting information in non-intrusive ways that are compliant with GDPR, CCPA, and HIPAA. Since both GDPR and CCPA focus on the collection and storage of personal data, the fact that Alfi does not collect any personal identifiers or information makes it a compliant pioneer in the revolution of the OOH sector.
""As cookies slowly come to an end and consumers demand improved experiences, it is important for advertisers to first ensure that their DOOH partners have ethics and respect ingrained in their fabric and fully compliant with all GDPR, CCPA and HIPAA standards, before starting any campaign,"" concluded Pereira.
Alfi's proprietary platform utilizes perceptual details to detect a face and predict the age and gender of the person looking at the screen, and then serves content based on their emotions to the first set of ads. The company does not track or identify anyone, nor does it record or store facial images or videos. Alfi believes this type of anonymized, yet targeted advertising is the future of the industry, especially as the industry moves away from cookies and deep collection of consumers' personal data and browsing history.
Methodology
Alfi commissioned market research company, PureProfile, to survey 100 senior advertising professionals from across the U.S., U.K., France, Germany, and Asia. Interviews were conducted online in June 2021
About Alfi Inc.
Alfi, Inc. provides solutions that bring transparency and accountability to the digital out-of-home advertising marketplace. Since 2018, Alfi, Inc. has been developing its artificial intelligence advertising platform to deliver targeted advertising in an ethical and privacy-conscious manner. For more information, please visit: https://www.getalfi.com 
Alfi Inc. U.S. Media Contacts
Danielle DeVoren / Laura Schooler
KCSA Strategic Communications
Alfi@kcsa.com 
Alfi Inc. U.K. Media Contact
Perception A
Phil Anderson / Taylor Marriott
07767 491 519 / 07983 335 021
Alfi Inc. Investor Relations
TraDigital IR
Kevin McGrath
+1-646-418-7002
kevin@tradigitalir.com 
SOURCE: Alfi, Inc.
View source version on accesswire.com: 
https://www.accesswire.com/667602/Advertising-Budgets-Rise-Along-with-Concerns-Around-Ethical-Standards-in-Digital-Out-of-Home-Sector-Alfi-Study-Finds
Classification
Language: ENGLISH
Publication-Type: Wire
Journal Code: 1458
Subject: ETHICS (93%); EXECUTIVES (90%); RESEARCH REPORTS (90%); CONSUMERS (89%); EU DATA PROTECTION REGULATION (88%); MARKET DEMOGRAPHICS (78%); MARKETING & ADVERTISING EXPENDITURE (78%); MARKETING & ADVERTISING SECTOR PERFORMANCE (78%); OUTPUT & DEMAND (78%); RESEARCH & DEVELOPMENT (78%); EMOTIONS (74%); ARTIFICIAL INTELLIGENCE (71%); US HEALTH INSURANCE PORTABILITY & ACCOUNTABILITY ACT (71%); HEALTH CARE REGULATORY COMPLIANCE (69%)
Industry: MARKETING & ADVERTISING (94%); BUDGETS (90%); MARKETING & ADVERTISING AGENCIES (90%); INTERNET COOKIES (89%); MARKET SIZE (89%); MARKETING & ADVERTISING SERVICES (89%); DATA STORAGE TECHNOLOGY (88%); EU DATA PROTECTION REGULATION (88%); MARKET DEMOGRAPHICS (78%); MARKET RESEARCH & ANALYSIS (78%); MARKET RESEARCH FIRMS (78%); MARKETING & ADVERTISING EXPENDITURE (78%); MARKETING & ADVERTISING SECTOR PERFORMANCE (78%); MARKET RESEARCH (77%); MARKET SEGMENTATION (77%); SOFTWARE AS A SERVICE (76%); ARTIFICIAL INTELLIGENCE (71%); US HEALTH INSURANCE PORTABILITY & ACCOUNTABILITY ACT (71%); HEALTH CARE REGULATORY COMPLIANCE (69%)
Geographic: MIAMI, FL, USA (73%); FLORIDA, USA (79%); GERMANY (79%); UNITED STATES (79%); FRANCE (50%)
Load-Date: October 12, 2021","85% of advertising executives have shifted their focus towards ethical standards in DOOH advertising
MIAMI BEACH, FL / ACCESSWIRE / October 12, 2021 / The Digital Out of Home (DOOH) market was estimated to be worth around $41.06 billion by the end of 2020, according to new research from Alfi (NASDAQ:ALF), an AI enterprise SaaS advertising platform, however, nearly two out of three (65%) advertising executives predict value will rise to between $50 billion and $55 billion by 2026, while 30% think it will be bigger.
The study also found that 85% of advertising executives are worried about the level of ethical standards DOOH advertising companies are adhering to, with 47% stating they are ""very concerned"" about ethics around DOOH advertising in terms of the use of cookies, the storage of personal data, and facial images.
""The DOOH advertising market has not experienced a technological revolution in decades, so naturally it has work to do in order to address concerns with operations and scalability with modern technology,"" said Paul Pereira, CEO, Alfi. ""Privacy and respect are key factors that not only the industry should be aware of but also the leaders of the pack.""
Alfi's next generation technology can detect behavior without collecting or storing personal data or facial images. It sets new standards by providing precise targeting information to advertisers by collecting information in non-intrusive ways that are compliant with GDPR, CCPA, and HIPAA. Since both GDPR and CCPA focus on the collection and storage of personal data, the fact that Alfi does not collect any personal identifiers or information makes it a compliant pioneer in the revolution of the OOH sector.
""As cookies slowly come to an end and consumers demand improved experiences, it is important for advertisers to first ensure that their DOOH partners have ethics and respect ingrained in their fabric and fully compliant with all GDPR, CCPA and HIPAA standards, before starting any campaign,"" concluded Pereira.
Alfi's proprietary platform utilizes perceptual details to detect a face and predict the age and gender of the person looking at the screen, and then serves content based on their emotions to the first set of ads. The company does not track or identify anyone, nor does it record or store facial images or videos. Alfi believes this type of anonymized, yet targeted advertising is the future of the industry, especially as the industry moves away from cookies and deep collection of consumers' personal data and browsing history.
Methodology
Alfi commissioned market research company, PureProfile, to survey 100 senior advertising professionals from across the U.S., U.K., France, Germany, and Asia. Interviews were conducted online in June 2021
About Alfi Inc.
Alfi, Inc. provides solutions that bring transparency and accountability to the digital out-of-home advertising marketplace. Since 2018, Alfi, Inc. has been developing its artificial intelligence advertising platform to deliver targeted advertising in an ethical and privacy-conscious manner. For more information, please visit: https://www.getalfi.com 
Alfi Inc. U.S. Media Contacts
Danielle DeVoren / Laura Schooler
KCSA Strategic Communications
Alfi@kcsa.com 
Alfi Inc. U.K. Media Contact
Perception A
Phil Anderson / Taylor Marriott
07767 491 519 / 07983 335 021
Alfi Inc. Investor Relations
TraDigital IR
Kevin McGrath
+1-646-418-7002
kevin@tradigitalir.com 
SOURCE: Alfi, Inc.
View source version on accesswire.com: 
https://www.accesswire.com/667602/Advertising-Budgets-Rise-Along-with-Concerns-Around-Ethical-Standards-in-Digital-Out-of-Home-Sector-Alfi-Study-Finds
Classification
Language: ENGLISH
Publication-Type: Wire
Journal Code: 1458
Subject: ETHICS (93%); EXECUTIVES (90%); RESEARCH REPORTS (90%); CONSUMERS (89%); EU DATA PROTECTION REGULATION (88%); MARKET DEMOGRAPHICS (78%); MARKETING & ADVERTISING EXPENDITURE (78%); MARKETING & ADVERTISING SECTOR PERFORMANCE (78%); OUTPUT & DEMAND (78%); RESEARCH & DEVELOPMENT (78%); EMOTIONS (74%); ARTIFICIAL INTELLIGENCE (71%); US HEALTH INSURANCE PORTABILITY & ACCOUNTABILITY ACT (71%); HEALTH CARE REGULATORY COMPLIANCE (69%)
Industry: MARKETING & ADVERTISING (94%); BUDGETS (90%); MARKETING & ADVERTISING AGENCIES (90%); INTERNET COOKIES (89%); MARKET SIZE (89%); MARKETING & ADVERTISING SERVICES (89%); DATA STORAGE TECHNOLOGY (88%); EU DATA PROTECTION REGULATION (88%); MARKET DEMOGRAPHICS (78%); MARKET RESEARCH & ANALYSIS (78%); MARKET RESEARCH FIRMS (78%); MARKETING & ADVERTISING EXPENDITURE (78%); MARKETING & ADVERTISING SECTOR PERFORMANCE (78%); MARKET RESEARCH (77%); MARKET SEGMENTATION (77%); SOFTWARE AS A SERVICE (76%); ARTIFICIAL INTELLIGENCE (71%); US HEALTH INSURANCE PORTABILITY & ACCOUNTABILITY ACT (71%); HEALTH CARE REGULATORY COMPLIANCE (69%)
Geographic: MIAMI, FL, USA (73%); FLORIDA, USA (79%); GERMANY (79%); UNITED STATES (79%); FRANCE (50%)
Load-Date: October 12, 2021",neutral,0.7087510824203491,balanced/neutral,"['privacy', 'transparency', 'accountability']",[],"['regulation', 'standards', 'compliance', 'should']",[],3,0,4,0
2021,Unknown Title,"Byline: States News Service
Dateline: ARLINGTON, Virginia 
Body
The following information was released by the Association of the United States Army:
As the U.S. military continues to explore the use of artificial intelligence in weapons, efforts are underway to enact ethical principles that would bind their activity.
A basic framework has been created requiring a human in the chain when using weapons in the future, experts said Feb. 9 during an Army-sponsored webinar that is part of the Army Futures Command's Mad Scientist series.
Following ethical principles is key to the effort, said Alka Patel, head of artificial intelligence ethics policy at the DoD Joint Artificial Intelligence Center. The military needs to trust technology and the people operating it, she said, warning that there is always a fear of moving too fast with a technology that is evolving and isn't well understood, she said.
There are long-term implications in the training process for soldiers who might be involved, she said. ""The end user really needs to understand these technologies,"" Patel said. Congress has passed a risk-assessment framework for use of artificial intelligence tools by the military, she said.
Philip Root of the Defense Advanced Research Projects Agency said it can be difficult to separate friendly from unfriendly combatants, which complicates efforts. This is a problem today when soldiers are dealing with civilians on the battlefield. It is even more complicated with an autonomous system in combat, said Root, a retired Army lieutenant colonel who is deputy director of DARPA's Defense Sciences Office.
Sophisticated automated weapons will require the ability to make tailorable decisions for different situations, Root said. It will be important that commanders and troops using the systems understand the decision-making process, even if it happens far faster than a human can think, and to have trust in the decisions, he said.
Machine-made decisions must be transparent, so they can be understood in after-action reviews, and the machines must be under controls in training and combat so that they follow guiding ethical principles and behaviors. There also must be a way to order them to disengage or deactivate in an instant, he said.
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ARMIES (94%); ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ARMED FORCES (89%); ASSOCIATIONS & ORGANIZATIONS (78%); DEFENSE RESEARCH (78%); US ARMY (78%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (73%)
Organization: ASSOCIATION OF THE UNITED STATES ARMY (84%); DEFENSE ADVANCED RESEARCH PROJECTS AGENCY (82%)
Industry: ARMIES (94%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ARMED FORCES (89%); DEFENSE RESEARCH (78%); US ARMY (78%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (73%); TELECONFERENCING (71%)
Geographic: VIRGINIA, USA (79%); UNITED STATES (92%)
Load-Date: March 11, 2021","The following information was released by the Association of the United States Army:
As the U.S. military continues to explore the use of artificial intelligence in weapons, efforts are underway to enact ethical principles that would bind their activity.
A basic framework has been created requiring a human in the chain when using weapons in the future, experts said Feb. 9 during an Army-sponsored webinar that is part of the Army Futures Command's Mad Scientist series.
Following ethical principles is key to the effort, said Alka Patel, head of artificial intelligence ethics policy at the DoD Joint Artificial Intelligence Center. The military needs to trust technology and the people operating it, she said, warning that there is always a fear of moving too fast with a technology that is evolving and isn't well understood, she said.
There are long-term implications in the training process for soldiers who might be involved, she said. ""The end user really needs to understand these technologies,"" Patel said. Congress has passed a risk-assessment framework for use of artificial intelligence tools by the military, she said.
Philip Root of the Defense Advanced Research Projects Agency said it can be difficult to separate friendly from unfriendly combatants, which complicates efforts. This is a problem today when soldiers are dealing with civilians on the battlefield. It is even more complicated with an autonomous system in combat, said Root, a retired Army lieutenant colonel who is deputy director of DARPA's Defense Sciences Office.
Sophisticated automated weapons will require the ability to make tailorable decisions for different situations, Root said. It will be important that commanders and troops using the systems understand the decision-making process, even if it happens far faster than a human can think, and to have trust in the decisions, he said.
Machine-made decisions must be transparent, so they can be understood in after-action reviews, and the machines must be under controls in training and combat so that they follow guiding ethical principles and behaviors. There also must be a way to order them to disengage or deactivate in an instant, he said.
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ARMIES (94%); ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ARMED FORCES (89%); ASSOCIATIONS & ORGANIZATIONS (78%); DEFENSE RESEARCH (78%); US ARMY (78%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (73%)
Organization: ASSOCIATION OF THE UNITED STATES ARMY (84%); DEFENSE ADVANCED RESEARCH PROJECTS AGENCY (82%)
Industry: ARMIES (94%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ARMED FORCES (89%); DEFENSE RESEARCH (78%); US ARMY (78%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (73%); TELECONFERENCING (71%)
Geographic: VIRGINIA, USA (79%); UNITED STATES (92%)
Load-Date: March 11, 2021",neutral,0.8878338932991028,balanced/neutral,['agency'],[],"['regulation', 'policy', 'framework', 'must']",[],1,0,4,0
2021,Unknown Title,"Body
The McGill Artificial Intelligence Society (MAIS) held its first in-person event of the school year, a panel titled ""Ethics in AI,"" on Nov. 17. The audience was at full capacity, drawing in a crowd of approximately 35 people from the McGill community to the Trottier lecture hall.
The panel featured three professionals who engage with issues surrounding AI ethics in their respective disciplines: Masa Sweidan, McGill alumna and business development manager at the Montreal AI Ethics institute (MAIEI); Ignacio Cofone, assistant professor of privacy law, AI law, and business associations at McGill; and Mark Likhten, legal innovation lead at Cyberjustice Lab at L'Universite de Montreal (UdeM).
Kaustav Das Sharma, U4 Engineering and team lead of the McGill AI Podcast, moderated the event. The panellists acknowledged that AI is often misrepresented in media and popular culture, and agreed that it is important for the public to gain a more holistic understanding of AI and the ethical barriers that emerge with its advancement.
""What is important is to [...] be clear about what AI is,"" Likhten said. ""It is a very powerful tool, but it's still a tool which needs [...] human intervention.""
Cofone considered more precise issues, namely bias, transparency, and privacy within AI, as the issues that should garner more public attention. These three issues are at the core of his research in AI regulation.
""One important aspect to be aware of [...] is AI bias,"" Cofone said. ""AI decision-making affects everyone, everyday [....] Transparency [in AI] is important particularly with decision-making processes such as calculating credit scores to see if you would get a house [or] calculating your risk score to see if you go to jail [....] Privacy is important because most AI is trained with [sensitive] information about us.""
There was also discussion regarding how public institutions can work to push inclusivity and diversity to the forefront of AI research and development. Sweidan stated that diversifying AI education is a crucial first step.
""Having education that includes women, BIPOC, and LGBTQ [communities] is extremely important,"" Sweidan said. ""Having people with different backgrounds, looking at it from the philosophy standpoint, [from computer science], from law, I think that is what leads to a more holistic education, and I think that is an extremely important first step.""
Panellists also discussed the potential of AI systems to inflict harm, and the importance of adequate personal data protection regulation. Ending on a positive note, Das Sharma asked panellists what makes them excited for a future blossoming with cutting-edge AI development. Sweidan said that she is excited by the possibilities for AI creativity, while Likhten cited the applications of AI in justice.
""I am actually very optimistic for the future,"" said Likhten. ""[Cyberjustice Lab] works a lot with tools [using] AI to improve access to justice, and the possibilities that we see in that field are endless [....] We talk a lot about people getting stripped of their personal data [...], and the bad sides of AI, but there are lots, and lots of [...] good things that you can do with AI that remain within the boundaries of ethical principles.""
Classification
Language: ENGLISH
Publication-Type: Newspaper
Subject: ARTIFICIAL INTELLIGENCE ETHICS (91%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (90%); DATA PROTECTION LAWS (89%); BUSINESS & PROFESSIONAL ASSOCIATIONS (78%); COMPUTER SCIENCE (78%); LAW SCHOOLS (78%); RESEARCH & DEVELOPMENT (78%); BUSINESS NEWS (77%); COMPANY ACTIVITIES & MANAGEMENT (76%); PRIVACY RIGHTS (75%); ENGINEERING (74%); COLLEGE & UNIVERSITY PROFESSORS (73%); LGBTQ+ PERSONS (73%); ASSOCIATIONS & ORGANIZATIONS (72%); BUSINESS DEVELOPMENT (71%); DIVERSITY & INCLUSION (65%)
Company:  AI SYSTEMS (51%)
Industry: SIC7372 PREPACKAGED SOFTWARE (51%); ARTIFICIAL INTELLIGENCE ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (90%); DATA PROTECTION LAWS (89%); COMPUTER SCIENCE (78%); LAW SCHOOLS (78%); INFORMATION SECURITY & PRIVACY (75%); ENGINEERING (74%); COLLEGE & UNIVERSITY PROFESSORS (73%); CREDIT RATINGS (63%); DATA SECURITY (62%); PODCASTING (54%)
Geographic: MONTREAL, QC, CANADA (89%); QUEBEC, CANADA (59%); Montreal; QC
Load-Date: November 24, 2021","The McGill Artificial Intelligence Society (MAIS) held its first in-person event of the school year, a panel titled ""Ethics in AI,"" on Nov. 17. The audience was at full capacity, drawing in a crowd of approximately 35 people from the McGill community to the Trottier lecture hall.
The panel featured three professionals who engage with issues surrounding AI ethics in their respective disciplines: Masa Sweidan, McGill alumna and business development manager at the Montreal AI Ethics institute (MAIEI); Ignacio Cofone, assistant professor of privacy law, AI law, and business associations at McGill; and Mark Likhten, legal innovation lead at Cyberjustice Lab at L'Universite de Montreal (UdeM).
Kaustav Das Sharma, U4 Engineering and team lead of the McGill AI Podcast, moderated the event. The panellists acknowledged that AI is often misrepresented in media and popular culture, and agreed that it is important for the public to gain a more holistic understanding of AI and the ethical barriers that emerge with its advancement.
""What is important is to [...] be clear about what AI is,"" Likhten said. ""It is a very powerful tool, but it's still a tool which needs [...] human intervention.""
Cofone considered more precise issues, namely bias, transparency, and privacy within AI, as the issues that should garner more public attention. These three issues are at the core of his research in AI regulation.
""One important aspect to be aware of [...] is AI bias,"" Cofone said. ""AI decision-making affects everyone, everyday [....] Transparency [in AI] is important particularly with decision-making processes such as calculating credit scores to see if you would get a house [or] calculating your risk score to see if you go to jail [....] Privacy is important because most AI is trained with [sensitive] information about us.""
There was also discussion regarding how public institutions can work to push inclusivity and diversity to the forefront of AI research and development. Sweidan stated that diversifying AI education is a crucial first step.
""Having education that includes women, BIPOC, and LGBTQ [communities] is extremely important,"" Sweidan said. ""Having people with different backgrounds, looking at it from the philosophy standpoint, [from computer science], from law, I think that is what leads to a more holistic education, and I think that is an extremely important first step.""
Panellists also discussed the potential of AI systems to inflict harm, and the importance of adequate personal data protection regulation. Ending on a positive note, Das Sharma asked panellists what makes them excited for a future blossoming with cutting-edge AI development. Sweidan said that she is excited by the possibilities for AI creativity, while Likhten cited the applications of AI in justice.
""I am actually very optimistic for the future,"" said Likhten. ""[Cyberjustice Lab] works a lot with tools [using] AI to improve access to justice, and the possibilities that we see in that field are endless [....] We talk a lot about people getting stripped of their personal data [...], and the bad sides of AI, but there are lots, and lots of [...] good things that you can do with AI that remain within the boundaries of ethical principles.""
Classification
Language: ENGLISH
Publication-Type: Newspaper
Subject: ARTIFICIAL INTELLIGENCE ETHICS (91%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (90%); DATA PROTECTION LAWS (89%); BUSINESS & PROFESSIONAL ASSOCIATIONS (78%); COMPUTER SCIENCE (78%); LAW SCHOOLS (78%); RESEARCH & DEVELOPMENT (78%); BUSINESS NEWS (77%); COMPANY ACTIVITIES & MANAGEMENT (76%); PRIVACY RIGHTS (75%); ENGINEERING (74%); COLLEGE & UNIVERSITY PROFESSORS (73%); LGBTQ+ PERSONS (73%); ASSOCIATIONS & ORGANIZATIONS (72%); BUSINESS DEVELOPMENT (71%); DIVERSITY & INCLUSION (65%)
Company:  AI SYSTEMS (51%)
Industry: SIC7372 PREPACKAGED SOFTWARE (51%); ARTIFICIAL INTELLIGENCE ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE REGULATION & POLICY (90%); DATA PROTECTION LAWS (89%); COMPUTER SCIENCE (78%); LAW SCHOOLS (78%); INFORMATION SECURITY & PRIVACY (75%); ENGINEERING (74%); COLLEGE & UNIVERSITY PROFESSORS (73%); CREDIT RATINGS (63%); DATA SECURITY (62%); PODCASTING (54%)
Geographic: MONTREAL, QC, CANADA (89%); QUEBEC, CANADA (59%); Montreal; QC
Load-Date: November 24, 2021",neutral,0.5109395980834961,balanced/neutral,"['privacy', 'bias', 'transparency', 'security', 'inclusivity', 'access']","['justice', 'justice']","['regulation', 'policy', 'law', 'should']",[],6,2,4,0
2021,Unknown Title,"Byline: Targeted News Service
Dateline: WASHINGTON 
Body
WASHINGTON, Sept. 17 -- Kenneth W. Goodman, professor of medicine and philosophy of the Institute for Bioethics at the University of Miami Miller School of Medicine, Florida, has issued a public comment on the National Institute of Standards and Technology notice entitled ""Artificial Intelligence Risk Management Framework"". The comment was written on Sept. 9, 2021, and posted on Sept. 16, 2021:
* * *
Thank you for this opportunity to comment on the AI-RMF. I offer a simple suggestion, with rather large granularity, namely, that NIST and its sibling institutes have a rare opportunity to formalize attention to applied ethics as a core component of the nation's AI mission.
There are countless good suggestions for guiding ethical and trustworthy AI; and no shortage of advice about how to achieve this. What seems to be wanted is a structure or process best conceived as analogous to that fledged in the early days of the Human Genome Project, namely, an explicit and sustained commitment to identify, study and resolve Ethical, Legal and Social Implications of an exciting new technology. Indeed, from transportation to law enforcement to medicine, more lives are and will be touched by AI than genetics... and the future of genetics itself will rely on both knowledge-based and machine-learning AI.
In many respects, the biomedical informatics and ethics communities have already done an exemplary job. Indeed, many of the challenges identified - often framed as ""concerns"" - have been studied by these communities for decades.
What is wanted is likely not an ongoing itemization of concerns and challenges but, rather, an ELSI program/unit/working group to (i) identify new issues as well as best practices for addressing those already recognized; (ii) conduct empirical and conceptual research on all such challenges; (iii) and offer best-practice policy and education guidance across the board.
For instance, it has already been suggested that standards themselves both raise and resolve ethical issues.* How they can - and ought - do so is a fertile field for additional inquiry and analysis.
An ELSI program for NIST, perhaps in coordination or conjunction with the many other federal agencies grappling with Artificial Intelligence, establishes a structured and transparent resource to identify and manage risk and, moreover, help ensure that these technologies both warrant and enjoy public trust.
Sincerely,
Kenneth W. Goodman, PhD, FACMI
Professor of Medicine and Philosophy
Director, Institute for Bioethics and Health Policy
Director, Institute for Data Science and Computing, Program on Data Ethics and Society
* * *
The notice can be viewed at: https://www.regulations.gov/document/NIST-2021-0004-0001
TARGETED NEWS SERVICE (founded 2004) features non-partisan 'edited journalism' news briefs and information for news organizations, public policy groups and individuals; as well as 'gathered' public policy information, including news releases, reports, speeches. For more information contact MYRON STRUCK, editor, editor@targetednews.com, Springfield, Virginia; 703/304-1897; https://targetednews.com
-1554778
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); BIOETHICS (90%); COLLEGE & UNIVERSITY PROFESSORS (90%); PHILOSOPHY (90%); BEST PRACTICES (89%); GRADUATE & PROFESSIONAL SCHOOLS (89%); PUBLIC POLICY (89%); BIOTECHNOLOGY & GENETIC SCIENCE (88%); BIOMEDICINE (78%); HEALTH CARE REGULATION & POLICY (78%); JOURNALISM (78%); NEWS BRIEFS (78%); RESEARCH INSTITUTES (78%); DATA SCIENCE (77%); RISK MANAGEMENT (77%); HEALTH CARE INFORMATION TECHNOLOGY (73%); GENOMICS (72%); GOVERNMENT DEPARTMENTS & AUTHORITIES (72%); MACHINE LEARNING (72%); STANDARDS & MEASUREMENTS (72%); INFORMATION SCIENCE (71%); ASSOCIATIONS & ORGANIZATIONS (70%); SCIENCE & TECHNOLOGY (67%)
Organization: NATIONAL INSTITUTE OF STANDARDS & TECHNOLOGY (84%)
Industry: ARTIFICIAL INTELLIGENCE (90%); COLLEGE & UNIVERSITY PROFESSORS (90%); GRADUATE & PROFESSIONAL SCHOOLS (89%); BIOMEDICINE (78%); HEALTH CARE REGULATION & POLICY (78%); DATA SCIENCE (77%); RISK MANAGEMENT (77%); HEALTH CARE INFORMATION TECHNOLOGY (73%); MACHINE LEARNING (72%)
Geographic: MIAMI, FL, USA (88%); FLORIDA, USA (90%)
Load-Date: September 17, 2021","WASHINGTON, Sept. 17 -- Kenneth W. Goodman, professor of medicine and philosophy of the Institute for Bioethics at the University of Miami Miller School of Medicine, Florida, has issued a public comment on the National Institute of Standards and Technology notice entitled ""Artificial Intelligence Risk Management Framework"". The comment was written on Sept. 9, 2021, and posted on Sept. 16, 2021:
* * *
Thank you for this opportunity to comment on the AI-RMF. I offer a simple suggestion, with rather large granularity, namely, that NIST and its sibling institutes have a rare opportunity to formalize attention to applied ethics as a core component of the nation's AI mission.
There are countless good suggestions for guiding ethical and trustworthy AI; and no shortage of advice about how to achieve this. What seems to be wanted is a structure or process best conceived as analogous to that fledged in the early days of the Human Genome Project, namely, an explicit and sustained commitment to identify, study and resolve Ethical, Legal and Social Implications of an exciting new technology. Indeed, from transportation to law enforcement to medicine, more lives are and will be touched by AI than genetics... and the future of genetics itself will rely on both knowledge-based and machine-learning AI.
In many respects, the biomedical informatics and ethics communities have already done an exemplary job. Indeed, many of the challenges identified - often framed as ""concerns"" - have been studied by these communities for decades.
What is wanted is likely not an ongoing itemization of concerns and challenges but, rather, an ELSI program/unit/working group to (i) identify new issues as well as best practices for addressing those already recognized; (ii) conduct empirical and conceptual research on all such challenges; (iii) and offer best-practice policy and education guidance across the board.
For instance, it has already been suggested that standards themselves both raise and resolve ethical issues.* How they can - and ought - do so is a fertile field for additional inquiry and analysis.
An ELSI program for NIST, perhaps in coordination or conjunction with the many other federal agencies grappling with Artificial Intelligence, establishes a structured and transparent resource to identify and manage risk and, moreover, help ensure that these technologies both warrant and enjoy public trust.
Sincerely,
Kenneth W. Goodman, PhD, FACMI
Professor of Medicine and Philosophy
Director, Institute for Bioethics and Health Policy
Director, Institute for Data Science and Computing, Program on Data Ethics and Society
* * *
The notice can be viewed at: https://www.regulations.gov/document/NIST-2021-0004-0001
TARGETED NEWS SERVICE (founded 2004) features non-partisan 'edited journalism' news briefs and information for news organizations, public policy groups and individuals; as well as 'gathered' public policy information, including news releases, reports, speeches. For more information contact MYRON STRUCK, editor, editor@targetednews.com, Springfield, Virginia; 703/304-1897; https://targetednews.com
-1554778
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); BIOETHICS (90%); COLLEGE & UNIVERSITY PROFESSORS (90%); PHILOSOPHY (90%); BEST PRACTICES (89%); GRADUATE & PROFESSIONAL SCHOOLS (89%); PUBLIC POLICY (89%); BIOTECHNOLOGY & GENETIC SCIENCE (88%); BIOMEDICINE (78%); HEALTH CARE REGULATION & POLICY (78%); JOURNALISM (78%); NEWS BRIEFS (78%); RESEARCH INSTITUTES (78%); DATA SCIENCE (77%); RISK MANAGEMENT (77%); HEALTH CARE INFORMATION TECHNOLOGY (73%); GENOMICS (72%); GOVERNMENT DEPARTMENTS & AUTHORITIES (72%); MACHINE LEARNING (72%); STANDARDS & MEASUREMENTS (72%); INFORMATION SCIENCE (71%); ASSOCIATIONS & ORGANIZATIONS (70%); SCIENCE & TECHNOLOGY (67%)
Organization: NATIONAL INSTITUTE OF STANDARDS & TECHNOLOGY (84%)
Industry: ARTIFICIAL INTELLIGENCE (90%); COLLEGE & UNIVERSITY PROFESSORS (90%); GRADUATE & PROFESSIONAL SCHOOLS (89%); BIOMEDICINE (78%); HEALTH CARE REGULATION & POLICY (78%); DATA SCIENCE (77%); RISK MANAGEMENT (77%); HEALTH CARE INFORMATION TECHNOLOGY (73%); MACHINE LEARNING (72%)
Geographic: MIAMI, FL, USA (88%); FLORIDA, USA (90%)
Load-Date: September 17, 2021",neutral,0.8698378205299377,balanced/neutral,[],[],"['regulation', 'policy', 'standards', 'framework', 'law']",['machine learning'],0,0,5,1
2021,Unknown Title,"Body
Aug. 25 -- Analytical Center for the Government of the Russian Federation issued the following news release:
Within the framework of the federal project ""Artificial Intelligence"", it is planned to develop a document regulating the ethical aspects of the development and application of artificial intelligence. Deputy Head of the Analytical Center, Head of the Center for Expertise for the Implementation of the Federal Project ""Artificial Intelligence"" Sergei Nakvasin spoke about this at the International Military-Technical Forum ""Army 2021"".
Nakvasin stressed the importance of developing human-centered technologies and urged developers to incorporate human-inherent qualities into algorithms. ""A person knows how to forgive, help, love, sympathize. If we come to a world where artificial intelligence is empowered to ""be human"" and make appropriate decisions, then it must have those qualities that allow us to remain human even in the most difficult times. I am ready to trust artificial intelligence, but I must be sure that its main task is to help me, my relatives and all people, ""said the deputy head of the AC.
""In October, together with the Government of Russia, the Alliance for Artificial Intelligence and the Presidential Administration, it is planned to hold a Forum on the Ethics of Artificial Intelligence, at which we will present and approve the National Code of Ethics for Artificial Intelligence. This is a very important event, and we are making every effort to achieve this, ""said Nakvasin.
Nakwasin ended his speech with an appeal to the developers of the Code of Ethics with an appeal to focus on the main human values.
Disclaimer: The Above Content is Auto-Translated
Source: Analytical Center for the Government of the Russian Federation
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ETHICS (90%); TALKS & MEETINGS (70%)
Industry: ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%)
Geographic: RUSSIAN FEDERATION (92%)
Load-Date: August 26, 2021","Aug. 25 -- Analytical Center for the Government of the Russian Federation issued the following news release:
Within the framework of the federal project ""Artificial Intelligence"", it is planned to develop a document regulating the ethical aspects of the development and application of artificial intelligence. Deputy Head of the Analytical Center, Head of the Center for Expertise for the Implementation of the Federal Project ""Artificial Intelligence"" Sergei Nakvasin spoke about this at the International Military-Technical Forum ""Army 2021"".
Nakvasin stressed the importance of developing human-centered technologies and urged developers to incorporate human-inherent qualities into algorithms. ""A person knows how to forgive, help, love, sympathize. If we come to a world where artificial intelligence is empowered to ""be human"" and make appropriate decisions, then it must have those qualities that allow us to remain human even in the most difficult times. I am ready to trust artificial intelligence, but I must be sure that its main task is to help me, my relatives and all people, ""said the deputy head of the AC.
""In October, together with the Government of Russia, the Alliance for Artificial Intelligence and the Presidential Administration, it is planned to hold a Forum on the Ethics of Artificial Intelligence, at which we will present and approve the National Code of Ethics for Artificial Intelligence. This is a very important event, and we are making every effort to achieve this, ""said Nakvasin.
Nakwasin ended his speech with an appeal to the developers of the Code of Ethics with an appeal to focus on the main human values.
Disclaimer: The Above Content is Auto-Translated
Source: Analytical Center for the Government of the Russian Federation
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ETHICS (90%); TALKS & MEETINGS (70%)
Industry: ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%)
Geographic: RUSSIAN FEDERATION (92%)
Load-Date: August 26, 2021",neutral,0.8079638481140137,balanced/neutral,[],[],"['framework', 'must']",[],0,0,2,0
2021,Unknown Title,"Body
Moscow: Sberbank has issued the following press release:
In the framework of the international forum Ethics of Artificial Intelligence: The Beginning of Trust, Russia ’ s largest tech companies adopted an AI code of ethics, which was drafted by the Artificial Intelligence Alliance. The document was signed by Sber, Yandex, MTS, VK, Gazprom Neft, and the Russian Direct Investment Fund (RDIF), as well as leading companies and research organizations. First Deputy Chief of Staff of the Presidential Executive Office Sergey Kiriyenko and Deputy Prime Minister Dmitry Chernyshenko were present at the signing.
The code of ethics enshrines a human-centric and humanistic approach to the development of AI technology, the principles of non-discrimination, data security, and information security, identification of AI in communication with humans, respect for human autonomy, and responsibility for the consequences of using AI. Commitment to the code of ethics will be a key social responsibility element for companies developing and implementing AI technology in Russia.
The AI Code of Ethics is to become a recommendation document on ethics for all AI market participants: government, business, Russian and foreign developers. The code establishes general ethical principles and behavioral standards, which the participants may choose to adhere to in the field of artificial intelligence.
As a tool for implementing the aforementioned principles and provisions, the code provides for the creation of a National AI Ethics Commission, the appointment of ethics commissioners, as well as the possibility of creating collegial industry ethics bodies. The Alliance will also develop methodological recommendations and a set of best and/or worst practices for dealing with emerging ethical issues in the AI life cycle.
Alexander Vedyakhin, first deputy chairman of the executive board, Sberbank:
“Many people consider AI technology to be controversial: on the one hand, it is a powerful driver of development for all of humanity; on the other hand, AI has a certain degree of autonomy in its decision-making, and it is not always obvious how these decisions are made. Today we have before us a great challenge – developing trust in AI technology. This is impossible without defining the rules for its ethical use. I am certain that the code we have adopted, in the drafting of which Sber has taken an active part, will be an important step in building trust in AI among our citizens and, as a result, in the development of the entire domestic artificial intelligence industry. ”
Vyacheslav Nikolaev, president, MTS:
“In the near future, it is solutions based on AI technology that will provide a real breakthrough in terms of the digitalization of healthcare and education and the realization of everyday tasks. At the same time, we can confidently say that AI is the technology that causes the most alarm in society. Now is the time to define the principles that will help us use AI technology with maximum benefit to society. At MTS, we have always paid particular attention to ethical issues in the development of products, including those that use artificial intelligence, and we are happy to share this experience during the preparation of the Russian AI Code of Ethics. ”
Anatoly Braverman, first deputy CEO, Russian Direct Investment Fund (RDIF):
“The shaping and development of the AI market requires effective interaction between its participants. The joint preparation, adoption, and future application of the AI Code of Ethics is an important stage in the formation of key infrastructure tools and the creation of success stories for developers, users, and investors in the field of artificial intelligence. Together with its co-investors and portfolio companies, RDIF has traditionally paid great attention to ethics and reputation issues, and it is ready to actively share its own best practices and experience and adopt those of others, too. ”
The document was prepared with the support of the Presidential Executive Office of the Russian Federation, the Analytical Center for the Government of the Russian Federation, and the Ministry of Economic Development. It underwent expert and public discussion on platforms hosted by Digital Economy, the Federation Council, and the Civic Chamber.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: IFN
Subject: ETHICS (97%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); BUSINESS NEWS (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (89%); BOARDS OF DIRECTORS (78%); BUSINESS ETHICS (78%); BUSINESS ETHICS & CORPORATE CITIZENSHIP (78%); FUNDS & INVESTMENT TRUSTS (78%); GOVERNMENT & PUBLIC ADMINISTRATION (75%); HEADS OF STATE & GOVERNMENT (75%); NEGATIVE SOCIETAL NEWS (74%); ESG FACTORS - SOCIAL (73%); PRESS RELEASES (73%); APPOINTMENTS (71%); ASSOCIATIONS & ORGANIZATIONS (71%); PRIME MINISTERS (70%)
Company:  OAO GAZPROM NEFT (72%);  SBERBANK ROSSII OAO (58%);  YANDEX NV (58%)
Ticker: SIBN (RTS) (72%); GAZ (LSE) (72%); SBER (RTS) (58%); SBER (LSE) (58%); YNDX (NASDAQ) (58%)
Industry: NAICS211130 NATURAL GAS EXTRACTION (72%); NAICS211120 CRUDE PETROLEUM EXTRACTION (72%); SIC1311 CRUDE PETROLEUM & NATURAL GAS (72%); NAICS522293 INTERNATIONAL TRADE FINANCING (58%); NAICS522110 COMMERCIAL BANKING (58%); SIC6082 FOREIGN TRADE & INTERNATIONAL BANKING INSTITUTIONS (58%); SIC6021 NATIONAL COMMERCIAL BANKS (58%); NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (58%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); BIG TECH (90%); INFORMATION TECHNOLOGY INDUSTRY (90%); DIGITALIZATION & DIGITAL TRANSFORMATION (78%); FUNDS & INVESTMENT TRUSTS (78%); DATA SECURITY (74%); INFORMATION SECURITY & PRIVACY (69%)
Geographic: MOSCOW, RUSSIAN FEDERATION (74%); RUSSIAN FEDERATION (90%)
Load-Date: October 29, 2021","Moscow: Sberbank has issued the following press release:
In the framework of the international forum Ethics of Artificial Intelligence: The Beginning of Trust, Russia ’ s largest tech companies adopted an AI code of ethics, which was drafted by the Artificial Intelligence Alliance. The document was signed by Sber, Yandex, MTS, VK, Gazprom Neft, and the Russian Direct Investment Fund (RDIF), as well as leading companies and research organizations. First Deputy Chief of Staff of the Presidential Executive Office Sergey Kiriyenko and Deputy Prime Minister Dmitry Chernyshenko were present at the signing.
The code of ethics enshrines a human-centric and humanistic approach to the development of AI technology, the principles of non-discrimination, data security, and information security, identification of AI in communication with humans, respect for human autonomy, and responsibility for the consequences of using AI. Commitment to the code of ethics will be a key social responsibility element for companies developing and implementing AI technology in Russia.
The AI Code of Ethics is to become a recommendation document on ethics for all AI market participants: government, business, Russian and foreign developers. The code establishes general ethical principles and behavioral standards, which the participants may choose to adhere to in the field of artificial intelligence.
As a tool for implementing the aforementioned principles and provisions, the code provides for the creation of a National AI Ethics Commission, the appointment of ethics commissioners, as well as the possibility of creating collegial industry ethics bodies. The Alliance will also develop methodological recommendations and a set of best and/or worst practices for dealing with emerging ethical issues in the AI life cycle.
Alexander Vedyakhin, first deputy chairman of the executive board, Sberbank:
“Many people consider AI technology to be controversial: on the one hand, it is a powerful driver of development for all of humanity; on the other hand, AI has a certain degree of autonomy in its decision-making, and it is not always obvious how these decisions are made. Today we have before us a great challenge – developing trust in AI technology. This is impossible without defining the rules for its ethical use. I am certain that the code we have adopted, in the drafting of which Sber has taken an active part, will be an important step in building trust in AI among our citizens and, as a result, in the development of the entire domestic artificial intelligence industry. ”
Vyacheslav Nikolaev, president, MTS:
“In the near future, it is solutions based on AI technology that will provide a real breakthrough in terms of the digitalization of healthcare and education and the realization of everyday tasks. At the same time, we can confidently say that AI is the technology that causes the most alarm in society. Now is the time to define the principles that will help us use AI technology with maximum benefit to society. At MTS, we have always paid particular attention to ethical issues in the development of products, including those that use artificial intelligence, and we are happy to share this experience during the preparation of the Russian AI Code of Ethics. ”
Anatoly Braverman, first deputy CEO, Russian Direct Investment Fund (RDIF):
“The shaping and development of the AI market requires effective interaction between its participants. The joint preparation, adoption, and future application of the AI Code of Ethics is an important stage in the formation of key infrastructure tools and the creation of success stories for developers, users, and investors in the field of artificial intelligence. Together with its co-investors and portfolio companies, RDIF has traditionally paid great attention to ethics and reputation issues, and it is ready to actively share its own best practices and experience and adopt those of others, too. ”
The document was prepared with the support of the Presidential Executive Office of the Russian Federation, the Analytical Center for the Government of the Russian Federation, and the Ministry of Economic Development. It underwent expert and public discussion on platforms hosted by Digital Economy, the Federation Council, and the Civic Chamber.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: IFN
Subject: ETHICS (97%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); BUSINESS NEWS (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (89%); BOARDS OF DIRECTORS (78%); BUSINESS ETHICS (78%); BUSINESS ETHICS & CORPORATE CITIZENSHIP (78%); FUNDS & INVESTMENT TRUSTS (78%); GOVERNMENT & PUBLIC ADMINISTRATION (75%); HEADS OF STATE & GOVERNMENT (75%); NEGATIVE SOCIETAL NEWS (74%); ESG FACTORS - SOCIAL (73%); PRESS RELEASES (73%); APPOINTMENTS (71%); ASSOCIATIONS & ORGANIZATIONS (71%); PRIME MINISTERS (70%)
Company:  OAO GAZPROM NEFT (72%);  SBERBANK ROSSII OAO (58%);  YANDEX NV (58%)
Ticker: SIBN (RTS) (72%); GAZ (LSE) (72%); SBER (RTS) (58%); SBER (LSE) (58%); YNDX (NASDAQ) (58%)
Industry: NAICS211130 NATURAL GAS EXTRACTION (72%); NAICS211120 CRUDE PETROLEUM EXTRACTION (72%); SIC1311 CRUDE PETROLEUM & NATURAL GAS (72%); NAICS522293 INTERNATIONAL TRADE FINANCING (58%); NAICS522110 COMMERCIAL BANKING (58%); SIC6082 FOREIGN TRADE & INTERNATIONAL BANKING INSTITUTIONS (58%); SIC6021 NATIONAL COMMERCIAL BANKS (58%); NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (58%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); BIG TECH (90%); INFORMATION TECHNOLOGY INDUSTRY (90%); DIGITALIZATION & DIGITAL TRANSFORMATION (78%); FUNDS & INVESTMENT TRUSTS (78%); DATA SECURITY (74%); INFORMATION SECURITY & PRIVACY (69%)
Geographic: MOSCOW, RUSSIAN FEDERATION (74%); RUSSIAN FEDERATION (90%)
Load-Date: October 29, 2021",neutral,0.827703058719635,balanced/neutral,"['privacy', 'discrimination', 'security', 'autonomy']",['autonomy'],"['standards', 'framework']",[],4,1,2,0
2021,Unknown Title,"Body
An article published in the Harvard Gazette mentions ethical challenges as Artificial Intelligence (AI) increasingly intervenes in decision-making. These decisions already cover various sectors and can affect people's economic, physical and psychological well-being. In fact, the European Union (EU) plans to fine companies up to 4% of their global annual turnover if their AI applications endanger people's security and fundamental rights and freedoms. Without a doubt, AI brings benefits in terms of efficiency and convenience. However, challenges arise when it comes to ethics, which is already complex, even in human interactions. Because of their cultural background, people can differ greatly in what is considered ethical. This begs the question: Who's ethics should reflect AI in the global marketplace to reduce potential harms and inequalities?
Some studies show that AI can be biased and even racist, leading to unethical decision-making. For example, AI computer vision is used to automatically tag images, with the ability to make a decision based on that tag. In the context of the current pandemic, a study showed that a Google computer vision service labeled as a ""weapon"" an image of a dark-skinned person holding a handheld thermometer. Instead, a similar image with a light-skinned person was labeled as an ""electronic device."" It is clear that the difference between a weapon and an electronic device can lead to erroneous profiling, labeling the dark-skinned person as dangerous. Now imagine that this AI-equipped software is used by law enforcement. Indeed, the dark-skinned person would be in unfair danger by holding a device as harmless as a thermometer. That kind of bias in AI is likely due to some socially and culturally ingrained unfair stereotypes that have been fed directly or indirectly into its algorithm. Therefore, if we want to create an ethical AI it is crucial to understand the link between culture and ethics.
Geert Hofstede describes culture as ""the collective programming of the mind that distinguishes members of one group or category of people from others."" Culture influences the way a person sees the world and can determine the perception of different ethical scenarios. [HG2] For example, making gifts among employees of companies is a very common and accepted practice in Chinese society. However, this practice may not be seen as ethical in other cultural contexts where such gifts may be seen as a form of bribery. If detected, should an AI notify and ""punish"" such practices? Now think even further. Imagine that an AI-powered military device has to decide who to kill or save. Needless to say, culturally induced ethical biases in such a scenario could lead to terrible results. This begs the question: Where do AI biases come from and how can we address this problem?
A recent article states that many AI systems are programmed and designed by people with similar training. For example, the article calls it Silicon Valley's ethical approach, in reference to the large number of tech companies that are based in this part of the world. The authors argue that many technology students can be trained with a singular approach to ethics that encompasses Western perspectives. Meanwhile, countries such as China, South Korea and Japan are investing heavily in the development of AI. Similarly, the AI developed in these nations can carry ethical biases based on their cultural frameworks. However, in a global market, an AI should be as ""free of prejudice"" as possible, as it can be applied beyond the borders of the country in which it was initially developed.
What should we do?
First, diversity and collaboration in AI programming will be critical. It is critical that key stakeholders in AI development around the world collaborate to ensure that key ethical biases are eliminated. This will be a challenge, as companies may not be willing to collaborate due to intellectual property protection. It is therefore important to have an international governing body for AI ethics that sets a global standard for AI developers to adhere to. This governing body should be composed of interdisciplinary teams that include specialists from fields such as computer science, philosophy, law, engineering, business, etc.
Secondly, people must act responsibly on the Internet. Some AI applications extract and learn directly from what is available on the Internet. For example, AI can process text, sound recordings, and images to learn what human behavior looks like. Therefore, we, as humanity, have to act responsibly in terms of what we post on the net to reduce prejudice, hatred and stereotypes. Like a child, if an AI feeds on racist messages on the Internet, it can develop racist tendencies. It is important that freedom of expression on the internet remains guaranteed, but priority must be given to ethical conduct.
These are just a few thoughts. The initial solutions may not be perfect, but addressing critically and raising awareness of these issues is already a big step towards a safer and more ethical future for all of us.
EntitiesHector GonzalezAssociate Professor Marketing in ESCP Business School Madrid       
Classification
Language: ENGLISH
Publication-Type: Newspaper
Journal Code: CENFENG
Subject: ARTIFICIAL INTELLIGENCE ETHICS (91%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); BUSINESS NEWS (90%); MACHINE VISION (88%); NEGATIVE SOCIETAL NEWS (78%); RACISM & XENOPHOBIA (78%); EUROPEAN UNION (77%); FINES & PENALTIES (77%); HUMAN RIGHTS & CIVIL LIBERTIES LAW (76%); NEGATIVE NEWS (76%); INTERNATIONAL ECONOMIC ORGANIZATIONS (74%); LAW ENFORCEMENT (71%); CRIME, LAW ENFORCEMENT & CORRECTIONS (63%)
Company: GOOGLE LLC (55%);  AI SYSTEMS (50%)
Organization:  EUROPEAN UNION (58%)
Industry: NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (55%); SIC7372 PREPACKAGED SOFTWARE (50%); ARTIFICIAL INTELLIGENCE ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); IMAGE PROCESSING & COMPUTER VISION (90%); MACHINE VISION (88%); COMPUTER SOFTWARE (76%); THERMAL SENSORS (70%)
Geographic: CHINA (79%); EUROPE (79%); EUROPEAN UNION MEMBER STATES (73%)
Load-Date: July 29, 2021","An article published in the Harvard Gazette mentions ethical challenges as Artificial Intelligence (AI) increasingly intervenes in decision-making. These decisions already cover various sectors and can affect people's economic, physical and psychological well-being. In fact, the European Union (EU) plans to fine companies up to 4% of their global annual turnover if their AI applications endanger people's security and fundamental rights and freedoms. Without a doubt, AI brings benefits in terms of efficiency and convenience. However, challenges arise when it comes to ethics, which is already complex, even in human interactions. Because of their cultural background, people can differ greatly in what is considered ethical. This begs the question: Who's ethics should reflect AI in the global marketplace to reduce potential harms and inequalities?
Some studies show that AI can be biased and even racist, leading to unethical decision-making. For example, AI computer vision is used to automatically tag images, with the ability to make a decision based on that tag. In the context of the current pandemic, a study showed that a Google computer vision service labeled as a ""weapon"" an image of a dark-skinned person holding a handheld thermometer. Instead, a similar image with a light-skinned person was labeled as an ""electronic device."" It is clear that the difference between a weapon and an electronic device can lead to erroneous profiling, labeling the dark-skinned person as dangerous. Now imagine that this AI-equipped software is used by law enforcement. Indeed, the dark-skinned person would be in unfair danger by holding a device as harmless as a thermometer. That kind of bias in AI is likely due to some socially and culturally ingrained unfair stereotypes that have been fed directly or indirectly into its algorithm. Therefore, if we want to create an ethical AI it is crucial to understand the link between culture and ethics.
Geert Hofstede describes culture as ""the collective programming of the mind that distinguishes members of one group or category of people from others."" Culture influences the way a person sees the world and can determine the perception of different ethical scenarios. [HG2] For example, making gifts among employees of companies is a very common and accepted practice in Chinese society. However, this practice may not be seen as ethical in other cultural contexts where such gifts may be seen as a form of bribery. If detected, should an AI notify and ""punish"" such practices? Now think even further. Imagine that an AI-powered military device has to decide who to kill or save. Needless to say, culturally induced ethical biases in such a scenario could lead to terrible results. This begs the question: Where do AI biases come from and how can we address this problem?
A recent article states that many AI systems are programmed and designed by people with similar training. For example, the article calls it Silicon Valley's ethical approach, in reference to the large number of tech companies that are based in this part of the world. The authors argue that many technology students can be trained with a singular approach to ethics that encompasses Western perspectives. Meanwhile, countries such as China, South Korea and Japan are investing heavily in the development of AI. Similarly, the AI developed in these nations can carry ethical biases based on their cultural frameworks. However, in a global market, an AI should be as ""free of prejudice"" as possible, as it can be applied beyond the borders of the country in which it was initially developed.
What should we do?
First, diversity and collaboration in AI programming will be critical. It is critical that key stakeholders in AI development around the world collaborate to ensure that key ethical biases are eliminated. This will be a challenge, as companies may not be willing to collaborate due to intellectual property protection. It is therefore important to have an international governing body for AI ethics that sets a global standard for AI developers to adhere to. This governing body should be composed of interdisciplinary teams that include specialists from fields such as computer science, philosophy, law, engineering, business, etc.
Secondly, people must act responsibly on the Internet. Some AI applications extract and learn directly from what is available on the Internet. For example, AI can process text, sound recordings, and images to learn what human behavior looks like. Therefore, we, as humanity, have to act responsibly in terms of what we post on the net to reduce prejudice, hatred and stereotypes. Like a child, if an AI feeds on racist messages on the Internet, it can develop racist tendencies. It is important that freedom of expression on the internet remains guaranteed, but priority must be given to ethical conduct.
These are just a few thoughts. The initial solutions may not be perfect, but addressing critically and raising awareness of these issues is already a big step towards a safer and more ethical future for all of us.
EntitiesHector GonzalezAssociate Professor Marketing in ESCP Business School Madrid       
Classification
Language: ENGLISH
Publication-Type: Newspaper
Journal Code: CENFENG
Subject: ARTIFICIAL INTELLIGENCE ETHICS (91%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); BUSINESS NEWS (90%); MACHINE VISION (88%); NEGATIVE SOCIETAL NEWS (78%); RACISM & XENOPHOBIA (78%); EUROPEAN UNION (77%); FINES & PENALTIES (77%); HUMAN RIGHTS & CIVIL LIBERTIES LAW (76%); NEGATIVE NEWS (76%); INTERNATIONAL ECONOMIC ORGANIZATIONS (74%); LAW ENFORCEMENT (71%); CRIME, LAW ENFORCEMENT & CORRECTIONS (63%)
Company: GOOGLE LLC (55%);  AI SYSTEMS (50%)
Organization:  EUROPEAN UNION (58%)
Industry: NAICS519130 INTERNET PUBLISHING & BROADCASTING & WEB SEARCH PORTALS (55%); SIC7372 PREPACKAGED SOFTWARE (50%); ARTIFICIAL INTELLIGENCE ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); IMAGE PROCESSING & COMPUTER VISION (90%); MACHINE VISION (88%); COMPUTER SOFTWARE (76%); THERMAL SENSORS (70%)
Geographic: CHINA (79%); EUROPE (79%); EUROPEAN UNION MEMBER STATES (73%)
Load-Date: July 29, 2021",neutral,0.6216887831687927,balanced/neutral,"['bias', 'security', 'human rights']",[],"['law', 'should', 'must']","['computer vision', 'algorithm']",3,0,3,2
2021,Unknown Title,"Byline: Express Computer
Body
An accelerated pace of digital transition, consumption of goods and services via app-based interface, and proliferation of data bring numerous risks such as biased decision-making processes being transferred to machines or algorithms development stage by humans
Deloitte Touche Tohmatsu India LLP (DTTILLP) together with the Bangalore Chamber of Commerce (BCIC) emphasised on an immediate need for India Inc. to introduce and adopt a ""Digital Ethics framework"" that would ensure a holistic view of ethics, and govern every digital intervention in the transformation journey of a business.
An accelerated pace of digital transition, consumption of goods and services via app-based interface, and proliferation of data bring numerous risks such as biased decision-making processes being transferred to machines or algorithms development stage by humans. These biases can be a threat to the reputation and trust towards stakeholders, as well as cause operational risks.
Digital Ethics: Ethical 'now' for a resilient 'next' is comprehensive thought leadership, highlighting five steps to a strategic mechanism on this phenomenon. These steps are built on a moral code of conduct, inclusive of digital policies that demarcate clear boundaries in human and technology intervention, whilst considering the consumer, corporate, societal, and national impact.
Speaking on the launch of the thought leadership, Vishal Jain, Partner, Deloitte India said, ""The pandemic compelled businesses and consumers to embrace digital technologies like artificial intelligence, big data, cloud, IoT and more in a big way. However, the need of the hour is to relook at the business operations layered on digital touchpoints with the lens of ethics, given biases might arise in the due course, owing to a faster response time to an issue.
Societal pressure to do ""the right thing"" now needs a carefully consideration of the trade-offs involved in the responsible usage of technology. Its interplay becomes vital to managing data privacy rights while actively adopting customer analytics for personalised service.
All these concerns have made it crucial to have an ethical framework that would ensure effective governance and risk mitigation aspects are in place.""
Manas Dasgupta, Chair of Young BCIC Expert Committee said, Tech is advancing at a neck-break speed. In fact, it is getting ahead of us so fast that we are grappling to assess the true abilities and what prudential norms are to be applied. Certain areas related to possible misuses of technologies such as privacy and security are fairly well-regulated both from legal as well as corporate governance aspects. However, inadvertent fallouts of technologies like autonomous machines that use AI / Robotics, etc. are yet to be fully understood.
It is the need of the hour that the Industries start meaningful conversations and note sharing around good governance on these technologies and ensure that we are within our limits to stay fair to everyone in the society, remain transparent and responsible in our Digital endeavors.""
These five steps include the following:
Committee: Formation of a committee which is a cross-functional team with business, technology, and community experts collaborating to address all ethical concerns. This committee should roll into the organisation's ethics committee, which would form the overall framework for ethics in the organisation. Digital Ethics: A draft the policy on Digital Ethics Adherence: Ensure adherence that all digital projects must be covered and assessed from the digital ethics perspective Emphasise: To emphasise and make ethics an important part of the digital governance of all projects. Education: Impart education on the need for the right ethics. Individuals involved must be assessed and reinforced with the knowledge from time to time.
The growth of digital media in India has largely been fueled by a moderate regulatory framework until now. Given the growing concerns around information and content available over digital platforms, digital ethics is a topic of concern not only for organizations but also for regulatory bodies and individuals for immediate consideration and deliberation.
Classification
Language: ENGLISH
Publication-Type: Magazine
Subject: ETHICS (92%); CONSUMPTION (90%); ARTIFICIAL INTELLIGENCE (89%); CORPORATE GOVERNANCE (89%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (89%); BUSINESS ANALYTICS (78%); COMPANY ACTIVITIES & MANAGEMENT (78%); CONSUMERS (78%); DATA ANALYTICS (78%); RISK MANAGEMENT (78%); BUSINESS & PROFESSIONAL ASSOCIATIONS (77%); TRADE DEVELOPMENT (77%); BUSINESS OPERATIONS (75%); CHAMBERS OF COMMERCE (72%); ESG FACTORS - GOVERNANCE (71%); ROBOTICS (64%)
Company:  DELOITTE LLP (84%);  DELOITTE TOUCHE TOHMATSU (72%)
Industry: NAICS541211 OFFICES OF CERTIFIED PUBLIC ACCOUNTANTS (84%); SIC8721 ACCOUNTING, AUDITING, & BOOKKEEPING SERVICES (84%); DIGITALIZATION & DIGITAL TRANSFORMATION (90%); ARTIFICIAL INTELLIGENCE (89%); INFORMATION SECURITY & PRIVACY (89%); BIG DATA (78%); BUSINESS ANALYTICS (78%); DATA ANALYTICS (78%); RISK MANAGEMENT (78%); INTERNET OF THINGS (71%); ROBOTICS (64%)
Geographic: BANGALORE, KARNATAKA, INDIA (58%); INDIA (90%)
Load-Date: April 8, 2021","An accelerated pace of digital transition, consumption of goods and services via app-based interface, and proliferation of data bring numerous risks such as biased decision-making processes being transferred to machines or algorithms development stage by humans
Deloitte Touche Tohmatsu India LLP (DTTILLP) together with the Bangalore Chamber of Commerce (BCIC) emphasised on an immediate need for India Inc. to introduce and adopt a ""Digital Ethics framework"" that would ensure a holistic view of ethics, and govern every digital intervention in the transformation journey of a business.
An accelerated pace of digital transition, consumption of goods and services via app-based interface, and proliferation of data bring numerous risks such as biased decision-making processes being transferred to machines or algorithms development stage by humans. These biases can be a threat to the reputation and trust towards stakeholders, as well as cause operational risks.
Digital Ethics: Ethical 'now' for a resilient 'next' is comprehensive thought leadership, highlighting five steps to a strategic mechanism on this phenomenon. These steps are built on a moral code of conduct, inclusive of digital policies that demarcate clear boundaries in human and technology intervention, whilst considering the consumer, corporate, societal, and national impact.
Speaking on the launch of the thought leadership, Vishal Jain, Partner, Deloitte India said, ""The pandemic compelled businesses and consumers to embrace digital technologies like artificial intelligence, big data, cloud, IoT and more in a big way. However, the need of the hour is to relook at the business operations layered on digital touchpoints with the lens of ethics, given biases might arise in the due course, owing to a faster response time to an issue.
Societal pressure to do ""the right thing"" now needs a carefully consideration of the trade-offs involved in the responsible usage of technology. Its interplay becomes vital to managing data privacy rights while actively adopting customer analytics for personalised service.
All these concerns have made it crucial to have an ethical framework that would ensure effective governance and risk mitigation aspects are in place.""
Manas Dasgupta, Chair of Young BCIC Expert Committee said, Tech is advancing at a neck-break speed. In fact, it is getting ahead of us so fast that we are grappling to assess the true abilities and what prudential norms are to be applied. Certain areas related to possible misuses of technologies such as privacy and security are fairly well-regulated both from legal as well as corporate governance aspects. However, inadvertent fallouts of technologies like autonomous machines that use AI / Robotics, etc. are yet to be fully understood.
It is the need of the hour that the Industries start meaningful conversations and note sharing around good governance on these technologies and ensure that we are within our limits to stay fair to everyone in the society, remain transparent and responsible in our Digital endeavors.""
These five steps include the following:
Committee: Formation of a committee which is a cross-functional team with business, technology, and community experts collaborating to address all ethical concerns. This committee should roll into the organisation's ethics committee, which would form the overall framework for ethics in the organisation. Digital Ethics: A draft the policy on Digital Ethics Adherence: Ensure adherence that all digital projects must be covered and assessed from the digital ethics perspective Emphasise: To emphasise and make ethics an important part of the digital governance of all projects. Education: Impart education on the need for the right ethics. Individuals involved must be assessed and reinforced with the knowledge from time to time.
The growth of digital media in India has largely been fueled by a moderate regulatory framework until now. Given the growing concerns around information and content available over digital platforms, digital ethics is a topic of concern not only for organizations but also for regulatory bodies and individuals for immediate consideration and deliberation.
Classification
Language: ENGLISH
Publication-Type: Magazine
Subject: ETHICS (92%); CONSUMPTION (90%); ARTIFICIAL INTELLIGENCE (89%); CORPORATE GOVERNANCE (89%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (89%); BUSINESS ANALYTICS (78%); COMPANY ACTIVITIES & MANAGEMENT (78%); CONSUMERS (78%); DATA ANALYTICS (78%); RISK MANAGEMENT (78%); BUSINESS & PROFESSIONAL ASSOCIATIONS (77%); TRADE DEVELOPMENT (77%); BUSINESS OPERATIONS (75%); CHAMBERS OF COMMERCE (72%); ESG FACTORS - GOVERNANCE (71%); ROBOTICS (64%)
Company:  DELOITTE LLP (84%);  DELOITTE TOUCHE TOHMATSU (72%)
Industry: NAICS541211 OFFICES OF CERTIFIED PUBLIC ACCOUNTANTS (84%); SIC8721 ACCOUNTING, AUDITING, & BOOKKEEPING SERVICES (84%); DIGITALIZATION & DIGITAL TRANSFORMATION (90%); ARTIFICIAL INTELLIGENCE (89%); INFORMATION SECURITY & PRIVACY (89%); BIG DATA (78%); BUSINESS ANALYTICS (78%); DATA ANALYTICS (78%); RISK MANAGEMENT (78%); INTERNET OF THINGS (71%); ROBOTICS (64%)
Geographic: BANGALORE, KARNATAKA, INDIA (58%); INDIA (90%)
Load-Date: April 8, 2021",neutral,0.725967526435852,balanced/neutral,"['privacy', 'security']",[],"['policy', 'governance', 'framework', 'should', 'must']",['robotics'],2,0,5,1
2021,Unknown Title,"Body
CHICAGO, Sept.  29, 2021  (GLOBE NEWSWIRE) -- The Organization for Ethical Source (OES), the nonprofit helping open source developers ensure that their work is being used for social good, today announced the release of the Hippocratic License 3.0 (HL3), a major revision of the preeminent ethical source license that specifically prohibits the use of open source software in violation of universal standards for human rights.
“Traditional open source is based on the premise that technology is neutral, and that absolute openness, ‘freedom zero’, is the only relevant ethical stance. But with the widespread adoption of open source by mega-corporations, governments, and the military, we’re seeing the limits of that kind of thinking,” said Coraline Ada Ehmke, co-founder of OES and creator of the Hippocratic License 1.0. “The OES continues to develop tools like the Hippocratic License to empower open source developers to take a more principled stance, based on the real-world impact of their work— not just their good intentions.”
Development of HL3 was led by Sameeul Haque, an IP and human rights attorney with OES partner Corporate Accountability Lab (CAL), in consultation with a special-purpose working group that included other ethical source license creators as well as open source maintainers.
“The position of many within the tech industry to keep it value-neutral, amoral, and apathetic does not immunize the industry from conversations about accountability and culpability,” said Sameeul Haque, staff attorney at CAL. “Hippocratic License 3.0 aims to confront the potential harms and abuses technology can have on people’s fundamental human rights.”
Strengthening the enforcement mechanism was a primary focus in the development of HL3, but one of its key innovations is its modularity. The core license provides protections for universally recognized human rights— including specific provisions for Indigenous rights— but also offers optional modules that focus on specific areas of concern, such as environmental justice or labor rights.
This modular approach, powered by a redesigned “license builder” website, empowers adopters to customize the Hippocratic License to reflect the needs and challenges of their particular communities. The site also provides plain-language explanations of each clause of the license, helping maintainers make informed decisions about adoption and customization.
“My experience working in open source ethical AI made apparent the need for deeper commitment and accountability around the impacts of the open source software we build,” said Maureen McElaney, OES working group member and Community Program Manager, IBM Quantum. “A license like this codifies this commitment to ethics and morality, bringing it beyond optics, and gives the contributors to these projects the peace of mind in knowing that their work will not be used to cause harm.”
“I came to the working group somewhat skeptical of the Hippocratic License, but I have become increasingly attuned to the benefits this license can have,” said Mike Nolan, member of the OES working group and assistant director Open@RIT (Rochester Institute of Technology). “It is a symbol and tool for workers to demand that our organizations and businesses build these guarantees into how our work is used.”
To learn more about the Hippocratic License visit https://firstdonoharm.dev/.
About The Organization for Ethical SourceThe Organization for Ethical Source (OES) is a global community of open source developers, legal specialists, non-profit leaders, activists, and human rights workers, collaborating on tools and strategies to promote more just and equitable outcomes for everyone who contributes to, benefits from, or is impacted by open source technologies.
To learn more about the Organization for Ethical Source, visit https://ethicalsource.dev. You can help us transform tech culture by joining our global, multidisciplinary community at https://ethicalsource.dev/join.
Melissa Loganmelissa@constantia.io
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ASSOCIATIONS & ORGANIZATIONS (91%); ETHICS (90%); HUMAN RIGHTS & CIVIL LIBERTIES LAW (90%); PRESS RELEASES (90%); SOFTWARE DEVELOPERS (90%); HUMAN RIGHTS (89%); NEW PRODUCTS (89%); NONPROFIT ORGANIZATIONS (89%); LABOR REGULATION & POLICY (79%); LAWYERS (79%); NEGATIVE NEWS (76%); LAW & LEGAL SYSTEM (74%); ARTIFICIAL INTELLIGENCE ETHICS (73%); MANAGERS & SUPERVISORS (64%); ENVIRONMENTAL JUSTICE (63%); ENVIRONMENT & NATURAL RESOURCES (50%); ETHICAL SOURCE (%); HIPPOCRATIC LICENSE (%)
Company: THE ORGANIZATION FOR ETHICAL SOURCE
Industry: OPEN SOURCE SOFTWARE (92%); SOFTWARE DEVELOPERS (90%); NEW PRODUCTS (89%); LAWYERS (79%); INFORMATION TECHNOLOGY INDUSTRY (78%); WEBSITE REDESIGN (77%); ARTIFICIAL INTELLIGENCE ETHICS (73%); COMPUTER SOFTWARE (73%); Software (%)
Company-Terms: Software The Organization for Ethical Source Geneva ::issuer-country=CH:: CH
Load-Date: September 29, 2021","CHICAGO, Sept.  29, 2021  (GLOBE NEWSWIRE) -- The Organization for Ethical Source (OES), the nonprofit helping open source developers ensure that their work is being used for social good, today announced the release of the Hippocratic License 3.0 (HL3), a major revision of the preeminent ethical source license that specifically prohibits the use of open source software in violation of universal standards for human rights.
“Traditional open source is based on the premise that technology is neutral, and that absolute openness, ‘freedom zero’, is the only relevant ethical stance. But with the widespread adoption of open source by mega-corporations, governments, and the military, we’re seeing the limits of that kind of thinking,” said Coraline Ada Ehmke, co-founder of OES and creator of the Hippocratic License 1.0. “The OES continues to develop tools like the Hippocratic License to empower open source developers to take a more principled stance, based on the real-world impact of their work— not just their good intentions.”
Development of HL3 was led by Sameeul Haque, an IP and human rights attorney with OES partner Corporate Accountability Lab (CAL), in consultation with a special-purpose working group that included other ethical source license creators as well as open source maintainers.
“The position of many within the tech industry to keep it value-neutral, amoral, and apathetic does not immunize the industry from conversations about accountability and culpability,” said Sameeul Haque, staff attorney at CAL. “Hippocratic License 3.0 aims to confront the potential harms and abuses technology can have on people’s fundamental human rights.”
Strengthening the enforcement mechanism was a primary focus in the development of HL3, but one of its key innovations is its modularity. The core license provides protections for universally recognized human rights— including specific provisions for Indigenous rights— but also offers optional modules that focus on specific areas of concern, such as environmental justice or labor rights.
This modular approach, powered by a redesigned “license builder” website, empowers adopters to customize the Hippocratic License to reflect the needs and challenges of their particular communities. The site also provides plain-language explanations of each clause of the license, helping maintainers make informed decisions about adoption and customization.
“My experience working in open source ethical AI made apparent the need for deeper commitment and accountability around the impacts of the open source software we build,” said Maureen McElaney, OES working group member and Community Program Manager, IBM Quantum. “A license like this codifies this commitment to ethics and morality, bringing it beyond optics, and gives the contributors to these projects the peace of mind in knowing that their work will not be used to cause harm.”
“I came to the working group somewhat skeptical of the Hippocratic License, but I have become increasingly attuned to the benefits this license can have,” said Mike Nolan, member of the OES working group and assistant director Open@RIT (Rochester Institute of Technology). “It is a symbol and tool for workers to demand that our organizations and businesses build these guarantees into how our work is used.”
To learn more about the Hippocratic License visit https://firstdonoharm.dev/.
About The Organization for Ethical SourceThe Organization for Ethical Source (OES) is a global community of open source developers, legal specialists, non-profit leaders, activists, and human rights workers, collaborating on tools and strategies to promote more just and equitable outcomes for everyone who contributes to, benefits from, or is impacted by open source technologies.
To learn more about the Organization for Ethical Source, visit https://ethicalsource.dev. You can help us transform tech culture by joining our global, multidisciplinary community at https://ethicalsource.dev/join.
Melissa Loganmelissa@constantia.io
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ASSOCIATIONS & ORGANIZATIONS (91%); ETHICS (90%); HUMAN RIGHTS & CIVIL LIBERTIES LAW (90%); PRESS RELEASES (90%); SOFTWARE DEVELOPERS (90%); HUMAN RIGHTS (89%); NEW PRODUCTS (89%); NONPROFIT ORGANIZATIONS (89%); LABOR REGULATION & POLICY (79%); LAWYERS (79%); NEGATIVE NEWS (76%); LAW & LEGAL SYSTEM (74%); ARTIFICIAL INTELLIGENCE ETHICS (73%); MANAGERS & SUPERVISORS (64%); ENVIRONMENTAL JUSTICE (63%); ENVIRONMENT & NATURAL RESOURCES (50%); ETHICAL SOURCE (%); HIPPOCRATIC LICENSE (%)
Company: THE ORGANIZATION FOR ETHICAL SOURCE
Industry: OPEN SOURCE SOFTWARE (92%); SOFTWARE DEVELOPERS (90%); NEW PRODUCTS (89%); LAWYERS (79%); INFORMATION TECHNOLOGY INDUSTRY (78%); WEBSITE REDESIGN (77%); ARTIFICIAL INTELLIGENCE ETHICS (73%); COMPUTER SOFTWARE (73%); Software (%)
Company-Terms: Software The Organization for Ethical Source Geneva ::issuer-country=CH:: CH
Load-Date: September 29, 2021",neutral,0.7516695261001587,balanced/neutral,"['accountability', 'human rights']","['justice', 'justice']","['regulation', 'policy', 'standards', 'law']",[],2,2,4,0
2021,Unknown Title,"Body
Ethical considerations will go into Artificial Intelligence (AI)s design and employment because it enables autonomy, decision making and system execution at incredibly fast speeds, a DoD official said.
These ethical principles build on the department's long history of ethical adoption of new technologies and rules of engagement and warfare, Alka Patel, head of artificial intelligence ethics policy at the Joint Artificial Intelligence Center, said today at the Defense One Genius Machines 2021 virtual summit.
The Defense Innovation Board spent months developing the principles and consulted with leading AI and technical experts, as well as current and former DoD leaders and the American public, she said.
According to a Pentagon statement, those principles state:
DoD personnel will exercise appropriate levels of judgment and care while remaining responsible for the development, deployment and use of AI capabilities.
The department will take deliberate steps to minimize unintended bias in AI capabilities.
The department's AI capabilities will be developed and deployed such that relevant personnel possess an appropriate understanding of the technology, development processes and operational methods, including transparent and auditable methodologies, data sources and design procedures and documentation.
The department's AI capabilities will have explicit, well-defined uses, and the safety, security and effectiveness of such capabilities will be subject to testing and assurance within those defined uses across their life cycles.
The department will design and engineer AI capabilities: (1) to fulfill their intended functions while possessing the ability to detect and avoid unintended consequences, and (2) to disengage or deactivate deployed systems that demonstrate unintended behavior.
Now that the principles have been established, the department is going about operationalizing them to AI applications across the services, she said, adding that it's a tall but necessary order. Training and education of ethical AI practices across the department also goes hand-in-hand with that.
Another important part of AI ethics is sharing the conversation with allies and partners, so that everyone is on the same page when it comes to how ethics comes to play in interoperability, she said.
Sharing the conversation with corporations that are helping the department in its AI efforts is also important, Patel said. Companies and their employees need assurance that what they are helping to build will be used in an ethically responsible manner and, in turn, that they need to build it to ethical standards. 2021 Global Data Point.
Classification
Language: ENGLISH
Publication-Type: Magazine
Journal Code: 1791
Subject: ETHICS (95%); ARTIFICIAL INTELLIGENCE (90%); REGULATORY COMPLIANCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (89%); EMERGING TECHNOLOGY (78%); LAW OF WAR (73%); UNCONSCIOUS BIAS (68%); SAFETY (66%)
Industry: ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (89%)
Geographic: UNITED STATES (79%)
Load-Date: January 22, 2021","Ethical considerations will go into Artificial Intelligence (AI)s design and employment because it enables autonomy, decision making and system execution at incredibly fast speeds, a DoD official said.
These ethical principles build on the department's long history of ethical adoption of new technologies and rules of engagement and warfare, Alka Patel, head of artificial intelligence ethics policy at the Joint Artificial Intelligence Center, said today at the Defense One Genius Machines 2021 virtual summit.
The Defense Innovation Board spent months developing the principles and consulted with leading AI and technical experts, as well as current and former DoD leaders and the American public, she said.
According to a Pentagon statement, those principles state:
DoD personnel will exercise appropriate levels of judgment and care while remaining responsible for the development, deployment and use of AI capabilities.
The department will take deliberate steps to minimize unintended bias in AI capabilities.
The department's AI capabilities will be developed and deployed such that relevant personnel possess an appropriate understanding of the technology, development processes and operational methods, including transparent and auditable methodologies, data sources and design procedures and documentation.
The department's AI capabilities will have explicit, well-defined uses, and the safety, security and effectiveness of such capabilities will be subject to testing and assurance within those defined uses across their life cycles.
The department will design and engineer AI capabilities: (1) to fulfill their intended functions while possessing the ability to detect and avoid unintended consequences, and (2) to disengage or deactivate deployed systems that demonstrate unintended behavior.
Now that the principles have been established, the department is going about operationalizing them to AI applications across the services, she said, adding that it's a tall but necessary order. Training and education of ethical AI practices across the department also goes hand-in-hand with that.
Another important part of AI ethics is sharing the conversation with allies and partners, so that everyone is on the same page when it comes to how ethics comes to play in interoperability, she said.
Sharing the conversation with corporations that are helping the department in its AI efforts is also important, Patel said. Companies and their employees need assurance that what they are helping to build will be used in an ethically responsible manner and, in turn, that they need to build it to ethical standards. 2021 Global Data Point.
Classification
Language: ENGLISH
Publication-Type: Magazine
Journal Code: 1791
Subject: ETHICS (95%); ARTIFICIAL INTELLIGENCE (90%); REGULATORY COMPLIANCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (89%); EMERGING TECHNOLOGY (78%); LAW OF WAR (73%); UNCONSCIOUS BIAS (68%); SAFETY (66%)
Industry: ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (89%)
Geographic: UNITED STATES (79%)
Load-Date: January 22, 2021",positive,0.4999837875366211,balanced/neutral,"['bias', 'safety', 'security', 'autonomy']",['autonomy'],"['policy', 'standards', 'law', 'compliance', 'need to']",[],4,1,5,0
2021,Unknown Title,"Byline: Society of Corporate Compliance and Ethics
Body
Mar 10, 2021( JD Supra: http://www.jdsupra.com Delivered by Newstex)  It's not everyday that a Chief Constable joins the Society of Corporate Compliance and Ethics, let alone one who is the United Kingdom's police national lead for ethics. So when Richard Lewis signed up we invited him to sit down for a podcast, and he graciously said yes. 
In this conversation he explains the structure of policing in the UK and its ethics function. Each of the 43 police agencies, he told me, has its own ethics Seemore+ It's not everyday that a Chief Constable joins the Society of Corporate Compliance and Ethics, let alone one who is the United Kingdom's police national lead for ethics. So when Richard Lewis signed up we invited him to sit down for a podcast, and he graciously said yes. In this conversation he explains the structure of policing in the UK and its ethics function. Each of the 43 police agencies, he told me, has its own ethics committee. Above them are four regional committees, and above them the National ethics Committee which he chairs and meets quarterly. To make the discussion robust and not an 'echo chamber' the National Ethics Committee includes police but also individuals from business and academia.Their mission is not to enforce ethics but to determine best practices that can be applied. In addition, they explore the ethics issues facing the police including the Black Lives Matter movement, #MeToo, abuse of authority and, as he put it both what they can and can't do as well as what they should and shouldn't do.Another area of discussion is digital ethics. Technology such as facial recognition software and AI are evolving faster than the ethical frameworks.In this conversation we also explore managing stress in these difficult jobs. One important strategy for maintain mental health: having an adequate work-life balance. It's a goal that has grown more difficult to achieve in this always-connected world, made worse by the fact that so many people can't escape an office that is now in a corner of their home. Seeless- 
Classification
Language: English
Publication-Type: Web Blog
Journal Code: JSUP-5263
Subject: ETHICS (92%); BUSINESS ETHICS (90%); POLICE FORCES (89%); BEST PRACTICES (73%); METOO MOVEMENT (64%); IDENTIFICATION TECHNOLOGIES (62%); MENTAL HEALTH (61%); WORK-LIFE BALANCE (61%); BLACK LIVES MATTER (50%)
Industry: PODCASTING (78%); PATTERN RECOGNITION (62%)
Geographic: UNITED KINGDOM (90%)
Load-Date: March 10, 2021","Mar 10, 2021( JD Supra: http://www.jdsupra.com Delivered by Newstex)  It's not everyday that a Chief Constable joins the Society of Corporate Compliance and Ethics, let alone one who is the United Kingdom's police national lead for ethics. So when Richard Lewis signed up we invited him to sit down for a podcast, and he graciously said yes. 
In this conversation he explains the structure of policing in the UK and its ethics function. Each of the 43 police agencies, he told me, has its own ethics Seemore+ It's not everyday that a Chief Constable joins the Society of Corporate Compliance and Ethics, let alone one who is the United Kingdom's police national lead for ethics. So when Richard Lewis signed up we invited him to sit down for a podcast, and he graciously said yes. In this conversation he explains the structure of policing in the UK and its ethics function. Each of the 43 police agencies, he told me, has its own ethics committee. Above them are four regional committees, and above them the National ethics Committee which he chairs and meets quarterly. To make the discussion robust and not an 'echo chamber' the National Ethics Committee includes police but also individuals from business and academia.Their mission is not to enforce ethics but to determine best practices that can be applied. In addition, they explore the ethics issues facing the police including the Black Lives Matter movement, #MeToo, abuse of authority and, as he put it both what they can and can't do as well as what they should and shouldn't do.Another area of discussion is digital ethics. Technology such as facial recognition software and AI are evolving faster than the ethical frameworks.In this conversation we also explore managing stress in these difficult jobs. One important strategy for maintain mental health: having an adequate work-life balance. It's a goal that has grown more difficult to achieve in this always-connected world, made worse by the fact that so many people can't escape an office that is now in a corner of their home. Seeless- 
Classification
Language: English
Publication-Type: Web Blog
Journal Code: JSUP-5263
Subject: ETHICS (92%); BUSINESS ETHICS (90%); POLICE FORCES (89%); BEST PRACTICES (73%); METOO MOVEMENT (64%); IDENTIFICATION TECHNOLOGIES (62%); MENTAL HEALTH (61%); WORK-LIFE BALANCE (61%); BLACK LIVES MATTER (50%)
Industry: PODCASTING (78%); PATTERN RECOGNITION (62%)
Geographic: UNITED KINGDOM (90%)
Load-Date: March 10, 2021",neutral,0.7983285188674927,balanced/neutral,[],[],"['compliance', 'should']",['facial recognition'],0,0,2,1
2021,Unknown Title,"Body
Moscow: Sberbank has issued the following press release:
A meeting of the working group on AI ethics has been held at Sber, chaired by Alexander Vedyakhin, first deputy chairman of the Sberbank Executive Board.
The working group was created to implement Sber's ethical principles for AI, approved in late 2020, and was one of the first corporate bodies of its kind in Russia. Its members include leading Sber experts in this area, including top managers from the bank and the ecosystem. The group will establish positions on the main ethical dilemmas that arise at all stages of the AI system life cycle.
Alexander Vedyakhin, first deputy chairman of the executive board, Sberbank:
“Sber pays great attention to the ethical aspects of the development of AI technology. We were among the first in the country to set up a special permanent working group to address these issues. Through the working group, we plan to consistently examine the ethical aspects of using AI technology in the work of all the bank's structural divisions. We believe that this is the appropriate model for dealing responsibly with ethical issues related to new technologies. ”
Ethical standards for AI are important not only for the business community and experts, but also for the security of countries and the well-being of humanity as a whole. The necessity of forming positions on sensitive issues in AI has been recognized at the international level, with credible organizations such as UNESCO and the Council of Europe addressing these issues. Over 30 companies around the world have adopted their own ethical principles for AI. To ensure that technology that is important for the development of humanity are not banned, there must be a considered position on ethical issues in AI. This is precisely why companies are setting up working bodies to advise executives on AI issues.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: IFN
Subject: ETHICS (95%); ARTIFICIAL INTELLIGENCE ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); BOARDS OF DIRECTORS (90%); BUSINESS NEWS (90%); ASSOCIATIONS & ORGANIZATIONS (78%); MANAGERS & SUPERVISORS (73%); PRESS RELEASES (73%)
Company:  SBERBANK ROSSII OAO (90%);  AI SYSTEMS (56%)
Organization: COUNCIL OF EUROPE (52%)
Ticker: SBER (RTS) (90%); SBER (LSE) (90%)
Industry: NAICS522293 INTERNATIONAL TRADE FINANCING (90%); NAICS522110 COMMERCIAL BANKING (90%); SIC6082 FOREIGN TRADE & INTERNATIONAL BANKING INSTITUTIONS (90%); SIC6021 NATIONAL COMMERCIAL BANKS (90%); SIC7372 PREPACKAGED SOFTWARE (56%); ARTIFICIAL INTELLIGENCE ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%)
Geographic: MOSCOW, RUSSIAN FEDERATION (74%); EUROPE (79%); RUSSIAN FEDERATION (59%)
Load-Date: October 14, 2021","Moscow: Sberbank has issued the following press release:
A meeting of the working group on AI ethics has been held at Sber, chaired by Alexander Vedyakhin, first deputy chairman of the Sberbank Executive Board.
The working group was created to implement Sber's ethical principles for AI, approved in late 2020, and was one of the first corporate bodies of its kind in Russia. Its members include leading Sber experts in this area, including top managers from the bank and the ecosystem. The group will establish positions on the main ethical dilemmas that arise at all stages of the AI system life cycle.
Alexander Vedyakhin, first deputy chairman of the executive board, Sberbank:
“Sber pays great attention to the ethical aspects of the development of AI technology. We were among the first in the country to set up a special permanent working group to address these issues. Through the working group, we plan to consistently examine the ethical aspects of using AI technology in the work of all the bank's structural divisions. We believe that this is the appropriate model for dealing responsibly with ethical issues related to new technologies. ”
Ethical standards for AI are important not only for the business community and experts, but also for the security of countries and the well-being of humanity as a whole. The necessity of forming positions on sensitive issues in AI has been recognized at the international level, with credible organizations such as UNESCO and the Council of Europe addressing these issues. Over 30 companies around the world have adopted their own ethical principles for AI. To ensure that technology that is important for the development of humanity are not banned, there must be a considered position on ethical issues in AI. This is precisely why companies are setting up working bodies to advise executives on AI issues.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: IFN
Subject: ETHICS (95%); ARTIFICIAL INTELLIGENCE ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); BOARDS OF DIRECTORS (90%); BUSINESS NEWS (90%); ASSOCIATIONS & ORGANIZATIONS (78%); MANAGERS & SUPERVISORS (73%); PRESS RELEASES (73%)
Company:  SBERBANK ROSSII OAO (90%);  AI SYSTEMS (56%)
Organization: COUNCIL OF EUROPE (52%)
Ticker: SBER (RTS) (90%); SBER (LSE) (90%)
Industry: NAICS522293 INTERNATIONAL TRADE FINANCING (90%); NAICS522110 COMMERCIAL BANKING (90%); SIC6082 FOREIGN TRADE & INTERNATIONAL BANKING INSTITUTIONS (90%); SIC6021 NATIONAL COMMERCIAL BANKS (90%); SIC7372 PREPACKAGED SOFTWARE (56%); ARTIFICIAL INTELLIGENCE ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%)
Geographic: MOSCOW, RUSSIAN FEDERATION (74%); EUROPE (79%); RUSSIAN FEDERATION (59%)
Load-Date: October 14, 2021",neutral,0.8429315090179443,balanced/neutral,['security'],[],"['standards', 'must']",[],1,0,2,0
2021,Unknown Title,"Body
Washington: U.S Department of Defense has issued the following news release:
Artificial intelligence is one of the Defense Department's top technology modernization priorities.
Because AI enables autonomy, decision making and system execution at incredibly fast speeds, department officials felt strongly that ethical considerations should go into its design and employment, Alka Patel, head of artificial intelligence ethics policy at the Joint Artificial Intelligence Center, said today at the Defense One Genius Machines 2021 virtual summit.
These ethical principles build on the department's long history of ethical adoption of new technologies and rules of engagement and warfare, she added.
The Defense Innovation Board spent months developing the principles and consulted with leading AI and technical experts, as well as current and former DOD leaders and the American public, she said.
Spotlight: Artificial Intelligence
Those principles state:
DOD personnel will exercise appropriate levels of judgment and care while remaining responsible for the development, deployment and use of AI capabilities.
The department will take deliberate steps to minimize unintended bias in AI capabilities.
The department's AI capabilities will be developed and deployed such that relevant personnel possess an appropriate understanding of the technology, development processes and operational methods, including transparent and auditable methodologies, data sources and design procedures and documentation.
The department's AI capabilities will have explicit, well-defined uses, and the safety, security and effectiveness of such capabilities will be subject to testing and assurance within those defined uses across their life cycles.
The department will design and engineer AI capabilities: (1) to fulfill their intended functions while possessing the ability to detect and avoid unintended consequences, and (2) to disengage or deactivate deployed systems that demonstrate unintended behavior.
Now that the principles have been established, the department is going about operationalizing them to AI applications across the services, she said, adding that it's a tall but necessary order. Training and education of ethical AI practices across the department also goes hand-in-hand with that.
Another important part of AI ethics is sharing the conversation with allies and partners, so that everyone is on the same page when it comes to how ethics comes to play in interoperability, she said.
Sharing the conversation with corporations that are helping the department in its AI efforts is also important, Patel said. Companies and their employees need assurance that what they are helping to build will be used in an ethically responsible manner and, in turn, that they need to build it to ethical standards.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: INS
Subject: ETHICS (93%); DEFENSE DEPARTMENTS (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ENGINEERING (90%); EMERGING TECHNOLOGY (78%); LAW OF WAR (78%); UNCONSCIOUS BIAS (67%); SAFETY (65%)
Industry: DEFENSE DEPARTMENTS (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ENGINEERING (90%)
Geographic: UNITED STATES (93%)
Load-Date: January 22, 2021","Washington: U.S Department of Defense has issued the following news release:
Artificial intelligence is one of the Defense Department's top technology modernization priorities.
Because AI enables autonomy, decision making and system execution at incredibly fast speeds, department officials felt strongly that ethical considerations should go into its design and employment, Alka Patel, head of artificial intelligence ethics policy at the Joint Artificial Intelligence Center, said today at the Defense One Genius Machines 2021 virtual summit.
These ethical principles build on the department's long history of ethical adoption of new technologies and rules of engagement and warfare, she added.
The Defense Innovation Board spent months developing the principles and consulted with leading AI and technical experts, as well as current and former DOD leaders and the American public, she said.
Spotlight: Artificial Intelligence
Those principles state:
DOD personnel will exercise appropriate levels of judgment and care while remaining responsible for the development, deployment and use of AI capabilities.
The department will take deliberate steps to minimize unintended bias in AI capabilities.
The department's AI capabilities will be developed and deployed such that relevant personnel possess an appropriate understanding of the technology, development processes and operational methods, including transparent and auditable methodologies, data sources and design procedures and documentation.
The department's AI capabilities will have explicit, well-defined uses, and the safety, security and effectiveness of such capabilities will be subject to testing and assurance within those defined uses across their life cycles.
The department will design and engineer AI capabilities: (1) to fulfill their intended functions while possessing the ability to detect and avoid unintended consequences, and (2) to disengage or deactivate deployed systems that demonstrate unintended behavior.
Now that the principles have been established, the department is going about operationalizing them to AI applications across the services, she said, adding that it's a tall but necessary order. Training and education of ethical AI practices across the department also goes hand-in-hand with that.
Another important part of AI ethics is sharing the conversation with allies and partners, so that everyone is on the same page when it comes to how ethics comes to play in interoperability, she said.
Sharing the conversation with corporations that are helping the department in its AI efforts is also important, Patel said. Companies and their employees need assurance that what they are helping to build will be used in an ethically responsible manner and, in turn, that they need to build it to ethical standards.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: INS
Subject: ETHICS (93%); DEFENSE DEPARTMENTS (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ENGINEERING (90%); EMERGING TECHNOLOGY (78%); LAW OF WAR (78%); UNCONSCIOUS BIAS (67%); SAFETY (65%)
Industry: DEFENSE DEPARTMENTS (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ENGINEERING (90%)
Geographic: UNITED STATES (93%)
Load-Date: January 22, 2021",neutral,0.5506268739700317,balanced/neutral,"['bias', 'safety', 'security', 'autonomy']",['autonomy'],"['policy', 'standards', 'law', 'should', 'need to']",[],4,1,5,0
2021,Unknown Title,"Byline: EU Reporter Correspondent
Body
Jul 09, 2021( EU Reporter: http://www.eureporter.co Delivered by Newstex)  
 On 8 July, the Commission held the first meeting of the expert group on Artificial Intelligence (AI) and data in education and training[1]. The expert group is part of the Digital Education Action Plan (2021-2027)[2], which will further promote understanding of the use of emerging technologies and raise awareness about the opportunities and risks of using AI and data in education and training. The 25 experts, selected via an open call, are to prepare ethical guidelines on AI and data targeting specifically the education and training sector. Acknowledging the potential and risks of AI technologies and data, the group will tackle challenges related to non-discrimination as well as ethical, security, and privacy concerns.  
It will also address the pressing need for educators and students to have a basic understanding of AI and data usage to engage positively, critically, and ethically with this technology. Mariya Gabriel, Commissioner for Innovation, Research, Culture, Education and Youth, said: 'Artificial intelligence and learning analytics are game-changing technologies. They are revolutionising the way students learn. At the same time, many educators, parents, and students are understandably worried about who collects, controls, and interprets the data generated about them. This is where our new expert group comes in: their work will be instrumental to prepare practical ethical guidelines for educators, addressing for example biases in decision-making.  
""The meeting was an important step towards implementing our Digital Education Action Plan - together we will ensure that AI meets real educational needs and is used safely and ethically by learners and educators across Europe.'  
The meeting was the first of four to take place over the next 12 months. The guidelines, to be presented in September 2022, will be accompanied by a training programme for researchers and students on the ethical aspects of AI, and include a target of 45% of female participation in activities. The group will also make sure that the guidelines take into account the Commission's April 2021 proposal for AI legal framework and new Co-ordinated Plan with member states[3]. Information about the launch and the work programme of the expert group is available online[4], further information on AI and education is available here[5]. 
 [ 1]: https://bit.ly/3jT5Nb5 [ 2]: https://ec.europa.eu/education/education-in-the-eu/digital-education-action-plan_en [ 3]: https://ec.europa.eu/commission/presscorner/detail/en/IP_21_1682 [ 4]: https://bit.ly/3jT5Nb5 [ 5]: https://knowledge4policy.ec.europa.eu/ai-watch/topic/education-skills_en 
Classification
Language: English
Publication-Type: Web Blog
Journal Code: EURP-134582
Subject: ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); EDUCATION & TRAINING (90%); STUDENTS & STUDENT LIFE (90%); EUROPEAN UNION (89%); TALKS & MEETINGS (89%); BLOGS & MESSAGE BOARDS (78%); DATA ANALYTICS (78%); EMERGING TECHNOLOGY (78%); Digital Society (%); Artificial intelligence (%); Digital technology (%); European Commission (%); Internet (%); artificial intelligence (%)
Organization: EUROPEAN COMMISSION (59%)
Industry: ARTIFICIAL INTELLIGENCE (90%); BLOGS & MESSAGE BOARDS (78%); DATA ANALYTICS (78%)
Geographic: EUROPE (88%); EUROPEAN UNION MEMBER STATES (79%)
Load-Date: July 9, 2021","Jul 09, 2021( EU Reporter: http://www.eureporter.co Delivered by Newstex)  
 On 8 July, the Commission held the first meeting of the expert group on Artificial Intelligence (AI) and data in education and training[1]. The expert group is part of the Digital Education Action Plan (2021-2027)[2], which will further promote understanding of the use of emerging technologies and raise awareness about the opportunities and risks of using AI and data in education and training. The 25 experts, selected via an open call, are to prepare ethical guidelines on AI and data targeting specifically the education and training sector. Acknowledging the potential and risks of AI technologies and data, the group will tackle challenges related to non-discrimination as well as ethical, security, and privacy concerns.  
It will also address the pressing need for educators and students to have a basic understanding of AI and data usage to engage positively, critically, and ethically with this technology. Mariya Gabriel, Commissioner for Innovation, Research, Culture, Education and Youth, said: 'Artificial intelligence and learning analytics are game-changing technologies. They are revolutionising the way students learn. At the same time, many educators, parents, and students are understandably worried about who collects, controls, and interprets the data generated about them. This is where our new expert group comes in: their work will be instrumental to prepare practical ethical guidelines for educators, addressing for example biases in decision-making.  
""The meeting was an important step towards implementing our Digital Education Action Plan - together we will ensure that AI meets real educational needs and is used safely and ethically by learners and educators across Europe.'  
The meeting was the first of four to take place over the next 12 months. The guidelines, to be presented in September 2022, will be accompanied by a training programme for researchers and students on the ethical aspects of AI, and include a target of 45% of female participation in activities. The group will also make sure that the guidelines take into account the Commission's April 2021 proposal for AI legal framework and new Co-ordinated Plan with member states[3]. Information about the launch and the work programme of the expert group is available online[4], further information on AI and education is available here[5]. 
 [ 1]: https://bit.ly/3jT5Nb5 [ 2]: https://ec.europa.eu/education/education-in-the-eu/digital-education-action-plan_en [ 3]: https://ec.europa.eu/commission/presscorner/detail/en/IP_21_1682 [ 4]: https://bit.ly/3jT5Nb5 [ 5]: https://knowledge4policy.ec.europa.eu/ai-watch/topic/education-skills_en 
Classification
Language: English
Publication-Type: Web Blog
Journal Code: EURP-134582
Subject: ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); EDUCATION & TRAINING (90%); STUDENTS & STUDENT LIFE (90%); EUROPEAN UNION (89%); TALKS & MEETINGS (89%); BLOGS & MESSAGE BOARDS (78%); DATA ANALYTICS (78%); EMERGING TECHNOLOGY (78%); Digital Society (%); Artificial intelligence (%); Digital technology (%); European Commission (%); Internet (%); artificial intelligence (%)
Organization: EUROPEAN COMMISSION (59%)
Industry: ARTIFICIAL INTELLIGENCE (90%); BLOGS & MESSAGE BOARDS (78%); DATA ANALYTICS (78%)
Geographic: EUROPE (88%); EUROPEAN UNION MEMBER STATES (79%)
Load-Date: July 9, 2021",neutral,0.8325721621513367,balanced/neutral,"['privacy', 'discrimination', 'security']",[],"['guidelines', 'framework']",[],3,0,2,0
2021,Unknown Title,"Body
ABU DHABI, 1st June, 2021 (WAM) -- The Abu Dhabi Agriculture and Food Safety Authority (ADAFSA) has issued a charter to introduce relevant stakeholders within its business to artificial intelligence (AI) principles and ethics.
The charter is made up of four guiding principles to enable the ethical use of AI in ADAFSA responsibly and without discrimination.
ADAFSA is one of the Abu Dhabi's first government entities to issue an ethical charter on the use of artificial intelligence. This comes in line with ADAFSA's commitment to the Abu Dhabi Government's plan and the National Programme for Artificial Intelligence, which aims to leverage AI technology in all activities to improve services and achieve customer satisfaction.
The charter's first principal focuses on impartiality and respect for current regulations and laws while designing, developing and applying AI systems and solutions in ADAFSA's activities. The second principle guarantees transparency and flexibility for users or beneficiaries of AI systems, to make data processing methods accessible and understandable.
Meanwhile, the third principle ensures objectivity and fairness to in the development and design of AI systems. Moreover, the fourth principle focuses on ensuring the use of secure, accurate and reliable data, to enable AI applications' contribution to agricultural sustainability, food security, biosecurity and food safety.
At ADAFSA, the Statistics and Analysis Division is tasked with managing, developing and applying AI systems to the Authority's activities, due to its importance in advancing businesses and improving services offered to the public. The division further succeeded in developing a smart algorithm to eradicate Paste des Petits Ruminants (PPR) and vaccinating animals regularly, thus improving its immune system.
In this regard, Aisha Al Nayli Al Shamsi, Statistics and Analysis Division Director and Chief Data Officer at ADAFSA, said, ""The moral and ethical aspect of AI applications is gaining increased importance worldwide. ADAFSA, therefore, has taken the initiative of issuing the charter to set out ethics for AI systems' developers, users and beneficiaries.""Al Shamsi clarified that AI systems should be objective, transparent, flexible, fair, aligned with regulations, accountable, secure and reliable, as these are the principles that ADAFSA's charter is based on.
""The commitment to these principles will contribute to digitalising the agricultural and food sector, as well as embracing AI systems in developing supply chains or addressing challenges facing this strategic sector,"" Al Shamsi noted.
She added that ADAFSA is launching several AI-based projects, such as analysing big data, applying the IoT, and developing algorithm to improve methods of combating animal diseases.
Al Shamsi highlighted the outstanding results of ADAFSA's smart algorithm to eradicate PPR, including improving animal immunity, reducing mortality of animals with PPR, raising veterinarians productivity, and increasing numbers of vaccinated livestock.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: 55
Subject: ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); FOOD SAFETY (90%); FOOD SAFETY REGULATION (90%); SAFETY (90%); ARTIFICIAL INTELLIGENCE ETHICS (89%); ANIMALS (85%); DISEASE IMMUNITY (85%); FOOD SECURITY (78%); SUSTAINABLE DEVELOPMENT (78%); SUSTAINABLE AGRICULTURE (73%); VACCINES (71%); EXECUTIVES (70%); CUSTOMER SATISFACTION (69%); LAW & LEGAL SYSTEM (68%); MAMMALS (68%)
Industry: ARTIFICIAL INTELLIGENCE (90%); FOOD SAFETY (90%); FOOD SAFETY REGULATION (90%); PRESS AGENCY RELEASES (90%); ARTIFICIAL INTELLIGENCE ETHICS (89%); AGRICULTURE, FORESTRY, FISHING & HUNTING (78%); FOOD & BEVERAGE (78%); FOOD INDUSTRY (78%); SUSTAINABLE DEVELOPMENT (78%); INTERNET OF THINGS (74%); VETERINARY SERVICES (74%); LIVESTOCK (73%); SUSTAINABLE AGRICULTURE (73%); VACCINES (71%); CUSTOMER SATISFACTION (69%)
Geographic: ABU DHABI, UNITED ARAB EMIRATES (90%)
Load-Date: June 1, 2021","ABU DHABI, 1st June, 2021 (WAM) -- The Abu Dhabi Agriculture and Food Safety Authority (ADAFSA) has issued a charter to introduce relevant stakeholders within its business to artificial intelligence (AI) principles and ethics.
The charter is made up of four guiding principles to enable the ethical use of AI in ADAFSA responsibly and without discrimination.
ADAFSA is one of the Abu Dhabi's first government entities to issue an ethical charter on the use of artificial intelligence. This comes in line with ADAFSA's commitment to the Abu Dhabi Government's plan and the National Programme for Artificial Intelligence, which aims to leverage AI technology in all activities to improve services and achieve customer satisfaction.
The charter's first principal focuses on impartiality and respect for current regulations and laws while designing, developing and applying AI systems and solutions in ADAFSA's activities. The second principle guarantees transparency and flexibility for users or beneficiaries of AI systems, to make data processing methods accessible and understandable.
Meanwhile, the third principle ensures objectivity and fairness to in the development and design of AI systems. Moreover, the fourth principle focuses on ensuring the use of secure, accurate and reliable data, to enable AI applications' contribution to agricultural sustainability, food security, biosecurity and food safety.
At ADAFSA, the Statistics and Analysis Division is tasked with managing, developing and applying AI systems to the Authority's activities, due to its importance in advancing businesses and improving services offered to the public. The division further succeeded in developing a smart algorithm to eradicate Paste des Petits Ruminants (PPR) and vaccinating animals regularly, thus improving its immune system.
In this regard, Aisha Al Nayli Al Shamsi, Statistics and Analysis Division Director and Chief Data Officer at ADAFSA, said, ""The moral and ethical aspect of AI applications is gaining increased importance worldwide. ADAFSA, therefore, has taken the initiative of issuing the charter to set out ethics for AI systems' developers, users and beneficiaries.""Al Shamsi clarified that AI systems should be objective, transparent, flexible, fair, aligned with regulations, accountable, secure and reliable, as these are the principles that ADAFSA's charter is based on.
""The commitment to these principles will contribute to digitalising the agricultural and food sector, as well as embracing AI systems in developing supply chains or addressing challenges facing this strategic sector,"" Al Shamsi noted.
She added that ADAFSA is launching several AI-based projects, such as analysing big data, applying the IoT, and developing algorithm to improve methods of combating animal diseases.
Al Shamsi highlighted the outstanding results of ADAFSA's smart algorithm to eradicate PPR, including improving animal immunity, reducing mortality of animals with PPR, raising veterinarians productivity, and increasing numbers of vaccinated livestock.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: 55
Subject: ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); FOOD SAFETY (90%); FOOD SAFETY REGULATION (90%); SAFETY (90%); ARTIFICIAL INTELLIGENCE ETHICS (89%); ANIMALS (85%); DISEASE IMMUNITY (85%); FOOD SECURITY (78%); SUSTAINABLE DEVELOPMENT (78%); SUSTAINABLE AGRICULTURE (73%); VACCINES (71%); EXECUTIVES (70%); CUSTOMER SATISFACTION (69%); LAW & LEGAL SYSTEM (68%); MAMMALS (68%)
Industry: ARTIFICIAL INTELLIGENCE (90%); FOOD SAFETY (90%); FOOD SAFETY REGULATION (90%); PRESS AGENCY RELEASES (90%); ARTIFICIAL INTELLIGENCE ETHICS (89%); AGRICULTURE, FORESTRY, FISHING & HUNTING (78%); FOOD & BEVERAGE (78%); FOOD INDUSTRY (78%); SUSTAINABLE DEVELOPMENT (78%); INTERNET OF THINGS (74%); VETERINARY SERVICES (74%); LIVESTOCK (73%); SUSTAINABLE AGRICULTURE (73%); VACCINES (71%); CUSTOMER SATISFACTION (69%)
Geographic: ABU DHABI, UNITED ARAB EMIRATES (90%)
Load-Date: June 1, 2021",neutral,0.7338531613349915,balanced/neutral,"['discrimination', 'fairness', 'transparency', 'safety', 'security', 'agency']",['fairness'],"['regulation', 'law', 'should']",['algorithm'],6,1,3,1
2021,Unknown Title,"Body
Eindhoven University of Technology: Why ethics should be integral part of Artificial Intelligence.
Mykola Pechenizkiy develops data mining and machine learning tools to prevent undesirable algorithmic biases in AI.
Much of current AI is about finding statistical patterns in historical data to make valuable predictions for the future. However, there is increasing evidence that the use of algorithms, including racial profiling, can severely disadvantage vulnerable sections of society, even when there is no explicit intention of discrimination. We ask Mykola Pechenizkiy, professor Data Mining at Eindhoven University of Technology and one of the pioneers in ethical AI, about his efforts to make AI fair and responsible. 'We should stop blaming the data and make AI trustworthy by design'.
On January 15th 2021, the Dutch government resigned, just two months ahead of the general elections. The reason was not, as you might expect, its handling of the Corona pandemic, but a scandal in which thousands of Dutch families, many of them from ethnic minorities, were wrongly accused of child welfare fraud.
The scandal is just one example of the increasing evidence that AI and machine learning have an impact on society and humans that is less beneficial. Other examples include facial-recognition software that discriminates against darker-skinned faces and the faces of women, image-generation algorithms that autocomplete cropped images of women with a low-cut top or bikini, or a state-of-the art language model (GPT-3) that has an explicit tendency to link muslims to violence.
'In other words: AI has a bias problem', says Pechenizkiy. 'Its predictions necessarily reflect the data they are based on, which, as we know, are often skewed in ways that reinforce existing inequalities in society. Or, as data analysts like to put it: garbage in, garbage out.'
ETHICS TAKES CENTER STAGE.
AI has an increasing impact on our everyday world - affecting millions of people not just in industry, but also in healthcare, education, government and many other areas. Take predictive analytics, for example, which can screen patients for potential corona infections based on data from routine blood tests, or predict hospital readmissions of heart failure patients using an attention-based neural network.
Therefore, bias in data is a big problem, which forces computer scientists to take a stand. 'I strongly believe that ethics should be an integral part of AI. It can no longer be considered as a mere afterthought to an optimization problem, as traditionally has been the case,' says Pechenizkiy.
'These days, AI and machine learning are tightly connected to fairness and non-discrimination. If you build an algorithm that helps employers decide who should be invited for a job interview, it should not only be accurate, but also respect different aspects of diversity, and be legally and ethically compliant. Or if you design an AI system to diagnose skin cancer, it should work for both men and women, and for people of brighter as well as darker skins.'
PIONEERS
Pechenizkiy and his colleagues at TU/e have been pioneering the modern field of ethical AI. 'The first PhD thesis on fairness-aware machine learning was written here at TU/e by Faisal Kamiran, under the supervision of Toon Calders. And back in 2010, when few people could see the relevance of it, we wrote a number of ground-breaking papers on this topic, for instance on discrimination aware decision tree learning. We designed a new classification technique based on decision tree learning, which generates accurate predictions for future decision making, yet does not discriminate against vulnerable groups,' says Pechenizkiy.
'The technique takes into account discrimination both in the construction of the decision tree, and in the labeling of the leaves, without losing in accuracy. This works better than just 'cleaning away' the bias before applying traditional classification algorithms. Our model also works in cases of indirect discrimination. For example when the ethnicity of a person is strongly linked with the area they live in, leading to a classifier that discriminates based on postal code.'
FAIRMATCH
Much more recent work, from 2020, addresses bias in recommender systems. Recommenders are used by companies like Amazon and Spotify to predict user preferences and give them personalized recommendations.
Research has shown that these systems, while very effective, suffer from a number of problems, because of feedback loops that introduce and amplify stereotypical behavior. As a result, they tend to favour a limited number of very popular content, and they can be biased against certain groups, such as women.
PhD candidate Masoud Mansoury has designed a graph-based algorithm, Fairmatch, that significantly improves diversity in recommendations. It works as a post-processing tool on top of any existing standard recommendation algorithm. 'The idea is to is to improve the visibility of high-quality items that have a low visibility in the original set of recommendations', says Pechenizkiy.
'The beautiful thing is that Fairmatch can also be used in other contexts, for instance in recommenders that currently generate less accurate predictions for women or to fairly distribute high-paying jobs among men and women.'
MAKING AI TRUSTWORTHY BY DESIGN
'This is truly an exciting time for computer scientists', says Pechenizkiy. 'We have to start thinking about how important properties like fairness and privacy can be formalized, how we can integrate them in research and development life cycles, and propagate them into machine learning pipelines. Sure, data is always biased in some ways, and messy in many ways. But by developing models that are explainable and certifiable, we can ensure that AI is trustworthy by design, reflecting both its added value and its vulnerabilities.'
'At the same time, it is important that computer scientists set up collaborations with researchers from other disciplines, including the ethical or legal domains', says Pechenizkiy. He points to some interesting paradoxes that he and his colleagues have come across in their work.
'Take for instance the fact that you need to have access to sensitive information to be able to learn unbiased models. Or the situation where different notions of fairness conflict, or need to be balanced. While these problems can partly be solved by computer scientists, they often touch on issues that go beyond the exact sciences, and enter into the realms of philosophy and law.'
FINDING A COMMON LANGUAGE
While there has been a lot of progress in this field over the last five years - also at TU/e, where computer scientists have been exploring collaborations with philosophers and ethicists -, Pechenizkiy still sees a big divide between the domains.
'Computer scientists have a tendency to oversimplify real-world problems, in order to be able to optimize them in their models, whereas social scientists often don't have a clear view of what can be made operational. I do hope we can move further, once we are able to find a common vocabulary, but it won't happen automatically. It requires commitment from all parties and also extra funding.'
At the same time, the Ukrainian-born researcher sees certain ethical challenges around AI that computer scientists cannot solve (and should not solve), even if they work together with ethicists. 'Take for instance the use of facial or speech recognition to predict certain properties, like sex or sexual orientiation. That would plainly be unethical. Or trying to predict IQ based on facial features, which would not only be unethical but also unscientific!'
PLAIN SCIENCE
In this context, it is interesting to see that a growing number of AI conferences (e.g. NeurIPS 2020) require researchers to provide an ethical impact statement, to make sure the research is responsible. 'I think this is a good thing. It is common practice to perform an ethical review in the social sciences, so why not in computer science?'.
Still, Pechenizkiy thinks we shouldn't overdo it. 'As there is a hype in AI, we also run the risk of creating a hype in AI ethics. Much research in computer science is just that: plain science, with probably no ethical impact at all. Ethics is important, but remember: if you don't have AI, you don't have ethics in AI. Sometimes people tend to forget that'.
Media contact
Henk van Appeven (Communications Adviser)
+31 40 247 6268 h.g.p.v.appeven@tue.nl
[Editorial queries for this story should be sent to newswire@enpublishing.co.uk ]   
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: ENPN
Subject: ARTIFICIAL INTELLIGENCE (90%); DATA MINING (90%); DISCRIMINATION (90%); ETHICS (90%); MACHINE LEARNING (90%); NEGATIVE MISC NEWS (90%); NEGATIVE NEWS (90%); TEACHING MATERIALS & MEDIA (90%); NEGATIVE SOCIETAL NEWS (89%); RACE & ETHNICITY (89%); ARTIFICIAL INTELLIGENCE ETHICS (78%); BUSINESS ANALYTICS (78%); COMPUTER SCIENCE (78%); DATA ANALYTICS (78%); DATA SCIENCE (78%); IDENTIFICATION TECHNOLOGIES (78%); NEURAL NETWORKS (78%); PREDICTIVE ANALYTICS (78%); PUBLIC HEALTH (78%); SOCIAL JUSTICE (78%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (78%); TECHNICIANS & TECHNOLOGICAL WORKERS (78%); COLORISM (77%); BIOMETRICS (76%); ETHNIC GROUPS (76%); MINORITY GROUPS (76%); PRESS RELEASES (74%); COLLEGE & UNIVERSITY PROFESSORS (73%); EPIDEMICS (73%); FRAUD & FINANCIAL CRIME (73%); FAMILY (72%); HISTORY (72%); RACIAL PROFILING (71%); CAMPAIGNS & ELECTIONS (69%); ELECTIONS (69%); ELECTIONS & POLITICS (69%); RECRUITMENT & HIRING (65%); CANCER (60%); EMPLOYMENT INTERVIEWS (60%); HEART DISEASE (60%); RELIGION (50%); SKIN CANCER (50%)
Company:  AI SYSTEMS (50%)
Industry: SIC7372 PREPACKAGED SOFTWARE (50%); ARTIFICIAL INTELLIGENCE (90%); DATA MINING (90%); MACHINE LEARNING (90%); ARTIFICIAL INTELLIGENCE ETHICS (78%); BUSINESS ANALYTICS (78%); COMPUTER SCIENCE (78%); DATA ANALYTICS (78%); DATA SCIENCE (78%); NEURAL NETWORKS (78%); PREDICTIVE ANALYTICS (78%); COLLEGE & UNIVERSITY PROFESSORS (73%); PATTERN RECOGNITION (73%); COMPUTER SOFTWARE (66%)
Geographic: NETHERLANDS (93%)
Load-Date: March 17, 2021","Eindhoven University of Technology: Why ethics should be integral part of Artificial Intelligence.
Mykola Pechenizkiy develops data mining and machine learning tools to prevent undesirable algorithmic biases in AI.
Much of current AI is about finding statistical patterns in historical data to make valuable predictions for the future. However, there is increasing evidence that the use of algorithms, including racial profiling, can severely disadvantage vulnerable sections of society, even when there is no explicit intention of discrimination. We ask Mykola Pechenizkiy, professor Data Mining at Eindhoven University of Technology and one of the pioneers in ethical AI, about his efforts to make AI fair and responsible. 'We should stop blaming the data and make AI trustworthy by design'.
On January 15th 2021, the Dutch government resigned, just two months ahead of the general elections. The reason was not, as you might expect, its handling of the Corona pandemic, but a scandal in which thousands of Dutch families, many of them from ethnic minorities, were wrongly accused of child welfare fraud.
The scandal is just one example of the increasing evidence that AI and machine learning have an impact on society and humans that is less beneficial. Other examples include facial-recognition software that discriminates against darker-skinned faces and the faces of women, image-generation algorithms that autocomplete cropped images of women with a low-cut top or bikini, or a state-of-the art language model (GPT-3) that has an explicit tendency to link muslims to violence.
'In other words: AI has a bias problem', says Pechenizkiy. 'Its predictions necessarily reflect the data they are based on, which, as we know, are often skewed in ways that reinforce existing inequalities in society. Or, as data analysts like to put it: garbage in, garbage out.'
ETHICS TAKES CENTER STAGE.
AI has an increasing impact on our everyday world - affecting millions of people not just in industry, but also in healthcare, education, government and many other areas. Take predictive analytics, for example, which can screen patients for potential corona infections based on data from routine blood tests, or predict hospital readmissions of heart failure patients using an attention-based neural network.
Therefore, bias in data is a big problem, which forces computer scientists to take a stand. 'I strongly believe that ethics should be an integral part of AI. It can no longer be considered as a mere afterthought to an optimization problem, as traditionally has been the case,' says Pechenizkiy.
'These days, AI and machine learning are tightly connected to fairness and non-discrimination. If you build an algorithm that helps employers decide who should be invited for a job interview, it should not only be accurate, but also respect different aspects of diversity, and be legally and ethically compliant. Or if you design an AI system to diagnose skin cancer, it should work for both men and women, and for people of brighter as well as darker skins.'
PIONEERS
Pechenizkiy and his colleagues at TU/e have been pioneering the modern field of ethical AI. 'The first PhD thesis on fairness-aware machine learning was written here at TU/e by Faisal Kamiran, under the supervision of Toon Calders. And back in 2010, when few people could see the relevance of it, we wrote a number of ground-breaking papers on this topic, for instance on discrimination aware decision tree learning. We designed a new classification technique based on decision tree learning, which generates accurate predictions for future decision making, yet does not discriminate against vulnerable groups,' says Pechenizkiy.
'The technique takes into account discrimination both in the construction of the decision tree, and in the labeling of the leaves, without losing in accuracy. This works better than just 'cleaning away' the bias before applying traditional classification algorithms. Our model also works in cases of indirect discrimination. For example when the ethnicity of a person is strongly linked with the area they live in, leading to a classifier that discriminates based on postal code.'
FAIRMATCH
Much more recent work, from 2020, addresses bias in recommender systems. Recommenders are used by companies like Amazon and Spotify to predict user preferences and give them personalized recommendations.
Research has shown that these systems, while very effective, suffer from a number of problems, because of feedback loops that introduce and amplify stereotypical behavior. As a result, they tend to favour a limited number of very popular content, and they can be biased against certain groups, such as women.
PhD candidate Masoud Mansoury has designed a graph-based algorithm, Fairmatch, that significantly improves diversity in recommendations. It works as a post-processing tool on top of any existing standard recommendation algorithm. 'The idea is to is to improve the visibility of high-quality items that have a low visibility in the original set of recommendations', says Pechenizkiy.
'The beautiful thing is that Fairmatch can also be used in other contexts, for instance in recommenders that currently generate less accurate predictions for women or to fairly distribute high-paying jobs among men and women.'
MAKING AI TRUSTWORTHY BY DESIGN
'This is truly an exciting time for computer scientists', says Pechenizkiy. 'We have to start thinking about how important properties like fairness and privacy can be formalized, how we can integrate them in research and development life cycles, and propagate them into machine learning pipelines. Sure, data is always biased in some ways, and messy in many ways. But by developing models that are explainable and certifiable, we can ensure that AI is trustworthy by design, reflecting both its added value and its vulnerabilities.'
'At the same time, it is important that computer scientists set up collaborations with researchers from other disciplines, including the ethical or legal domains', says Pechenizkiy. He points to some interesting paradoxes that he and his colleagues have come across in their work.
'Take for instance the fact that you need to have access to sensitive information to be able to learn unbiased models. Or the situation where different notions of fairness conflict, or need to be balanced. While these problems can partly be solved by computer scientists, they often touch on issues that go beyond the exact sciences, and enter into the realms of philosophy and law.'
FINDING A COMMON LANGUAGE
While there has been a lot of progress in this field over the last five years - also at TU/e, where computer scientists have been exploring collaborations with philosophers and ethicists -, Pechenizkiy still sees a big divide between the domains.
'Computer scientists have a tendency to oversimplify real-world problems, in order to be able to optimize them in their models, whereas social scientists often don't have a clear view of what can be made operational. I do hope we can move further, once we are able to find a common vocabulary, but it won't happen automatically. It requires commitment from all parties and also extra funding.'
At the same time, the Ukrainian-born researcher sees certain ethical challenges around AI that computer scientists cannot solve (and should not solve), even if they work together with ethicists. 'Take for instance the use of facial or speech recognition to predict certain properties, like sex or sexual orientiation. That would plainly be unethical. Or trying to predict IQ based on facial features, which would not only be unethical but also unscientific!'
PLAIN SCIENCE
In this context, it is interesting to see that a growing number of AI conferences (e.g. NeurIPS 2020) require researchers to provide an ethical impact statement, to make sure the research is responsible. 'I think this is a good thing. It is common practice to perform an ethical review in the social sciences, so why not in computer science?'.
Still, Pechenizkiy thinks we shouldn't overdo it. 'As there is a hype in AI, we also run the risk of creating a hype in AI ethics. Much research in computer science is just that: plain science, with probably no ethical impact at all. Ethics is important, but remember: if you don't have AI, you don't have ethics in AI. Sometimes people tend to forget that'.
Media contact
Henk van Appeven (Communications Adviser)
+31 40 247 6268 h.g.p.v.appeven@tue.nl
[Editorial queries for this story should be sent to newswire@enpublishing.co.uk ]   
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: ENPN
Subject: ARTIFICIAL INTELLIGENCE (90%); DATA MINING (90%); DISCRIMINATION (90%); ETHICS (90%); MACHINE LEARNING (90%); NEGATIVE MISC NEWS (90%); NEGATIVE NEWS (90%); TEACHING MATERIALS & MEDIA (90%); NEGATIVE SOCIETAL NEWS (89%); RACE & ETHNICITY (89%); ARTIFICIAL INTELLIGENCE ETHICS (78%); BUSINESS ANALYTICS (78%); COMPUTER SCIENCE (78%); DATA ANALYTICS (78%); DATA SCIENCE (78%); IDENTIFICATION TECHNOLOGIES (78%); NEURAL NETWORKS (78%); PREDICTIVE ANALYTICS (78%); PUBLIC HEALTH (78%); SOCIAL JUSTICE (78%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (78%); TECHNICIANS & TECHNOLOGICAL WORKERS (78%); COLORISM (77%); BIOMETRICS (76%); ETHNIC GROUPS (76%); MINORITY GROUPS (76%); PRESS RELEASES (74%); COLLEGE & UNIVERSITY PROFESSORS (73%); EPIDEMICS (73%); FRAUD & FINANCIAL CRIME (73%); FAMILY (72%); HISTORY (72%); RACIAL PROFILING (71%); CAMPAIGNS & ELECTIONS (69%); ELECTIONS (69%); ELECTIONS & POLITICS (69%); RECRUITMENT & HIRING (65%); CANCER (60%); EMPLOYMENT INTERVIEWS (60%); HEART DISEASE (60%); RELIGION (50%); SKIN CANCER (50%)
Company:  AI SYSTEMS (50%)
Industry: SIC7372 PREPACKAGED SOFTWARE (50%); ARTIFICIAL INTELLIGENCE (90%); DATA MINING (90%); MACHINE LEARNING (90%); ARTIFICIAL INTELLIGENCE ETHICS (78%); BUSINESS ANALYTICS (78%); COMPUTER SCIENCE (78%); DATA ANALYTICS (78%); DATA SCIENCE (78%); NEURAL NETWORKS (78%); PREDICTIVE ANALYTICS (78%); COLLEGE & UNIVERSITY PROFESSORS (73%); PATTERN RECOGNITION (73%); COMPUTER SOFTWARE (66%)
Geographic: NETHERLANDS (93%)
Load-Date: March 17, 2021",neutral,0.810208261013031,balanced/neutral,"['privacy', 'bias', 'discrimination', 'fairness', 'access']","['justice', 'fairness', 'justice']","['law', 'should', 'need to']","['machine learning', 'neural network', 'gpt', 'speech recognition', 'algorithm', 'predictive analytics', 'data mining']",5,3,3,7
2021,Unknown Title,"Body
The recent apartment building collapse in Miami, Florida, is a tragic reminder of the huge impacts engineering can have on our lives. Disasters such as this force engineers to reflect on their practice and perhaps fundamentally change their approach. Specifically, we should give much greater weight to ethics when training engineers. 
Engineers work in a vast range of fields that pose ethical concerns. These include artificial intelligence, data privacy, building construction, public health, and activity on shared environments (including Indigenous communities). The decisions engineers make, if not fully thought through, can have unintended consequences - including building failures and climate change. 
Engineers have ethical obligations (such as Engineers Australia's code of ethics) that they must follow. However, as identified at UNSW, the complexity of emerging social concerns creates a need for engineers' education to equip them with much deeper ethical skill sets. 
Engineering is seen as a trusted and ethical profession. In a 2019 Gallup poll, 66% rated the honesty and ethical standards of engineers as high/very high, on a par with medical doctors (65%). 
However, ethics as a body of knowledge is massive. There are nearly as many academic papers on ethics as mathematics, and clearly more than on artificial intelligence. 
Comparison of numbers of research papers by keyword (mathematics, ethics and AI).
With such a rich backdrop of knowledge, engineers must embrace ethics in a way that previous generations embraced mathematics. Complex societal problems make much greater demands on engineering thinking than in the past. We need to consider whole and complex systems, not just issues as individual challenges. 
Ethics and the construction industry
The construction industry provides a topical example of such complexity. Opal Tower in Sydney, Lacrosse building in Melbourne, Grenfell Tower in London and Torch Tower in Dubai became household names for all the wrong reasons. 
Importantly, these issues of poor quality and performance don't arise from new technology or know-how. They involve well-established technical domains of engineering: combustible cladding, fire safety, structural adequacy and so on. A fragmented design and delivery process with unclear responsibility and/or accountability has led to poor outcomes. 
These issues prompted the Australian Building Ministers' Forum to commission the Shergold Weir Report, followed by a task force to implement its recommendations across Australia. 
There are real shortcomings in the legal and contractual processes for allocating and ""commoditising"" risk in the industry. However, ethics should do the heavy lifting when legal frameworks are lacking. One key question is whether erosion of professional ethics has played a part in this state of affairs. The answer is a likely ""yes"". 
Engineers face ethical dilemmas such as: 
""Should I accept a narrow or inadequately framed design commission within a design and build delivery model when there is no certainty my design will be appropriately integrated with other parts of the project?""   
""How can I accept a commission when my client provides no budget for my oversight of the construction to ensure the technical integrity of my design is maintained when built?""   
""How do I play in a commercially competitive landscape with pressures to produce ""leaner"" designs to save cost without compromising safety and long-term performance of my design?""   
""Do I hide behind the contractual clauses (or minimum requirements of codes of practice) when I know the overall process is flawed and does not deliver quality and/or value for money for the end user?""
 Or worse: ""Do I resort to phoenixing to avoid any accountability?""The ethics of engineering involve much more than ensuring buildings don't collapse. Chad Davis/Flickr, CC BY-SAEngineering on Country The enduring connection of Aboriginal Australians to Country requires engineers to navigate ethical considerations in Indigenous communities. Engineers must reconcile the legal, technical and regulatory requirements of their projects with Indigenous cultural values and needs. They might not be properly equipped to navigate ethical scenarios when they encounter unfamiliar cultural connections, or regulations are insufficient.  Consider, for example, the sacred sites of the McArthur River Mine. Traditional owners have raised concerns that current mining activities do not adequately protect sacred and cultural heritage sites. Evidence given by community leaders provides insight into the intimate and diverse relationship that traditional owners have with the land.  In considering such evidence, engineers must be able to evaluate both physical site risks (such as acidification of mine tailings and contamination of water bodies) and cultural risks (such as failing to identify all locations of cultural value).  How might we tackle such complicated projects? By properly engaging with traditional communities and by having diverse teams with multiple worldviews and experiences, along with strong technical skills. The broad field of ethical knowledge provides the skill sets to attempt to reconcile the diverse considerations. Cornell Dam on the Croton River near Croton-on-Hudson, New York, was the tallest dam in the world when completed in 1906. The dam was built with beauty and the environment in mind, but protests and disputes still impacted its construction. Malinda Rathnayake/Flickr, CC BYWhat should the curriculum look like? Engineering students' ethical development requires a holistic approach. One assessment suggested: 
""[...] that institutions integrate ethics instruction throughout the formal curriculum, support use of varied approaches that foster high‐quality experiences, and leverage both influences of co‐curricular experiences and students' desires to engage in positive ethical behaviours.""
The curriculum should include: 
skills/expertise - the underlying intellectual basis for discerning what is ethical and what is not, which is much more than codes of conduct or a prescriptive, formulaic approach   
practice - practical know-how in terms of ethical solutions that engineers can apply   
mindset - having an individual and group culture of acting ethically. The engineers' problem-solving mindset must be supplemented by constant reflection on the decisions made and their ethical consequences.
 Ethics is not an ""add-on"" subject. It must permeate all aspects of tertiary education - teaching, research and professional behaviour.  While the arguments for acting now are strong, market realities will also drive the process. The upcoming generation will likely displace those who are slow or reluctant to adapt.  For instance, engineering firms are under pressure from their own staff on the issue of climate change. More than 1,900 Australian engineers and nearly 180 engineering organisations have signed a declaration committing them to evaluate all new projects against the need to mitigate climate change.  Future engineers must transcend any remaining single-solution mindsets from the past. They'll need to embrace a much more complex and socially minded ethics. And that begins with their university education.     This article is republished from The Conversation under a Creative Commons license. Read the original article.    READ MORE:Reforms give construction regulator power to reject projects, push for national adoption
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (96%); ARTIFICIAL INTELLIGENCE (90%); ENGINEERING (90%); PROFESSIONAL WORKERS (90%); SOCIETAL ISSUES (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (90%); TECHNICIANS & TECHNOLOGICAL WORKERS (90%); CONSTRUCTION SECTOR PERFORMANCE (89%); NEGATIVE NEWS (89%); 2017 GRENFELL TOWER FIRE (78%); ARTIFICIAL INTELLIGENCE ETHICS (78%); BUILDING COLLAPSES (78%); NEGATIVE SOCIETAL NEWS (78%); RESEARCH REPORTS (78%); SAFETY (78%); SAFETY, ACCIDENTS & DISASTERS (73%); PHYSICIANS & SURGEONS (72%); POLLS & SURVEYS (72%); PUBLIC HEALTH (72%); INDIGENOUS PEOPLES (71%); FIRE PREVENTION & SAFETY (69%)
Industry: ARTIFICIAL INTELLIGENCE (90%); CONSTRUCTION (90%); ENGINEERING (90%); CONSTRUCTION SECTOR PERFORMANCE (89%); ARTIFICIAL INTELLIGENCE ETHICS (78%); KNOWLEDGE MANAGEMENT (76%); PHYSICIANS & SURGEONS (72%); INFORMATION SECURITY & PRIVACY (56%)
Geographic: MELBOURNE, AUSTRALIA (71%); SYDNEY, AUSTRALIA (71%); MIAMI, FL, USA (58%); LONDON, ENGLAND (53%); DUBAI, UNITED ARAB EMIRATES (79%); FLORIDA, USA (79%); AUSTRALIA (91%); UNITED ARAB EMIRATES (73%)
Load-Date: July 19, 2021","The recent apartment building collapse in Miami, Florida, is a tragic reminder of the huge impacts engineering can have on our lives. Disasters such as this force engineers to reflect on their practice and perhaps fundamentally change their approach. Specifically, we should give much greater weight to ethics when training engineers. 
Engineers work in a vast range of fields that pose ethical concerns. These include artificial intelligence, data privacy, building construction, public health, and activity on shared environments (including Indigenous communities). The decisions engineers make, if not fully thought through, can have unintended consequences - including building failures and climate change. 
Engineers have ethical obligations (such as Engineers Australia's code of ethics) that they must follow. However, as identified at UNSW, the complexity of emerging social concerns creates a need for engineers' education to equip them with much deeper ethical skill sets. 
Engineering is seen as a trusted and ethical profession. In a 2019 Gallup poll, 66% rated the honesty and ethical standards of engineers as high/very high, on a par with medical doctors (65%). 
However, ethics as a body of knowledge is massive. There are nearly as many academic papers on ethics as mathematics, and clearly more than on artificial intelligence. 
Comparison of numbers of research papers by keyword (mathematics, ethics and AI).
With such a rich backdrop of knowledge, engineers must embrace ethics in a way that previous generations embraced mathematics. Complex societal problems make much greater demands on engineering thinking than in the past. We need to consider whole and complex systems, not just issues as individual challenges. 
Ethics and the construction industry
The construction industry provides a topical example of such complexity. Opal Tower in Sydney, Lacrosse building in Melbourne, Grenfell Tower in London and Torch Tower in Dubai became household names for all the wrong reasons. 
Importantly, these issues of poor quality and performance don't arise from new technology or know-how. They involve well-established technical domains of engineering: combustible cladding, fire safety, structural adequacy and so on. A fragmented design and delivery process with unclear responsibility and/or accountability has led to poor outcomes. 
These issues prompted the Australian Building Ministers' Forum to commission the Shergold Weir Report, followed by a task force to implement its recommendations across Australia. 
There are real shortcomings in the legal and contractual processes for allocating and ""commoditising"" risk in the industry. However, ethics should do the heavy lifting when legal frameworks are lacking. One key question is whether erosion of professional ethics has played a part in this state of affairs. The answer is a likely ""yes"". 
Engineers face ethical dilemmas such as: 
""Should I accept a narrow or inadequately framed design commission within a design and build delivery model when there is no certainty my design will be appropriately integrated with other parts of the project?""   
""How can I accept a commission when my client provides no budget for my oversight of the construction to ensure the technical integrity of my design is maintained when built?""   
""How do I play in a commercially competitive landscape with pressures to produce ""leaner"" designs to save cost without compromising safety and long-term performance of my design?""   
""Do I hide behind the contractual clauses (or minimum requirements of codes of practice) when I know the overall process is flawed and does not deliver quality and/or value for money for the end user?""
 Or worse: ""Do I resort to phoenixing to avoid any accountability?""The ethics of engineering involve much more than ensuring buildings don't collapse. Chad Davis/Flickr, CC BY-SAEngineering on Country The enduring connection of Aboriginal Australians to Country requires engineers to navigate ethical considerations in Indigenous communities. Engineers must reconcile the legal, technical and regulatory requirements of their projects with Indigenous cultural values and needs. They might not be properly equipped to navigate ethical scenarios when they encounter unfamiliar cultural connections, or regulations are insufficient.  Consider, for example, the sacred sites of the McArthur River Mine. Traditional owners have raised concerns that current mining activities do not adequately protect sacred and cultural heritage sites. Evidence given by community leaders provides insight into the intimate and diverse relationship that traditional owners have with the land.  In considering such evidence, engineers must be able to evaluate both physical site risks (such as acidification of mine tailings and contamination of water bodies) and cultural risks (such as failing to identify all locations of cultural value).  How might we tackle such complicated projects? By properly engaging with traditional communities and by having diverse teams with multiple worldviews and experiences, along with strong technical skills. The broad field of ethical knowledge provides the skill sets to attempt to reconcile the diverse considerations. Cornell Dam on the Croton River near Croton-on-Hudson, New York, was the tallest dam in the world when completed in 1906. The dam was built with beauty and the environment in mind, but protests and disputes still impacted its construction. Malinda Rathnayake/Flickr, CC BYWhat should the curriculum look like? Engineering students' ethical development requires a holistic approach. One assessment suggested: 
""[...] that institutions integrate ethics instruction throughout the formal curriculum, support use of varied approaches that foster high‐quality experiences, and leverage both influences of co‐curricular experiences and students' desires to engage in positive ethical behaviours.""
The curriculum should include: 
skills/expertise - the underlying intellectual basis for discerning what is ethical and what is not, which is much more than codes of conduct or a prescriptive, formulaic approach   
practice - practical know-how in terms of ethical solutions that engineers can apply   
mindset - having an individual and group culture of acting ethically. The engineers' problem-solving mindset must be supplemented by constant reflection on the decisions made and their ethical consequences.
 Ethics is not an ""add-on"" subject. It must permeate all aspects of tertiary education - teaching, research and professional behaviour.  While the arguments for acting now are strong, market realities will also drive the process. The upcoming generation will likely displace those who are slow or reluctant to adapt.  For instance, engineering firms are under pressure from their own staff on the issue of climate change. More than 1,900 Australian engineers and nearly 180 engineering organisations have signed a declaration committing them to evaluate all new projects against the need to mitigate climate change.  Future engineers must transcend any remaining single-solution mindsets from the past. They'll need to embrace a much more complex and socially minded ethics. And that begins with their university education.     This article is republished from The Conversation under a Creative Commons license. Read the original article.    READ MORE:Reforms give construction regulator power to reject projects, push for national adoption
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (96%); ARTIFICIAL INTELLIGENCE (90%); ENGINEERING (90%); PROFESSIONAL WORKERS (90%); SOCIETAL ISSUES (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (90%); TECHNICIANS & TECHNOLOGICAL WORKERS (90%); CONSTRUCTION SECTOR PERFORMANCE (89%); NEGATIVE NEWS (89%); 2017 GRENFELL TOWER FIRE (78%); ARTIFICIAL INTELLIGENCE ETHICS (78%); BUILDING COLLAPSES (78%); NEGATIVE SOCIETAL NEWS (78%); RESEARCH REPORTS (78%); SAFETY (78%); SAFETY, ACCIDENTS & DISASTERS (73%); PHYSICIANS & SURGEONS (72%); POLLS & SURVEYS (72%); PUBLIC HEALTH (72%); INDIGENOUS PEOPLES (71%); FIRE PREVENTION & SAFETY (69%)
Industry: ARTIFICIAL INTELLIGENCE (90%); CONSTRUCTION (90%); ENGINEERING (90%); CONSTRUCTION SECTOR PERFORMANCE (89%); ARTIFICIAL INTELLIGENCE ETHICS (78%); KNOWLEDGE MANAGEMENT (76%); PHYSICIANS & SURGEONS (72%); INFORMATION SECURITY & PRIVACY (56%)
Geographic: MELBOURNE, AUSTRALIA (71%); SYDNEY, AUSTRALIA (71%); MIAMI, FL, USA (58%); LONDON, ENGLAND (53%); DUBAI, UNITED ARAB EMIRATES (79%); FLORIDA, USA (79%); AUSTRALIA (91%); UNITED ARAB EMIRATES (73%)
Load-Date: July 19, 2021",negative,0.6865031719207764,balanced/neutral,"['privacy', 'accountability', 'safety', 'security']",[],"['oversight', 'standards', 'should', 'must', 'need to']",[],4,0,5,0
2021,Unknown Title,"Body
ABSTRACT
Philosophical ethics is the practice of asking the question when making a decision, to ensure you make choices that reflect your values.
FULL TEXT
Building skills in philosophical ethics could assist public servants in their response to complex issues and boost public trust, according to ethicist Dr Matt Beard. 
While ethics is often thought of as a set of rules, or the answer to a question, philosophical ethics is the practice of asking the question. It allows people to weigh up their options when making a decision to ensure they make choices that reflect their values. 
Beard, who is the new program director of Cranlana Centre for Ethical Leadership's Vincent Fairfax Fellowship, says this practice can be a valuable resource to public service leaders because it encourages decision-making attributes that demonstrate competence to the public. In a time when there is a low level of public trust and confidence in political leaders and the public service, this is crucial. 
""We need to pump [the public service] full of people who have the kinds of characteristics and the kind of traits that we tend to trust, and that we tend to recognise as being competent in being able to lead and make complex and difficult decisions,"" Beard tells The Mandarin. 
""And those skills, those competencies, tend to correspond to the kinds of things that we need in order to do philosophical ethics well. The two things are closely related.""
Beard argues that philosophical ethics could also be a better alternative to nudging when it comes to solving behavioural problems. 
""There is a growing trend within organisations, both public service organisations and corporate organisations, of trying to deal with the problem of people who might not always do the things that they should -- people who might behave unethically in certain times -- through behavioural economics, through nudges, by designing systems that effectively deactivate people's ability to make conscious and reflective choices,"" he says. 
""What we're dealing with there is we're trying to turn a person into more of a cog in a wheel who performs their function, and we put them in a system in which they simply perform that function by designing the system really well, so that the other options don't occur."" 
The public service's use of behavioural economics to tackle problems could contribute to the negative 'mindless, faceless bureaucracy' stereotype, Beard warns. 
""When we try to deal with the problem of unethical behaviour by doubling down on the mindlessness of it, we start to create precisely the thing that people distrust in the first place,"" he says. 
""Instead of trying to deal with the problem of unethical behaviour by deactivating people's ability to choose, if instead we focus on expanding and developing their capacity to choose well -- which is the work that philosophical ethics does -- then we start to build a public service that people are more likely to trust."" 
READ MORE: PM&C leader makes case for 'nudging' people with public policy
When undertaking a consultation process for a new policy or program, government will often utilise the expertise of academics or technical experts. While this is all well and good, Beard says considering the views of those who work in areas such as philosophy, the arts and humanities can also be beneficial for bureaucrats. In regard to complex topics, such as artificial intelligence, these people could raise questions that might otherwise go unanswered. 
""I think one of the functions of philosophy, when it is done well and applied to questions of public affairs and public service, is it allows us to see what might be missed between the gaps of disciplinary expertise,"" he says. 
The COVID-19 pandemic is a prime example of an area where skills in philosophical ethics could be beneficial to the public service, Beard says, as the pandemic is a multifaceted problem. It is not just a public health issue, but an economic and social problem as well. 
""That has meant that political leaders and the public servants providing them with advice have needed to inhabit a bunch of different value systems and a bunch of different logic systems in order to work out what is ultimately a political decision about how we respond to this,"" he explains. 
If a leader were to focus solely on solving the public health aspect of the pandemic, for example, then they might initiate a lockdown until the virus has been eradicated. 
""But of course there are enormous economic and mental health costs to that, so there has been a trade off that has needed to be made between competing value systems. And that is precisely the kind of thing that developing skills in philosophical ethics equips us for,"" Beard says. 
READ MORE: Is it ethical to use behavioural insights?
Public servants can bring philosophical ethics into their work by continuing to place people and the community at the heart of their decision-making, and by being wary of any trends -- particularly related to language -- that discourage this person-centred approach. 
""The public service has done a wonderful job in a lot of areas in embracing certain design methodologies, like human-centred design, to try to think about the people who are going to be affected by certain problems and trying to solve it with those people in mind by including them in the process,"" Beard says. 
""But they've also adopted the same kind of language as the for-profit corporations that develop these methodologies. So they're talking about the people who are affected by certain processes as users, or as customers."" 
Beard notes that the relationship between the public service and citizens is 'profoundly different' to the relationship between a business and its customers or a technological platform and its users. 
""Those language shifts seem small but they can have really significant ramifications in terms of the way we think of those relationships,"" he says. 
READ MORE: How 'ethical allies' elevate public service capability
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (91%); GOVERNMENT ETHICS (90%); ASSOCIATIONS & ORGANIZATIONS (89%); TRENDS (78%); ECONOMICS (62%)
Industry: BEHAVIORAL FINANCE (69%)
Load-Date: June 23, 2021","ABSTRACT
Philosophical ethics is the practice of asking the question when making a decision, to ensure you make choices that reflect your values.
FULL TEXT
Building skills in philosophical ethics could assist public servants in their response to complex issues and boost public trust, according to ethicist Dr Matt Beard. 
While ethics is often thought of as a set of rules, or the answer to a question, philosophical ethics is the practice of asking the question. It allows people to weigh up their options when making a decision to ensure they make choices that reflect their values. 
Beard, who is the new program director of Cranlana Centre for Ethical Leadership's Vincent Fairfax Fellowship, says this practice can be a valuable resource to public service leaders because it encourages decision-making attributes that demonstrate competence to the public. In a time when there is a low level of public trust and confidence in political leaders and the public service, this is crucial. 
""We need to pump [the public service] full of people who have the kinds of characteristics and the kind of traits that we tend to trust, and that we tend to recognise as being competent in being able to lead and make complex and difficult decisions,"" Beard tells The Mandarin. 
""And those skills, those competencies, tend to correspond to the kinds of things that we need in order to do philosophical ethics well. The two things are closely related.""
Beard argues that philosophical ethics could also be a better alternative to nudging when it comes to solving behavioural problems. 
""There is a growing trend within organisations, both public service organisations and corporate organisations, of trying to deal with the problem of people who might not always do the things that they should -- people who might behave unethically in certain times -- through behavioural economics, through nudges, by designing systems that effectively deactivate people's ability to make conscious and reflective choices,"" he says. 
""What we're dealing with there is we're trying to turn a person into more of a cog in a wheel who performs their function, and we put them in a system in which they simply perform that function by designing the system really well, so that the other options don't occur."" 
The public service's use of behavioural economics to tackle problems could contribute to the negative 'mindless, faceless bureaucracy' stereotype, Beard warns. 
""When we try to deal with the problem of unethical behaviour by doubling down on the mindlessness of it, we start to create precisely the thing that people distrust in the first place,"" he says. 
""Instead of trying to deal with the problem of unethical behaviour by deactivating people's ability to choose, if instead we focus on expanding and developing their capacity to choose well -- which is the work that philosophical ethics does -- then we start to build a public service that people are more likely to trust."" 
READ MORE: PM&C leader makes case for 'nudging' people with public policy
When undertaking a consultation process for a new policy or program, government will often utilise the expertise of academics or technical experts. While this is all well and good, Beard says considering the views of those who work in areas such as philosophy, the arts and humanities can also be beneficial for bureaucrats. In regard to complex topics, such as artificial intelligence, these people could raise questions that might otherwise go unanswered. 
""I think one of the functions of philosophy, when it is done well and applied to questions of public affairs and public service, is it allows us to see what might be missed between the gaps of disciplinary expertise,"" he says. 
The COVID-19 pandemic is a prime example of an area where skills in philosophical ethics could be beneficial to the public service, Beard says, as the pandemic is a multifaceted problem. It is not just a public health issue, but an economic and social problem as well. 
""That has meant that political leaders and the public servants providing them with advice have needed to inhabit a bunch of different value systems and a bunch of different logic systems in order to work out what is ultimately a political decision about how we respond to this,"" he explains. 
If a leader were to focus solely on solving the public health aspect of the pandemic, for example, then they might initiate a lockdown until the virus has been eradicated. 
""But of course there are enormous economic and mental health costs to that, so there has been a trade off that has needed to be made between competing value systems. And that is precisely the kind of thing that developing skills in philosophical ethics equips us for,"" Beard says. 
READ MORE: Is it ethical to use behavioural insights?
Public servants can bring philosophical ethics into their work by continuing to place people and the community at the heart of their decision-making, and by being wary of any trends -- particularly related to language -- that discourage this person-centred approach. 
""The public service has done a wonderful job in a lot of areas in embracing certain design methodologies, like human-centred design, to try to think about the people who are going to be affected by certain problems and trying to solve it with those people in mind by including them in the process,"" Beard says. 
""But they've also adopted the same kind of language as the for-profit corporations that develop these methodologies. So they're talking about the people who are affected by certain processes as users, or as customers."" 
Beard notes that the relationship between the public service and citizens is 'profoundly different' to the relationship between a business and its customers or a technological platform and its users. 
""Those language shifts seem small but they can have really significant ramifications in terms of the way we think of those relationships,"" he says. 
READ MORE: How 'ethical allies' elevate public service capability
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (91%); GOVERNMENT ETHICS (90%); ASSOCIATIONS & ORGANIZATIONS (89%); TRENDS (78%); ECONOMICS (62%)
Industry: BEHAVIORAL FINANCE (69%)
Load-Date: June 23, 2021",neutral,0.6834699511528015,balanced/neutral,[],[],"['policy', 'should', 'need to']",[],0,0,3,0
2021,Unknown Title,"Body
Sept. 17 -- Analytical Center for the Government of the Russian Federation issued the following news release:
At a round table organized by the Federation Council, experts discussed the ethical and legal problems of digital transformation. A National AI Code of Ethics will be developed and presented shortly.
""The problem of regulating artificial intelligence is becoming more and more urgent every day in connection with the widespread use of new digital opportunities. Therefore, the cross-sectoral alliance in the field of artificial intelligence, together with the Analytical Center and the Ministry of Economic Development of the Russian Federation, has developed a draft National Code in the field of AI ethics, ""said Irina Rukavishnikova, First Deputy Chairman of the Federation Council Committee on Constitutional Legislation and State Construction.
Deputy Head of the Analytical Center, Head of the Center of Expertise for the implementation of the federal project ""Artificial Intelligence"" Sergei Nakvasin specified that the Code will be presented to the public at the forum on the ethics of artificial intelligence, which the AC will hold on October 12 on behalf of the Russian Government.
The Code of Ethics will establish general principles and standards of conduct that can and should be guided by persons who take various roles in the functioning of artificial intelligence systems on the territory of Russia. The Code will be advisory in nature. It is assumed that in cases of non-compliance with ethical norms in the field of AI, the main punishment mechanism will be public censure, which will bring negative reputational consequences for unscrupulous actors.
Director of the Department for Strategic Development and Innovations of the Ministry of Economic Development Rustam Tikhonov expressed hope that with the advent of the Code, the number of companies adhering to ethical principles when working in the field of artificial intelligence will expand and there will be a transition from corporate codes of ethics to the formation of a common corporate ethical core.
The roundtable participants noted that the Code of Ethics is the first document of its kind in Russia. And legislators need to go further and think about consolidating regulations that will regulate ethical behavior when working with artificial intelligence.
Disclaimer: The Above Content is Auto-Translated
Source: Analytical Center for the Government of the Russian Federation
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (97%); ARTIFICIAL INTELLIGENCE ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); BUSINESS NEWS (90%); BUSINESS ETHICS (89%); ECONOMIC DEVELOPMENT (89%); COMPANY STRATEGY (78%); LEGISLATIVE BODIES (78%); TALKS & MEETINGS (78%); CONSTITUTIONAL LAW (69%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); DIGITALIZATION & DIGITAL TRANSFORMATION (78%)
Geographic: RUSSIAN FEDERATION (94%)
Load-Date: September 18, 2021","Sept. 17 -- Analytical Center for the Government of the Russian Federation issued the following news release:
At a round table organized by the Federation Council, experts discussed the ethical and legal problems of digital transformation. A National AI Code of Ethics will be developed and presented shortly.
""The problem of regulating artificial intelligence is becoming more and more urgent every day in connection with the widespread use of new digital opportunities. Therefore, the cross-sectoral alliance in the field of artificial intelligence, together with the Analytical Center and the Ministry of Economic Development of the Russian Federation, has developed a draft National Code in the field of AI ethics, ""said Irina Rukavishnikova, First Deputy Chairman of the Federation Council Committee on Constitutional Legislation and State Construction.
Deputy Head of the Analytical Center, Head of the Center of Expertise for the implementation of the federal project ""Artificial Intelligence"" Sergei Nakvasin specified that the Code will be presented to the public at the forum on the ethics of artificial intelligence, which the AC will hold on October 12 on behalf of the Russian Government.
The Code of Ethics will establish general principles and standards of conduct that can and should be guided by persons who take various roles in the functioning of artificial intelligence systems on the territory of Russia. The Code will be advisory in nature. It is assumed that in cases of non-compliance with ethical norms in the field of AI, the main punishment mechanism will be public censure, which will bring negative reputational consequences for unscrupulous actors.
Director of the Department for Strategic Development and Innovations of the Ministry of Economic Development Rustam Tikhonov expressed hope that with the advent of the Code, the number of companies adhering to ethical principles when working in the field of artificial intelligence will expand and there will be a transition from corporate codes of ethics to the formation of a common corporate ethical core.
The roundtable participants noted that the Code of Ethics is the first document of its kind in Russia. And legislators need to go further and think about consolidating regulations that will regulate ethical behavior when working with artificial intelligence.
Disclaimer: The Above Content is Auto-Translated
Source: Analytical Center for the Government of the Russian Federation
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (97%); ARTIFICIAL INTELLIGENCE ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); BUSINESS NEWS (90%); BUSINESS ETHICS (89%); ECONOMIC DEVELOPMENT (89%); COMPANY STRATEGY (78%); LEGISLATIVE BODIES (78%); TALKS & MEETINGS (78%); CONSTITUTIONAL LAW (69%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (92%); ARTIFICIAL INTELLIGENCE (90%); DIGITALIZATION & DIGITAL TRANSFORMATION (78%)
Geographic: RUSSIAN FEDERATION (94%)
Load-Date: September 18, 2021",neutral,0.8872413635253906,balanced/neutral,[],[],"['standards', 'legislation', 'law', 'compliance', 'should', 'need to']",[],0,0,6,0
2021,Unknown Title,"Dateline: Peshawar 
Body
Peshawar, July 18 -- US Secretary of Defense Lloyd Austin wants the Defense Department to do artificial intelligence (AI) ""the right way,"" even if our main competitor, China, does not.
Speaking to the National Security Commission on Artificial Intelligence Austin said: ""... our use of AI must reinforce our democratic values, protect our rights, ensure our safety and defend our privacy.""
This is close to the formula assigned to the fictional ""Officer Murphy"" in the 1987 movie Robocop. Murphy, a murdered police officer, was turned into Robocop, a cyborg with his human memory (mostly) erased.
He was given four directives, the first three being to serve the public trust, protect the innocent and uphold the law. He was also given a classified order, Directive 4, which blocked Robocop from causing harm to employees of Omni Consumer Products (OCP), the company that built him.
This was an attempt to install company ethics in a cyborg.
Sort of like attempting to install American military ethics in an AI-enhanced weapon. Secretary Austin appears to believe in Officer Murphy, but the ethics of warfare, practiced by the United States or our allies or our adversaries, are in the soldiers and commanders.
If we can't do this right now without AI, then we can't do it with AI-enhanced weapons.
Defense ministries around the world are anxious to infuse military systems with AI. Building on the success of surveillance and armed drones and their increasing combat importance, the US, Israel, Russia and China are all seeking autonomous war-fighting systems.
At their simplest, these are capable of carrying out a task without a person in the loop. A drone can be sent to destroy a target without any communication or control system outside the weapon itself.
When the Iranians sent cruise missiles and drones against Saudi Arabian oil installations in September 2019, it appeared to some experts that the drones that hit the Abqaiq oil facility were operating autonomously. Autonomy, however, only lets a weapon do the job it is programmed to do. AI has the weapon making decisions.
In some cases, that seems fairly straightforward.
During the Gaza conflict in May, Israel claims to have launched and operated a large swarm of drones that were managed by AI. According to recent reports, the drones each covered a preselected surveillance area and were capable of sending coordinates back to gun and mortar brigades to attack the selected targets.
The technology is said to have been developed by the Israeli army's Unit 8200 specializing in signals intelligence - roughly equivalent to the National Security Agency (NSA) in the United States. Israel has not provided detailed information about the drone swarms, but it makes sense that each drone not only could map a specific area but could also detect missile launch sites and other military activities and select them as targets.
Whether there was a person in the loop or not - the likelihood is not - isn't known.
The US is working on a number of autonomous vehicles ranging from land systems to surface and underwater naval vessels to autonomous refueling aircraft. The army, for example, is adding AI capability to land vehicles, including tanks that ultimately will be able to coordinate with surveillance drones and select the safest available roadways, predict where blockages may be and automatically take alternative action.
These systems are built on civilian technology developed in Israel - eg, WAZE - and in the US - self-driving vehicles, pioneered by Tesla.
Some of this is readily apparent in targeting and killing terrorists using drones and hellfire missiles. The US has been at this for many years now, with considerable success, but it has occasionally hit the wrong target.
These are not AI systems currently, but it is a singular path to ""improve"" them with AI so that the AI has instructions that it carries out without external decision making - ""Did we choose the right target?"" or ""Is there too great a danger of collateral damage?""
The real crunch comes when we get to the future soldier. To make the future soldier effective, he or she has to eliminate perceived threats before the threats eliminate them. Thus, the soldier is not actually a cyborg, but his or her capabilities are reaching a cyborg level of capability.
There is no reason to believe that an AI-driven system would be any better or worse than a purely human-operated system. Furthermore, in a complex combat environment, AI might do a better job than stressed humans fighting for their lives.
AI can be programmed to obey some ""ethical"" rules, particularly when it comes to civilians in a combat environment, but commanders may find this programming interferes with the mission if built-in ""ethical"" limitations endanger warfighters.
For example, an AI system might lock a soldier's gun to prevent a civilian from being killed, when in fact the civilian is a combatant or terrorist.
In urban terror events, it is virtually impossible to know who is, or is not, a civilian. Israel, for example, has been struggling, with or without AI, how to minimize civilian casualties when terrorists launch rockets from schools and apartment buildings.
AI is not going to solve this problem by itself or even in combination with human operators. Our adversaries are not in the least worried about constraints on the use of AI. While it is practically impossible to make AI ""ethical,"" it is possible and absolutely essential to press our military and civilian leaders to act ethically and unleash weapons only when justified and essential for our security.
In short, while ethical AI may be a myth, ethical leaders are not in the least mythological.
Published by HT Digital Content Services with permission from The Statesman. For any query with respect to this article or any other content requirement, please contact Editor at contentservices@htlive.com
Classification
Language: ENGLISH
Publication-Type: Newspaper
Subject: ETHICS (92%); DEFENSE DEPARTMENTS (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); GOVERNMENT ADVISORS & MINISTERS (90%); WEAPONS & ARMS (90%); GOVERNMENT DEPARTMENTS & AUTHORITIES (89%); INTELLIGENCE SERVICES (89%); SURVEILLANCE (89%); ARMIES (78%); BUSINESS ETHICS (78%); GOVERNMENT ETHICS (78%); ISRAELI-PALESTINIAN CONFLICTS (78%); NATIONAL SECURITY (78%); SMART WEAPONS (78%); SPECIAL INVESTIGATIVE FORCES (78%); LAW ENFORCEMENT (77%); SAFETY (77%); US FEDERAL GOVERNMENT (77%); ARMED FORCES (72%); MEMORY (55%)
Industry: DEFENSE DEPARTMENTS (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ARMIES (78%); DEFENSE ELECTRONICS (78%); ARMED FORCES (72%); CONSUMER PRODUCTS (68%)
Geographic: ISRAEL (94%); UNITED STATES (94%); CHINA (92%); PAKISTAN (92%); RUSSIAN FEDERATION (79%); SAUDI ARABIA (79%); STATE OF PALESTINE (79%)
Load-Date: July 18, 2021","Peshawar, July 18 -- US Secretary of Defense Lloyd Austin wants the Defense Department to do artificial intelligence (AI) ""the right way,"" even if our main competitor, China, does not.
Speaking to the National Security Commission on Artificial Intelligence Austin said: ""... our use of AI must reinforce our democratic values, protect our rights, ensure our safety and defend our privacy.""
This is close to the formula assigned to the fictional ""Officer Murphy"" in the 1987 movie Robocop. Murphy, a murdered police officer, was turned into Robocop, a cyborg with his human memory (mostly) erased.
He was given four directives, the first three being to serve the public trust, protect the innocent and uphold the law. He was also given a classified order, Directive 4, which blocked Robocop from causing harm to employees of Omni Consumer Products (OCP), the company that built him.
This was an attempt to install company ethics in a cyborg.
Sort of like attempting to install American military ethics in an AI-enhanced weapon. Secretary Austin appears to believe in Officer Murphy, but the ethics of warfare, practiced by the United States or our allies or our adversaries, are in the soldiers and commanders.
If we can't do this right now without AI, then we can't do it with AI-enhanced weapons.
Defense ministries around the world are anxious to infuse military systems with AI. Building on the success of surveillance and armed drones and their increasing combat importance, the US, Israel, Russia and China are all seeking autonomous war-fighting systems.
At their simplest, these are capable of carrying out a task without a person in the loop. A drone can be sent to destroy a target without any communication or control system outside the weapon itself.
When the Iranians sent cruise missiles and drones against Saudi Arabian oil installations in September 2019, it appeared to some experts that the drones that hit the Abqaiq oil facility were operating autonomously. Autonomy, however, only lets a weapon do the job it is programmed to do. AI has the weapon making decisions.
In some cases, that seems fairly straightforward.
During the Gaza conflict in May, Israel claims to have launched and operated a large swarm of drones that were managed by AI. According to recent reports, the drones each covered a preselected surveillance area and were capable of sending coordinates back to gun and mortar brigades to attack the selected targets.
The technology is said to have been developed by the Israeli army's Unit 8200 specializing in signals intelligence - roughly equivalent to the National Security Agency (NSA) in the United States. Israel has not provided detailed information about the drone swarms, but it makes sense that each drone not only could map a specific area but could also detect missile launch sites and other military activities and select them as targets.
Whether there was a person in the loop or not - the likelihood is not - isn't known.
The US is working on a number of autonomous vehicles ranging from land systems to surface and underwater naval vessels to autonomous refueling aircraft. The army, for example, is adding AI capability to land vehicles, including tanks that ultimately will be able to coordinate with surveillance drones and select the safest available roadways, predict where blockages may be and automatically take alternative action.
These systems are built on civilian technology developed in Israel - eg, WAZE - and in the US - self-driving vehicles, pioneered by Tesla.
Some of this is readily apparent in targeting and killing terrorists using drones and hellfire missiles. The US has been at this for many years now, with considerable success, but it has occasionally hit the wrong target.
These are not AI systems currently, but it is a singular path to ""improve"" them with AI so that the AI has instructions that it carries out without external decision making - ""Did we choose the right target?"" or ""Is there too great a danger of collateral damage?""
The real crunch comes when we get to the future soldier. To make the future soldier effective, he or she has to eliminate perceived threats before the threats eliminate them. Thus, the soldier is not actually a cyborg, but his or her capabilities are reaching a cyborg level of capability.
There is no reason to believe that an AI-driven system would be any better or worse than a purely human-operated system. Furthermore, in a complex combat environment, AI might do a better job than stressed humans fighting for their lives.
AI can be programmed to obey some ""ethical"" rules, particularly when it comes to civilians in a combat environment, but commanders may find this programming interferes with the mission if built-in ""ethical"" limitations endanger warfighters.
For example, an AI system might lock a soldier's gun to prevent a civilian from being killed, when in fact the civilian is a combatant or terrorist.
In urban terror events, it is virtually impossible to know who is, or is not, a civilian. Israel, for example, has been struggling, with or without AI, how to minimize civilian casualties when terrorists launch rockets from schools and apartment buildings.
AI is not going to solve this problem by itself or even in combination with human operators. Our adversaries are not in the least worried about constraints on the use of AI. While it is practically impossible to make AI ""ethical,"" it is possible and absolutely essential to press our military and civilian leaders to act ethically and unleash weapons only when justified and essential for our security.
In short, while ethical AI may be a myth, ethical leaders are not in the least mythological.
Published by HT Digital Content Services with permission from The Statesman. For any query with respect to this article or any other content requirement, please contact Editor at contentservices@htlive.com
Classification
Language: ENGLISH
Publication-Type: Newspaper
Subject: ETHICS (92%); DEFENSE DEPARTMENTS (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); GOVERNMENT ADVISORS & MINISTERS (90%); WEAPONS & ARMS (90%); GOVERNMENT DEPARTMENTS & AUTHORITIES (89%); INTELLIGENCE SERVICES (89%); SURVEILLANCE (89%); ARMIES (78%); BUSINESS ETHICS (78%); GOVERNMENT ETHICS (78%); ISRAELI-PALESTINIAN CONFLICTS (78%); NATIONAL SECURITY (78%); SMART WEAPONS (78%); SPECIAL INVESTIGATIVE FORCES (78%); LAW ENFORCEMENT (77%); SAFETY (77%); US FEDERAL GOVERNMENT (77%); ARMED FORCES (72%); MEMORY (55%)
Industry: DEFENSE DEPARTMENTS (91%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ARMIES (78%); DEFENSE ELECTRONICS (78%); ARMED FORCES (72%); CONSUMER PRODUCTS (68%)
Geographic: ISRAEL (94%); UNITED STATES (94%); CHINA (92%); PAKISTAN (92%); RUSSIAN FEDERATION (79%); SAUDI ARABIA (79%); STATE OF PALESTINE (79%)
Load-Date: July 18, 2021",neutral,0.8575199842453003,balanced/neutral,"['privacy', 'surveillance', 'safety', 'security', 'autonomy', 'agency']",['autonomy'],"['law', 'must']",['drone'],6,1,2,1
2021,Unknown Title,"Byline: Sohini Bagchi
Body
Ethical AI can no longer an afterthought for the enterprise; it must be built into the fabric of AI, believe experts.
As Artificial Intelligence (AI) begins to play a much larger role in our daily lives, streamlining our work, resolving customer issues, talking to us as companion bots, driving autonomous cars, and helping employees make more informed and faster decisions, ethical AI can no longer an afterthought for the enterprise; it must be built into the fabric of AI.
""AI ethics"" refers to the organizational constructs that reaffirms commitment to corporate values, policies, codes of ethics, and guiding principles in the age of AI. These constructs set guidelines and governance for AI throughout the organization, from research and design, to build and train, to change and operate,"" says According to Prashanth Kaddi, Partner, Deloitte India.
Issues around ethical AI have garnered more attention over the past several years with tech giants from Facebook to Google to Microsoft and IBM have already established and published principles to demonstrate to stakeholders - customers, employees, and investors - that they understand the importance of ethical or responsible AI.
The pandemic has further proved that businesses are betting big on AI, with analyst firms forecasting AI investments to grow from $27.23 billion in 2019 to $266.92 billion by 2027. And as investments increase, the need to give the technology a ""moral compass"" has become more urgent.
However, there is growing evidence that AI based applications can lead to increased discrimination based on gender, class, caste, ethnicity, religion and other identity forming characteristics. As Prof. Amit Prakash, Associate Professor and Coordinator at IIIT-Bangalore observes, ""This can come through an inadequate attention to the processes associated with collection of digital data used to train the AI models as well as through algorithmic biases, which get introduced when design teams are not sensitive to, or even aware of, the diversity in the implementation context.""
He believes, more than ushering in any transformation, such AI applications reinforce the status quo, both in the business and societal landscapes. ""It should, therefore, be a strategic imperative for AI technology designers and policy makers to engage more closely with the various dimensions of ethics.""
Eliminating the biases in AI
Experts outline that ethical AI should be based on principles such as accountability, equity, fairness, human agency, inclusiveness, transparency, security and privacy. They believe, organizational leaders need to ensure their AI teams have expertise on these equity aspects, while approving staffing decisions.
Recent articles note, racism that has been built into AI-based risk assessment algorithms used by the healthcare industry is responsible for a 46% failure rate in identifying at-risk patients of color. The IT industry too is unconsciously biased against women and people of color, though the industry has made great strides and is actively working to change that.
According to Shubhangi Vashisth, Senior Principal Analyst (AI) at Gartner, there is a need to recognize the challenges in AI-systems that deliver biased results. She cites an example of a hiring tool that reinforces racial discrimination or entrenches these prejudices against certain communities. Oftentimes users and developers are not aware of the system's process to reach the output. ""This opacity increases the bias in datasets and decision systems,"" believes Vashisth.
Taking fairness as a parameter, Vashisth explains that it means that the AI systems should be inclusive, accessible and should not have unfair discrimination against individuals, communities, or groups. It should provide equitable access and treatment to everyone. Bringing more diversity into the team can however mitigate the bias often developed by algorithms, according to Vashisth.
Transparency and explainability are also the keys to developing ethical AI applications. Unconscious biases must be prevented or removed, and human review processes must occur regularly. As Siddhesh Naik, Data, AI & Automation Sales Leader, IBM Technology Sales, India / South Asia, explains, ""Companies must be clear about who trains their AI systems, what data was used in training and, most importantly, what went into their algorithms' recommendations. Organizations who want to employ AI to unlock new value and insights, to accelerate discovery or to gain competitive edge have a fundamental responsibility to foster trust in the technology.""
""AI models can be complex, hence limiting transparency, explainability and being unbiased. This undermines the trust of companies and customers- hence the need for a clear vision and governance structure of AI from enterprises. Being able to explain AI based decisions and ensuring a fair unbiased decision making algorithm are prudent to organization's market reputation and trust externally as well as internally,"" Kaddi says.
""Humans delegate a lot of decisions to AI systems. And this will only increase. For instance, if you have used Gmail's smart compose feature, chances are you have accepted its suggestions more often than not. For organizations using AI in their businesses or those helping clients adopt AI - good intentions are no longer enough. 'Do no harm' is no longer enough,"" Satish Viswanathan, Head of Social Change, Thoughtworks, says.
Eliminating such biases, whether they are conscious or not, is vital for AI to be trusted and accepted in society, as Vashisth says, ""If individuals and teams are cognizant of the existence of bias, then they have the necessary tools at the data, algorithm, and human levels to build a more responsible AI.""
Ethical AI trends
Talking about the ethical AI trends in the market, in 2022, Rahul Joshi, CTO, CSS Corp notes that there will be a high demand for responsible AI solutions in the market. Responsible AI solutions offer a range of capabilities that help companies turn AI principles such as fairness and transparency into consistent practices.
""If we look at the industry today, most tech or non-tech organizations generate consumer benefits and business value by leveraging 70% to 80% AI-led operations and creating AI-infused products and applications,"" says Joshi, adding that while AI can be a helpful tool to increase productivity and reduce the need for people to perform repetitive tasks, it can also give rise to a host of significant unintended (or maliciously intended) consequences for individuals, organizations, and society.
He believes, there are many cases where the algorithms cause problems by replicating the (often unconscious) biases of the developers/programmers who built them. So, it's crucial to ensure that comprehensive datasets are used. Most organizations have bias bounties in place, and this trend will run rampant in the coming years.
""To ensure that no company is marred with data or AI ethics enrage that can impact its reputation and revenue, it's imperative to build an ethical & responsible AI,"" says Joshi.
According to Viswanathan, ""Organizations should proactively examine the underlying principles that their AI systems adhere to. Leaders pursuing responsible technology should embed ethics and responsible use of AI as a basic tenet in their technology strategies. We also expect organizations to work with ethical frameworks and build a movement where explainability needs to be a first principle when building AI systems - if you can't explain it, don't use it."" -
""More organizations will need to adopt standardized frameworks, and responsible AI engineering practices that will instill a sense of fairness in their data management. Furthermore, organizations should also put in place a formal governance process to mitigate any ethical and compliance risks that may emerge in the future,"" agrees Sangram Kadam, Vice President and Business Head (APAC & META), Birlasoft.
When implemented in the right way, AI ethics can help an organization create competitive differentiation by inculcating trust in AI applications. They will also be able to attract and establish greater customer confidence which will have a larger impact in the long run towards driving accelerated business results, says Kadam.
Final Thoughts
As AI plays a larger role in our lives, it is crucial to build a framework of ethical and transparent AI and business leaders must make ethics a priority in their AI endeavors for this to occur.
Experts also note that regardless of guidelines and frameworks for ethical AI, the need of the hour is to also use top-quality data for training the AI. Poor, incomplete, skewed and biased data is often the root cause of AI bias. Hence the unconscious biases must be eliminated, and diverse voices need to play a role in the discussion and development of AI to ensure that new biases are not introduced.
If we reduce data bias and launch the technology on an ethical backbone, we can create new, interesting and trustworthy futures for AI - one where AI does not confuse us or harm businesses or cause social damage.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ARTIFICIAL INTELLIGENCE ETHICS (94%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); NEGATIVE SOCIETAL NEWS (89%); RACE & ETHNICITY (89%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (79%); INDUSTRY ANALYSTS (78%); RACISM & XENOPHOBIA (78%); DISCRIMINATION (77%); HUMAN RESOURCES & PERSONNEL MANAGEMENT (77%); NEGATIVE NEWS (77%); GENDER & SEX DISCRIMINATION IN EMPLOYMENT (76%); RISK MANAGEMENT (75%); GENDER & SEX DISCRIMINATION (72%); UNCONSCIOUS BIAS (72%); COMPUTING & IT SECTOR PERFORMANCE (71%); HEALTH CARE SECTOR PERFORMANCE (71%)
Company:  META PLATFORMS INC (56%);  DELOITTE LLP (56%);  MICROSOFT CORP (56%);  GOOGLE LLC (56%)
Ticker: META (NASDAQ) (56%); MSFT (NASDAQ) (56%)
Industry: NAICS516210 MEDIA STREAMING DIST SVCS, SOCIAL NETWORKS, AND OTHER MEDIA NETWORKS AND CONTENT PROVIDERS (56%); SIC7374 COMPUTER PROCESSING & DATA PREPARATION & PROCESSING SERVICES (56%); NAICS541211 OFFICES OF CERTIFIED PUBLIC ACCOUNTANTS (56%); SIC8721 ACCOUNTING, AUDITING, & BOOKKEEPING SERVICES (56%); SIC7372 PREPACKAGED SOFTWARE (56%); NAICS519290 WEB SEARCH PORTALS AND ALL OTHER INFORMATION SERVICES (56%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); BIG TECH (78%); INDUSTRY ANALYSTS (78%); INFORMATION TECHNOLOGY INDUSTRY (78%); RISK MANAGEMENT (75%); COMPUTING & IT SECTOR PERFORMANCE (71%); HEALTH CARE SECTOR PERFORMANCE (71%); COMPUTING & INFORMATION TECHNOLOGY (68%); INFORMATION SECURITY & PRIVACY (67%); HEALTH CARE (60%); AUTONOMOUS MOTOR VEHICLES (57%)
Geographic: BANGALORE, KARNATAKA, INDIA (56%); INDIA (71%)
Load-Date: July 18, 2023","Ethical AI can no longer an afterthought for the enterprise; it must be built into the fabric of AI, believe experts.
As Artificial Intelligence (AI) begins to play a much larger role in our daily lives, streamlining our work, resolving customer issues, talking to us as companion bots, driving autonomous cars, and helping employees make more informed and faster decisions, ethical AI can no longer an afterthought for the enterprise; it must be built into the fabric of AI.
""AI ethics"" refers to the organizational constructs that reaffirms commitment to corporate values, policies, codes of ethics, and guiding principles in the age of AI. These constructs set guidelines and governance for AI throughout the organization, from research and design, to build and train, to change and operate,"" says According to Prashanth Kaddi, Partner, Deloitte India.
Issues around ethical AI have garnered more attention over the past several years with tech giants from Facebook to Google to Microsoft and IBM have already established and published principles to demonstrate to stakeholders - customers, employees, and investors - that they understand the importance of ethical or responsible AI.
The pandemic has further proved that businesses are betting big on AI, with analyst firms forecasting AI investments to grow from $27.23 billion in 2019 to $266.92 billion by 2027. And as investments increase, the need to give the technology a ""moral compass"" has become more urgent.
However, there is growing evidence that AI based applications can lead to increased discrimination based on gender, class, caste, ethnicity, religion and other identity forming characteristics. As Prof. Amit Prakash, Associate Professor and Coordinator at IIIT-Bangalore observes, ""This can come through an inadequate attention to the processes associated with collection of digital data used to train the AI models as well as through algorithmic biases, which get introduced when design teams are not sensitive to, or even aware of, the diversity in the implementation context.""
He believes, more than ushering in any transformation, such AI applications reinforce the status quo, both in the business and societal landscapes. ""It should, therefore, be a strategic imperative for AI technology designers and policy makers to engage more closely with the various dimensions of ethics.""
Eliminating the biases in AI
Experts outline that ethical AI should be based on principles such as accountability, equity, fairness, human agency, inclusiveness, transparency, security and privacy. They believe, organizational leaders need to ensure their AI teams have expertise on these equity aspects, while approving staffing decisions.
Recent articles note, racism that has been built into AI-based risk assessment algorithms used by the healthcare industry is responsible for a 46% failure rate in identifying at-risk patients of color. The IT industry too is unconsciously biased against women and people of color, though the industry has made great strides and is actively working to change that.
According to Shubhangi Vashisth, Senior Principal Analyst (AI) at Gartner, there is a need to recognize the challenges in AI-systems that deliver biased results. She cites an example of a hiring tool that reinforces racial discrimination or entrenches these prejudices against certain communities. Oftentimes users and developers are not aware of the system's process to reach the output. ""This opacity increases the bias in datasets and decision systems,"" believes Vashisth.
Taking fairness as a parameter, Vashisth explains that it means that the AI systems should be inclusive, accessible and should not have unfair discrimination against individuals, communities, or groups. It should provide equitable access and treatment to everyone. Bringing more diversity into the team can however mitigate the bias often developed by algorithms, according to Vashisth.
Transparency and explainability are also the keys to developing ethical AI applications. Unconscious biases must be prevented or removed, and human review processes must occur regularly. As Siddhesh Naik, Data, AI & Automation Sales Leader, IBM Technology Sales, India / South Asia, explains, ""Companies must be clear about who trains their AI systems, what data was used in training and, most importantly, what went into their algorithms' recommendations. Organizations who want to employ AI to unlock new value and insights, to accelerate discovery or to gain competitive edge have a fundamental responsibility to foster trust in the technology.""
""AI models can be complex, hence limiting transparency, explainability and being unbiased. This undermines the trust of companies and customers- hence the need for a clear vision and governance structure of AI from enterprises. Being able to explain AI based decisions and ensuring a fair unbiased decision making algorithm are prudent to organization's market reputation and trust externally as well as internally,"" Kaddi says.
""Humans delegate a lot of decisions to AI systems. And this will only increase. For instance, if you have used Gmail's smart compose feature, chances are you have accepted its suggestions more often than not. For organizations using AI in their businesses or those helping clients adopt AI - good intentions are no longer enough. 'Do no harm' is no longer enough,"" Satish Viswanathan, Head of Social Change, Thoughtworks, says.
Eliminating such biases, whether they are conscious or not, is vital for AI to be trusted and accepted in society, as Vashisth says, ""If individuals and teams are cognizant of the existence of bias, then they have the necessary tools at the data, algorithm, and human levels to build a more responsible AI.""
Ethical AI trends
Talking about the ethical AI trends in the market, in 2022, Rahul Joshi, CTO, CSS Corp notes that there will be a high demand for responsible AI solutions in the market. Responsible AI solutions offer a range of capabilities that help companies turn AI principles such as fairness and transparency into consistent practices.
""If we look at the industry today, most tech or non-tech organizations generate consumer benefits and business value by leveraging 70% to 80% AI-led operations and creating AI-infused products and applications,"" says Joshi, adding that while AI can be a helpful tool to increase productivity and reduce the need for people to perform repetitive tasks, it can also give rise to a host of significant unintended (or maliciously intended) consequences for individuals, organizations, and society.
He believes, there are many cases where the algorithms cause problems by replicating the (often unconscious) biases of the developers/programmers who built them. So, it's crucial to ensure that comprehensive datasets are used. Most organizations have bias bounties in place, and this trend will run rampant in the coming years.
""To ensure that no company is marred with data or AI ethics enrage that can impact its reputation and revenue, it's imperative to build an ethical & responsible AI,"" says Joshi.
According to Viswanathan, ""Organizations should proactively examine the underlying principles that their AI systems adhere to. Leaders pursuing responsible technology should embed ethics and responsible use of AI as a basic tenet in their technology strategies. We also expect organizations to work with ethical frameworks and build a movement where explainability needs to be a first principle when building AI systems - if you can't explain it, don't use it."" -
""More organizations will need to adopt standardized frameworks, and responsible AI engineering practices that will instill a sense of fairness in their data management. Furthermore, organizations should also put in place a formal governance process to mitigate any ethical and compliance risks that may emerge in the future,"" agrees Sangram Kadam, Vice President and Business Head (APAC & META), Birlasoft.
When implemented in the right way, AI ethics can help an organization create competitive differentiation by inculcating trust in AI applications. They will also be able to attract and establish greater customer confidence which will have a larger impact in the long run towards driving accelerated business results, says Kadam.
Final Thoughts
As AI plays a larger role in our lives, it is crucial to build a framework of ethical and transparent AI and business leaders must make ethics a priority in their AI endeavors for this to occur.
Experts also note that regardless of guidelines and frameworks for ethical AI, the need of the hour is to also use top-quality data for training the AI. Poor, incomplete, skewed and biased data is often the root cause of AI bias. Hence the unconscious biases must be eliminated, and diverse voices need to play a role in the discussion and development of AI to ensure that new biases are not introduced.
If we reduce data bias and launch the technology on an ethical backbone, we can create new, interesting and trustworthy futures for AI - one where AI does not confuse us or harm businesses or cause social damage.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ARTIFICIAL INTELLIGENCE ETHICS (94%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); NEGATIVE SOCIETAL NEWS (89%); RACE & ETHNICITY (89%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (79%); INDUSTRY ANALYSTS (78%); RACISM & XENOPHOBIA (78%); DISCRIMINATION (77%); HUMAN RESOURCES & PERSONNEL MANAGEMENT (77%); NEGATIVE NEWS (77%); GENDER & SEX DISCRIMINATION IN EMPLOYMENT (76%); RISK MANAGEMENT (75%); GENDER & SEX DISCRIMINATION (72%); UNCONSCIOUS BIAS (72%); COMPUTING & IT SECTOR PERFORMANCE (71%); HEALTH CARE SECTOR PERFORMANCE (71%)
Company:  META PLATFORMS INC (56%);  DELOITTE LLP (56%);  MICROSOFT CORP (56%);  GOOGLE LLC (56%)
Ticker: META (NASDAQ) (56%); MSFT (NASDAQ) (56%)
Industry: NAICS516210 MEDIA STREAMING DIST SVCS, SOCIAL NETWORKS, AND OTHER MEDIA NETWORKS AND CONTENT PROVIDERS (56%); SIC7374 COMPUTER PROCESSING & DATA PREPARATION & PROCESSING SERVICES (56%); NAICS541211 OFFICES OF CERTIFIED PUBLIC ACCOUNTANTS (56%); SIC8721 ACCOUNTING, AUDITING, & BOOKKEEPING SERVICES (56%); SIC7372 PREPACKAGED SOFTWARE (56%); NAICS519290 WEB SEARCH PORTALS AND ALL OTHER INFORMATION SERVICES (56%); ARTIFICIAL INTELLIGENCE ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); BIG TECH (78%); INDUSTRY ANALYSTS (78%); INFORMATION TECHNOLOGY INDUSTRY (78%); RISK MANAGEMENT (75%); COMPUTING & IT SECTOR PERFORMANCE (71%); HEALTH CARE SECTOR PERFORMANCE (71%); COMPUTING & INFORMATION TECHNOLOGY (68%); INFORMATION SECURITY & PRIVACY (67%); HEALTH CARE (60%); AUTONOMOUS MOTOR VEHICLES (57%)
Geographic: BANGALORE, KARNATAKA, INDIA (56%); INDIA (71%)
Load-Date: July 18, 2023",neutral,0.6289655566215515,balanced/neutral,"['privacy', 'bias', 'discrimination', 'fairness', 'transparency', 'explainability', 'accountability', 'security', 'agency', 'access']","['fairness', 'equity']","['policy', 'governance', 'guidelines', 'framework', 'compliance', 'should', 'must', 'need to']",['algorithm'],10,2,8,1
2021,Unknown Title,"Body
Dublin, Nov.  24, 2021  (GLOBE NEWSWIRE) -- The ""Ethical AI is Pivotal to the Maximization of the Future Growth Potential of the Global AI Market"" report has been added to ResearchAndMarkets.com's offering.
Artificial intelligence (AI) is transforming organizations, industries, and the technology landscape. The world is moving to the increased adoption of AI-powered smart applications/systems, and this trend will increase exponentially over the next few years. AI technologies are maturing, and the need to leverage their capabilities is becoming a CXO priority.As businesses make AI part of their core strategy, the transformation of business functions, measures, and controls to ensure ethical best practices will gain importance. The implementation and the governance of ethical AI practices will become a priority and a board-level concern.
The deployment of AI solutions that are ethical (from a regulatory and a legal standpoint), transparent, and without bias will become essential. As governments and industry bodies across the world articulate AI regulations, AI companies must establish their ethical frameworks until roadmaps are clearly defined.The operationalization of ethical AI principles is challenging for enterprises, given the large volumes of user-centric data that need to be processed, the breadth of use-cases, the regulatory variations in operating markets, and the diverse stakeholder priorities.
This also opens up opportunities for technology vendors and service providers. To effectively partner with enterprises and monetize these opportunities, ICT providers need to assess potential areas impacting AI ethics and evaluate opportunities across the people-process-technology spectrum.Forward-thinking technology and service companies, including large ICT providers and start-ups, are working with enterprises and industry stakeholders to leverage potential opportunities. Ethical challenges will continue to be discovered and remediated to create sustained growth in potential advisory services.
As enterprises define goals, values, strategic outcomes, and key performance metrics, the time is right for technology companies to strategically partner with enterprises in the detection and the mitigation of ethical AI concerns.
Key Topics Covered:
1. Strategic Imperatives
Why is it Increasingly Difficult to Grow?
The Strategic Imperative
The Impact of the Top Three Strategic Imperatives on Ethical AI
Growth Opportunities Fuel the Growth Pipeline Engine
2. Growth Environment
AI Growth Drivers
AI Key Use-Cases
AI Ethics and Challenges to Adoption
3. Growth Opportunity Analysis
Increasing Ecosystem Focus on Ethical AI
Ethical AI Principles Require the Addressal of Multiple Elements/Dimensions
Growth Opportunity Segments
People
Process
Technology
The Way Forward
4. Growth Opportunity Universe
Growth Opportunity 1: Consulting and Advisory Services for AI Roadmaps and Ethical Considerations
For more information about this report visit https://www.researchandmarkets.com/r/48wz78
CONTACT: ResearchAndMarkets.com          Laura Wood, Senior Press Manager          press@researchandmarkets.com          For E.S.T Office Hours Call 1-917-300-0470          For U.S./CAN Toll Free Call 1-800-526-8630          For GMT Office Hours Call +353-1-416-8900 
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ARTIFICIAL INTELLIGENCE ETHICS (93%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); BUSINESS REPORTS & FORECASTS (90%); MARKET RESEARCH REPORTS (90%); NEWS BRIEFS (90%); PRESS RELEASES (90%); BEST PRACTICES (78%); TRENDS (78%); BUSINESS & PROFESSIONAL ASSOCIATIONS (77%); TECHNOLOGY MATURITY (77%); BUSINESS METRICS (75%); ASSOCIATIONS & ORGANIZATIONS (72%); ARTIFICIAL INTELLIGENCE (%); ETHICAL AI (%); SMART APPLICATIONS (%)
Company: RESEARCH AND MARKETS
Industry: ARTIFICIAL INTELLIGENCE ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); MARKET RESEARCH REPORTS (90%); INFORMATION TECHNOLOGY INDUSTRY (78%); Support Services (%)
Company-Terms: Support Services Research and Markets Dublin ::issuer-country=IE:: IE
Load-Date: November 24, 2021","Dublin, Nov.  24, 2021  (GLOBE NEWSWIRE) -- The ""Ethical AI is Pivotal to the Maximization of the Future Growth Potential of the Global AI Market"" report has been added to ResearchAndMarkets.com's offering.
Artificial intelligence (AI) is transforming organizations, industries, and the technology landscape. The world is moving to the increased adoption of AI-powered smart applications/systems, and this trend will increase exponentially over the next few years. AI technologies are maturing, and the need to leverage their capabilities is becoming a CXO priority.As businesses make AI part of their core strategy, the transformation of business functions, measures, and controls to ensure ethical best practices will gain importance. The implementation and the governance of ethical AI practices will become a priority and a board-level concern.
The deployment of AI solutions that are ethical (from a regulatory and a legal standpoint), transparent, and without bias will become essential. As governments and industry bodies across the world articulate AI regulations, AI companies must establish their ethical frameworks until roadmaps are clearly defined.The operationalization of ethical AI principles is challenging for enterprises, given the large volumes of user-centric data that need to be processed, the breadth of use-cases, the regulatory variations in operating markets, and the diverse stakeholder priorities.
This also opens up opportunities for technology vendors and service providers. To effectively partner with enterprises and monetize these opportunities, ICT providers need to assess potential areas impacting AI ethics and evaluate opportunities across the people-process-technology spectrum.Forward-thinking technology and service companies, including large ICT providers and start-ups, are working with enterprises and industry stakeholders to leverage potential opportunities. Ethical challenges will continue to be discovered and remediated to create sustained growth in potential advisory services.
As enterprises define goals, values, strategic outcomes, and key performance metrics, the time is right for technology companies to strategically partner with enterprises in the detection and the mitigation of ethical AI concerns.
Key Topics Covered:
1. Strategic Imperatives
Why is it Increasingly Difficult to Grow?
The Strategic Imperative
The Impact of the Top Three Strategic Imperatives on Ethical AI
Growth Opportunities Fuel the Growth Pipeline Engine
2. Growth Environment
AI Growth Drivers
AI Key Use-Cases
AI Ethics and Challenges to Adoption
3. Growth Opportunity Analysis
Increasing Ecosystem Focus on Ethical AI
Ethical AI Principles Require the Addressal of Multiple Elements/Dimensions
Growth Opportunity Segments
People
Process
Technology
The Way Forward
4. Growth Opportunity Universe
Growth Opportunity 1: Consulting and Advisory Services for AI Roadmaps and Ethical Considerations
For more information about this report visit https://www.researchandmarkets.com/r/48wz78
CONTACT: ResearchAndMarkets.com          Laura Wood, Senior Press Manager          press@researchandmarkets.com          For E.S.T Office Hours Call 1-917-300-0470          For U.S./CAN Toll Free Call 1-800-526-8630          For GMT Office Hours Call +353-1-416-8900 
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ARTIFICIAL INTELLIGENCE ETHICS (93%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); BUSINESS REPORTS & FORECASTS (90%); MARKET RESEARCH REPORTS (90%); NEWS BRIEFS (90%); PRESS RELEASES (90%); BEST PRACTICES (78%); TRENDS (78%); BUSINESS & PROFESSIONAL ASSOCIATIONS (77%); TECHNOLOGY MATURITY (77%); BUSINESS METRICS (75%); ASSOCIATIONS & ORGANIZATIONS (72%); ARTIFICIAL INTELLIGENCE (%); ETHICAL AI (%); SMART APPLICATIONS (%)
Company: RESEARCH AND MARKETS
Industry: ARTIFICIAL INTELLIGENCE ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); MARKET RESEARCH REPORTS (90%); INFORMATION TECHNOLOGY INDUSTRY (78%); Support Services (%)
Company-Terms: Support Services Research and Markets Dublin ::issuer-country=IE:: IE
Load-Date: November 24, 2021",neutral,0.5731924772262573,balanced/neutral,['bias'],[],"['governance', 'must', 'need to']",[],1,0,3,0
2021,Unknown Title,"Body
ABSTRACT
The report says that Defence's challenge is that failure to adopt the emerging technologies in a timely manner may result in a military disadvantage, while premature adoption without sufficient research and analysis may result in inadvertent harms.
FULL TEXT
The Department of Defence has launched a technical report aiming to assist in the development of ethical artificial intelligence systems for Defence. 
The report, published on Tuesday, noted that while there is potential for AI technology to increase Defence capability and reduce risk in military operations, significant steps must be undertaken to ensure that introduction of the technology does not result in ""adverse outcomes"". 
""Defence's challenge is that failure to adopt the emerging technologies in a timely manner may result in a military disadvantage, while premature adoption without sufficient research and analysis may result in inadvertent harms,"" it said. 
To address this, Plan Jericho, the Defence Science and Technology Group, and the Trusted Autonomous Defence Cooperative Research Centre ran a workshop in Canberra in 2019, with the aim of developing a pragmatic ethical methodology for AI projects in Defence. 
Attended by more than 100 representatives from Defence, other government agencies, industry, academia, international organisations and media, the discussions and theories which came out of the workshop have been outlined in the new report. 
Read more: Defence experts discuss ethical AI, while NSW wants to ensure AI in government places customers at the centre
Attendees identified five facets of ethical AI in Defence, including: 
Responsibility -- who is responsible for AI?   
Governance -- how is AI controlled?   
Trust -- how can AI be trusted?   
Law -- how can AI be used lawfully?   
Traceability -- how are the actions of AI recorded?
 They also put forward 20 topics to be explored when considering AI, such as transparency, safety, accountability, human factors, supply chain, and misuse and risks.  Chief defence scientist Professor Tanya Monro said AI can offer benefits such as removing humans from high-threat environments and improving Australian advantage by providing more in-depth and faster situational awareness, but there are risks.  ""Upfront engagement on AI technologies, and consideration of ethical aspects needs to occur in parallel with technology development,"" she said in a statement on Tuesday.  The paper outlined three tools to assist Defence and Industry in developing AI systems for Defence, including: an AI Checklist for the development of ethical AI systems; an Ethical AI Risk Matrix to describe identified risks and proposed treatment; and, for larger programs, a data item descriptor for contractors to develop a formal Legal, Ethical and Assurance Program Plan to be included in project documentation for AI programs where an ethical risk assessment is above a certain threshold.    Read more: Former digital government boss criticises new ethical AI guidelines    Air Vice-Marshal Cath Roberts, head of air force capability said AI and human-machine teaming would play a ""pivotal"" role for air and space power in the future.  Ethical and legal issues must be resolved at the same pace that the technology is developed, she said.  ""This paper is useful in suggesting consideration of ethical issues that may arise to ensure responsibility for AI systems within traceable systems of control,"" she said.  ""Practical application of these tools into projects such as the Loyal Wingman will assist Defence to explore autonomy, AI, and teaming concepts in an iterative, learning and collaborative way.""  The release of the findings has come a week after the CSIRO released a study showing that AI can be used to influence human decision-making by exploiting vulnerabilities in an individual's habits and patterns.  CSIRO Data61 director Dr Jon Whittle said the study was further proof that AI technologies have ""tremendous potential"" to provide societal benefit, but also ethical risks.  ""Like any technology, AI could be used for good or bad, so proper governance is critical to ensure that AI and machine learning are implemented in a responsible manner. Organisations need to ensure they are educated on what these technologies can and cannot do and be aware of potential risks as well as rewards,"" he said.    Read more: AI can now learn to manipulate human behaviour     
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ARTIFICIAL INTELLIGENCE ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); DEFENSE DEPARTMENTS (90%); ETHICS (90%); MILITARY OPERATIONS (90%); REPORTS, REVIEWS & SECTIONS (90%); DEFENSE RESEARCH (78%); ELECTRONIC GOVERNMENT (78%); RESEARCH INSTITUTES (78%); RESEARCH REPORTS (78%); ARMED FORCES (77%); RISK MANAGEMENT (77%); SAFETY (77%); HUMAN FACTORS ENGINEERING (75%); AIR FORCES (73%); EMERGING TECHNOLOGY (73%); GOVERNMENT & PUBLIC ADMINISTRATION (73%); ASSOCIATIONS & ORGANIZATIONS (69%); COLLEGE & UNIVERSITY PROFESSORS (67%); LAW & LEGAL SYSTEM (64%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); DEFENSE DEPARTMENTS (90%); DEFENSE ELECTRONICS (90%); MILITARY OPERATIONS (90%); DEFENSE INDUSTRY (78%); DEFENSE RESEARCH (78%); INFORMATION TECHNOLOGY INDUSTRY (78%); ARMED FORCES (77%); RISK MANAGEMENT (77%); HUMAN FACTORS ENGINEERING (75%); AIR FORCES (73%); COLLEGE & UNIVERSITY PROFESSORS (67%)
Geographic: CANBERRA, AUSTRALIA (79%); AUSTRALIAN CAPITAL TERRITORY (79%); NEW SOUTH WALES, AUSTRALIA (79%); AUSTRALIA (92%)
Load-Date: February 18, 2021","ABSTRACT
The report says that Defence's challenge is that failure to adopt the emerging technologies in a timely manner may result in a military disadvantage, while premature adoption without sufficient research and analysis may result in inadvertent harms.
FULL TEXT
The Department of Defence has launched a technical report aiming to assist in the development of ethical artificial intelligence systems for Defence. 
The report, published on Tuesday, noted that while there is potential for AI technology to increase Defence capability and reduce risk in military operations, significant steps must be undertaken to ensure that introduction of the technology does not result in ""adverse outcomes"". 
""Defence's challenge is that failure to adopt the emerging technologies in a timely manner may result in a military disadvantage, while premature adoption without sufficient research and analysis may result in inadvertent harms,"" it said. 
To address this, Plan Jericho, the Defence Science and Technology Group, and the Trusted Autonomous Defence Cooperative Research Centre ran a workshop in Canberra in 2019, with the aim of developing a pragmatic ethical methodology for AI projects in Defence. 
Attended by more than 100 representatives from Defence, other government agencies, industry, academia, international organisations and media, the discussions and theories which came out of the workshop have been outlined in the new report. 
Read more: Defence experts discuss ethical AI, while NSW wants to ensure AI in government places customers at the centre
Attendees identified five facets of ethical AI in Defence, including: 
Responsibility -- who is responsible for AI?   
Governance -- how is AI controlled?   
Trust -- how can AI be trusted?   
Law -- how can AI be used lawfully?   
Traceability -- how are the actions of AI recorded?
 They also put forward 20 topics to be explored when considering AI, such as transparency, safety, accountability, human factors, supply chain, and misuse and risks.  Chief defence scientist Professor Tanya Monro said AI can offer benefits such as removing humans from high-threat environments and improving Australian advantage by providing more in-depth and faster situational awareness, but there are risks.  ""Upfront engagement on AI technologies, and consideration of ethical aspects needs to occur in parallel with technology development,"" she said in a statement on Tuesday.  The paper outlined three tools to assist Defence and Industry in developing AI systems for Defence, including: an AI Checklist for the development of ethical AI systems; an Ethical AI Risk Matrix to describe identified risks and proposed treatment; and, for larger programs, a data item descriptor for contractors to develop a formal Legal, Ethical and Assurance Program Plan to be included in project documentation for AI programs where an ethical risk assessment is above a certain threshold.    Read more: Former digital government boss criticises new ethical AI guidelines    Air Vice-Marshal Cath Roberts, head of air force capability said AI and human-machine teaming would play a ""pivotal"" role for air and space power in the future.  Ethical and legal issues must be resolved at the same pace that the technology is developed, she said.  ""This paper is useful in suggesting consideration of ethical issues that may arise to ensure responsibility for AI systems within traceable systems of control,"" she said.  ""Practical application of these tools into projects such as the Loyal Wingman will assist Defence to explore autonomy, AI, and teaming concepts in an iterative, learning and collaborative way.""  The release of the findings has come a week after the CSIRO released a study showing that AI can be used to influence human decision-making by exploiting vulnerabilities in an individual's habits and patterns.  CSIRO Data61 director Dr Jon Whittle said the study was further proof that AI technologies have ""tremendous potential"" to provide societal benefit, but also ethical risks.  ""Like any technology, AI could be used for good or bad, so proper governance is critical to ensure that AI and machine learning are implemented in a responsible manner. Organisations need to ensure they are educated on what these technologies can and cannot do and be aware of potential risks as well as rewards,"" he said.    Read more: AI can now learn to manipulate human behaviour     
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ARTIFICIAL INTELLIGENCE ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); DEFENSE DEPARTMENTS (90%); ETHICS (90%); MILITARY OPERATIONS (90%); REPORTS, REVIEWS & SECTIONS (90%); DEFENSE RESEARCH (78%); ELECTRONIC GOVERNMENT (78%); RESEARCH INSTITUTES (78%); RESEARCH REPORTS (78%); ARMED FORCES (77%); RISK MANAGEMENT (77%); SAFETY (77%); HUMAN FACTORS ENGINEERING (75%); AIR FORCES (73%); EMERGING TECHNOLOGY (73%); GOVERNMENT & PUBLIC ADMINISTRATION (73%); ASSOCIATIONS & ORGANIZATIONS (69%); COLLEGE & UNIVERSITY PROFESSORS (67%); LAW & LEGAL SYSTEM (64%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (93%); ARTIFICIAL INTELLIGENCE (90%); DEFENSE DEPARTMENTS (90%); DEFENSE ELECTRONICS (90%); MILITARY OPERATIONS (90%); DEFENSE INDUSTRY (78%); DEFENSE RESEARCH (78%); INFORMATION TECHNOLOGY INDUSTRY (78%); ARMED FORCES (77%); RISK MANAGEMENT (77%); HUMAN FACTORS ENGINEERING (75%); AIR FORCES (73%); COLLEGE & UNIVERSITY PROFESSORS (67%)
Geographic: CANBERRA, AUSTRALIA (79%); AUSTRALIAN CAPITAL TERRITORY (79%); NEW SOUTH WALES, AUSTRALIA (79%); AUSTRALIA (92%)
Load-Date: February 18, 2021",neutral,0.6385864019393921,balanced/neutral,"['transparency', 'accountability', 'safety', 'autonomy']",['autonomy'],"['governance', 'guidelines', 'law', 'must', 'need to']",['machine learning'],4,1,5,1
2021,Unknown Title,"Byline: States News Service
Dateline: DENVER, CO 
Body
The following information was released by the Daniels Fund:
The Daniels Fund Ethics Initiative's ninth annual Collegiate Program Case Competition took place April 16. The Case Competition exposes college students to a thought-provoking business ethics case, and is designed to challenge students' ethical reasoning, give them tools for ethical decision-making, and raise awareness of the importance of principle-based ethics.
The event included two separate competition tracks one for undergraduate students and another for graduate students. For the first time ever, the competition was held in a live virtual format, due to COVID-19 considerations, with students participating from across Colorado, New Mexico, Utah, and Wyoming.
In the undergraduate track, students from University of Northern Colorado's Monfort College of Business were the first place winners. The second place team was from University of Utah's David Eccles School of Business and the third place team was from the University of Wyoming's College of Business. Students from Colorado State University's College of Business placed first in the graduate competition. University of Utah's David Eccles School of Business placed second and University of Colorado Denver's Business School placed third in the graduate track.
""This was an amazing opportunity, I learned so much,"" said Racheal Guse from the CSU graduate team. ""It was the highlight of my MBA experience.""
In advance of the competition, teams were provided with a business ethics case involving a fictional defense industry technology company, specializing in artificial intelligence and biological experiments. Students take on the role of an ethics consulting firm hired by the company's board of directors to help deal with ethical issues and risks at the company.
Each team analyzed the ethical issues involved and presented their prepared recommendations to a panel of judges. After their presentation, the teams received new information a twist on the case that introduced a crisis. They had just four hours to re-analyze their original recommendations, and make a second presentation to the judges incorporating the new information.
""This competition is designed to help students prepare for the real world by learning how to incorporate ethical decision-making into situations similar to what they might face in their professional careers,"" said Hanna Skandera, President and CEO of the Daniels Fund. ""It was impressive to see how well the teams performed and were able to pivot when they received the crisis part of the case.""
Panels of judges for each track were comprised of business and community leaders who played the role of the company's board of directors. Jandel Allen-Davis, CEO and President of Craig Hospital, served as head judge for the graduate track and Mark Cordova, Founder and President of Centennial Bolt, served as head judge for the undergraduate track. The panels of judges included Albus Brooks, Vice President of Business Development and Public Affairs at Milender White; Ted Harms, Executive Director of the Anschutz Foundation; Richard Martinez, President and CEO of Young Americans Center for Financial Education and Young Americans Bank; Christopher Picardi, Senior Vice President of KeyBank; Mary Rhinehart, Chairman of Johns Mansville; Mike Talamantes, Managing Director at RBC Capital Markets; and Robin Wise, President and CEO of Junior Achievement Rocky Mountain.
Judges evaluated presentations against established criteria, including the Daniels Fund Ethics Initiative Principles: integrity, trust, accountability, transparency, fairness, respect, rule of law, and viability.
Eleven undergraduate and eleven graduate student teams participated in this unique competition, designed exclusively for business schools that are part of the Daniels Fund Ethics Initiative Collegiate Program from Colorado, New Mexico, Utah, and Wyoming. In addition to the top teams already mentioned, there were competing teams from: Colorado Mesa University's Department of Business, New Mexico State University's College of Business, University of Colorado Boulder's Leeds School of Business, University of Colorado Colorado Springs' College of Business, University of Denver's Daniels College of Business, and University of New Mexico's Anderson School of Management. Students from the University of Colorado Law School served as legal advisors to each of the teams.
About the Daniels Fund Ethics Initiative Collegiate Program
Reflecting Bill Daniels' personal commitment to ethics and integrity, the Daniels Fund Ethics Initiative Collegiate Program was established in 2010 to instill a high standard of ethics in students and strengthen principle-based ethics education in participating schools. The current phase of the program consists of twelve business and law school university partners in Colorado, New Mexico, Utah, and Wyoming. To date, more than 450,000 students, faculty, and business people have been impacted by the Collegiate Program.
About the Daniels Fund
The Daniels Fund, established by cable television pioneer Bill Daniels, is a private charitable foundation dedicated to making life better for the people of Colorado, New Mexico, Utah, and Wyoming through its grants program, scholarship program, and ethics initiative. Visit DanielsFund.org to learn more.
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: STUDENTS & STUDENT LIFE (95%); ETHICS (92%); BUSINESS EDUCATION (90%); COLLEGES & UNIVERSITIES (90%); GRADUATE & PROFESSIONAL SCHOOLS (90%); BUSINESS ETHICS (89%); BUSINESS ETHICS & CORPORATE CITIZENSHIP (89%); EXECUTIVES (88%); COMPANY ACTIVITIES & MANAGEMENT (77%); COVID CORONAVIRUS (73%); ARTIFICIAL INTELLIGENCE (72%); BUSINESS NEWS (72%); INFECTIOUS DISEASE (70%); BOARDS OF DIRECTORS (69%); COVID-19 CORONAVIRUS (55%)
Organization: COLORADO STATE UNIVERSITY (91%); UNIVERSITY OF UTAH (83%); UNIVERSITY OF WYOMING (56%); UNIVERSITY OF COLORADO (55%)
Industry: COLLEGES & UNIVERSITIES (90%); GRADUATE & PROFESSIONAL SCHOOLS (90%); INFORMATION TECHNOLOGY INDUSTRY (77%); ARTIFICIAL INTELLIGENCE (72%); DEFENSE & AEROSPACE (72%); CONSULTING SERVICES (63%); DEFENSE INDUSTRY (50%)
Person: ROBERT G DAVIS (50%)
Geographic: DENVER, CO, USA (90%); COLORADO, USA (96%); UTAH, USA (93%); WYOMING, USA (79%)
Load-Date: April 25, 2021","The following information was released by the Daniels Fund:
The Daniels Fund Ethics Initiative's ninth annual Collegiate Program Case Competition took place April 16. The Case Competition exposes college students to a thought-provoking business ethics case, and is designed to challenge students' ethical reasoning, give them tools for ethical decision-making, and raise awareness of the importance of principle-based ethics.
The event included two separate competition tracks one for undergraduate students and another for graduate students. For the first time ever, the competition was held in a live virtual format, due to COVID-19 considerations, with students participating from across Colorado, New Mexico, Utah, and Wyoming.
In the undergraduate track, students from University of Northern Colorado's Monfort College of Business were the first place winners. The second place team was from University of Utah's David Eccles School of Business and the third place team was from the University of Wyoming's College of Business. Students from Colorado State University's College of Business placed first in the graduate competition. University of Utah's David Eccles School of Business placed second and University of Colorado Denver's Business School placed third in the graduate track.
""This was an amazing opportunity, I learned so much,"" said Racheal Guse from the CSU graduate team. ""It was the highlight of my MBA experience.""
In advance of the competition, teams were provided with a business ethics case involving a fictional defense industry technology company, specializing in artificial intelligence and biological experiments. Students take on the role of an ethics consulting firm hired by the company's board of directors to help deal with ethical issues and risks at the company.
Each team analyzed the ethical issues involved and presented their prepared recommendations to a panel of judges. After their presentation, the teams received new information a twist on the case that introduced a crisis. They had just four hours to re-analyze their original recommendations, and make a second presentation to the judges incorporating the new information.
""This competition is designed to help students prepare for the real world by learning how to incorporate ethical decision-making into situations similar to what they might face in their professional careers,"" said Hanna Skandera, President and CEO of the Daniels Fund. ""It was impressive to see how well the teams performed and were able to pivot when they received the crisis part of the case.""
Panels of judges for each track were comprised of business and community leaders who played the role of the company's board of directors. Jandel Allen-Davis, CEO and President of Craig Hospital, served as head judge for the graduate track and Mark Cordova, Founder and President of Centennial Bolt, served as head judge for the undergraduate track. The panels of judges included Albus Brooks, Vice President of Business Development and Public Affairs at Milender White; Ted Harms, Executive Director of the Anschutz Foundation; Richard Martinez, President and CEO of Young Americans Center for Financial Education and Young Americans Bank; Christopher Picardi, Senior Vice President of KeyBank; Mary Rhinehart, Chairman of Johns Mansville; Mike Talamantes, Managing Director at RBC Capital Markets; and Robin Wise, President and CEO of Junior Achievement Rocky Mountain.
Judges evaluated presentations against established criteria, including the Daniels Fund Ethics Initiative Principles: integrity, trust, accountability, transparency, fairness, respect, rule of law, and viability.
Eleven undergraduate and eleven graduate student teams participated in this unique competition, designed exclusively for business schools that are part of the Daniels Fund Ethics Initiative Collegiate Program from Colorado, New Mexico, Utah, and Wyoming. In addition to the top teams already mentioned, there were competing teams from: Colorado Mesa University's Department of Business, New Mexico State University's College of Business, University of Colorado Boulder's Leeds School of Business, University of Colorado Colorado Springs' College of Business, University of Denver's Daniels College of Business, and University of New Mexico's Anderson School of Management. Students from the University of Colorado Law School served as legal advisors to each of the teams.
About the Daniels Fund Ethics Initiative Collegiate Program
Reflecting Bill Daniels' personal commitment to ethics and integrity, the Daniels Fund Ethics Initiative Collegiate Program was established in 2010 to instill a high standard of ethics in students and strengthen principle-based ethics education in participating schools. The current phase of the program consists of twelve business and law school university partners in Colorado, New Mexico, Utah, and Wyoming. To date, more than 450,000 students, faculty, and business people have been impacted by the Collegiate Program.
About the Daniels Fund
The Daniels Fund, established by cable television pioneer Bill Daniels, is a private charitable foundation dedicated to making life better for the people of Colorado, New Mexico, Utah, and Wyoming through its grants program, scholarship program, and ethics initiative. Visit DanielsFund.org to learn more.
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: STUDENTS & STUDENT LIFE (95%); ETHICS (92%); BUSINESS EDUCATION (90%); COLLEGES & UNIVERSITIES (90%); GRADUATE & PROFESSIONAL SCHOOLS (90%); BUSINESS ETHICS (89%); BUSINESS ETHICS & CORPORATE CITIZENSHIP (89%); EXECUTIVES (88%); COMPANY ACTIVITIES & MANAGEMENT (77%); COVID CORONAVIRUS (73%); ARTIFICIAL INTELLIGENCE (72%); BUSINESS NEWS (72%); INFECTIOUS DISEASE (70%); BOARDS OF DIRECTORS (69%); COVID-19 CORONAVIRUS (55%)
Organization: COLORADO STATE UNIVERSITY (91%); UNIVERSITY OF UTAH (83%); UNIVERSITY OF WYOMING (56%); UNIVERSITY OF COLORADO (55%)
Industry: COLLEGES & UNIVERSITIES (90%); GRADUATE & PROFESSIONAL SCHOOLS (90%); INFORMATION TECHNOLOGY INDUSTRY (77%); ARTIFICIAL INTELLIGENCE (72%); DEFENSE & AEROSPACE (72%); CONSULTING SERVICES (63%); DEFENSE INDUSTRY (50%)
Person: ROBERT G DAVIS (50%)
Geographic: DENVER, CO, USA (90%); COLORADO, USA (96%); UTAH, USA (93%); WYOMING, USA (79%)
Load-Date: April 25, 2021",neutral,0.7531899809837341,balanced/neutral,"['fairness', 'transparency', 'accountability']",['fairness'],['law'],[],3,1,1,0
2021,Unknown Title,"Body
Link to Story
LOS ANGELES-Sunday 12 December 2021 [ AETOS Wire ]
(BUSINESS WIRE )-- Scientific discoveries deepen our understanding of nature and ourselves, with the potential to transform our everyday lives, yet can raise ethical concerns or risks for society.
Cutting-edge neuroscience, genetics, and artificial intelligence are a few examples that are driving the need to discuss: Who bears responsibility for broad ethical considerations of scientific discoveries? When is it optimal to consider implications and risks? How can the public be empowered to participate in these discussions?
Two Kavli Centers for Ethics, Science, and the Public - at the University of California, Berkeley, and the University of Cambridge - are launching to engage the public in identifying and exploring ethical considerations and impacts born from scientific discovery.
The Kavli Foundation's vision for the centers is a paradigm shift to meet an as-yet unmet need within science: a proactive and sustained effort that is intentional in connecting the public, scientists, ethicists, social scientists, and science communicators early in the process of scientific discoveries to identify and discuss potential impacts on society.
""We're embarking on a democratization of the way we think, collaborate, and communicate about scientific discoveries and their ethical aspects - and ensuring the public is included,"" said The Kavli Foundation President Cynthia Friend .""It's long past due for this to happen.""
Until now, there hasn't been a sustained and proactive venture to address ethical implications born from scientific discovery that involves the public early and intentionally in the scientific process. And while there is increasing recognition within the scientific community that the public should be involved, mechanisms and infrastructure to do this are lacking. The public is too often left out of these important discussions, or they are brought in too late.
""With the Kavli Centers for Ethics, Science, and the Public, we are taking necessary action to create the infrastructure that enables early and intentional public engagement in the ethical considerations born from scientific discoveries,"" remarked The Kavli Foundation Director of Public Engagement Brooke Smith .
Two centers were selected for this new venture based on their vision, approach, and experience. While both are multi-faceted and complementary in their approaches working across disciplines in the sciences and humanities, each will have an initial focus that is unique.
The Kavli Center for Ethics, Science, and the Public at UC Berkeley will reimagine how scientists are trained, beginning in the fields of neuroscience, genetics, and artificial intelligence. Leading the center is AI expert Stuart Russell , along with Nobel-Prize Laureate Saul Perlmutter , who provided some of the first evidence that the expansion of the universe is accelerating; Nobel and Kavli Prize Laureate Jennifer Doudna , known for her discovery of the gene-editing tool CRISPR; theoretical and moral philosopher Jay Wallace ; bioethicist Jodi Halpern ; neuroscientist Jack Gallant ; and historian and writer Elena Conis .
""The impetus from The Kavli Foundation has helped to mobilize Berkeley's unparalleled resources in the humanities, social sciences, natural sciences, and engineering to collaborate on addressing one of humanity's most pressing problems: how to ensure that our rapidly advancing scientific and technological capabilities are directed towards the interests of humanity,"" said Stuart Russell, who serves as the inaugural Director of the Kavli Center for Ethics, Science, and the Public at UC Berkeley.
In a unique collaboration with Wellcome Connecting Science, the Kavli Center for Ethics, Science, and the Public at the University of Cambridge will be led by internationally recognized social scientist and genetic counsellor Anna Middleton ; supported by sociologist and bioethicist Richard Milne ; and journalist and broadcaster Catherine Galloway ; with creative industry expertise from broadcaster Vivienne Parry , OBE; sociology of education expertise from Susan Robertson ; and genomics and public engagement expertise from Julian Rayner . Drawing on a network of experts in ethics and public engagement from the UK, China, Russia, India, and Japan, the new center will explore how ethical implications raised by science are tackled in different cultural contexts within the domains of genomics, big data, health research, and emerging technologies.
""From the discovery of DNA's structure to sequencing 20% of the world's COVID virus and the development of the first artificial intelligence, Cambridge has been at the cutting edge of science for centuries,"" remarked Anna Middleton, director for the Kavli Center for Ethics, Science, and the Public at the University of Cambridge.""Through collaboration with experts in popular culture we will find the evidence base to communicate complex ideas around the ethical issues raised by science so that all of us can share in decision making around the implications of science for society.""
The idea for the Centers was sparked by The Kavli Foundation's work and observations in science and society, including research at the 20 Kavli Institutes globally, where inspiring and transformative science is being done—ranging from decoding brain activity to fabricating artificial cells.
""This is a long-overdue beginning of an important journey for the scientific community, and we look forward to the impact the Kavli Centers for Ethics, Science, and the Public will have on the future role of science within society,"" said Friend.
The Kavli Foundation is dedicated to advancing science for the benefit of humanity. The foundation's mission is implemented through Kavli research institutes globally and programs that support basic science in the fields of astrophysics, nanoscience, neuroscience, and theoretical physics; initiatives that strengthen the relationship between science and society; and prizes and awards including the international Kavli Prizes and the AAAS Kavli Science Journalism Awards. Learn more at kavlifoundation.org and follow @kavlifoundation.
This press release features multimedia. View the full release here: 
MENAFN12122021004146001356ID1103345278
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (95%); SCIENCE & TECHNOLOGY (95%); HUMANITIES & SOCIAL SCIENCE (89%); NEUROSCIENCE (89%); AWARDS & PRIZES (86%); NOBEL PRIZES (78%); ARTIFICIAL INTELLIGENCE (77%); BIOETHICS (77%); GENE EDITING (77%); GENES & CHROMOSOMES (77%); GENETIC ENGINEERING (77%); WRITERS (73%); HISTORY (65%)
Organization: UNIVERSITY OF CALIFORNIA BERKELEY (57%)
Industry: ARTIFICIAL INTELLIGENCE (77%); WRITERS (73%)
Geographic: SAN FRANCISCO BAY AREA, CA, USA (90%); LOS ANGELES, CA, USA (78%); CALIFORNIA, USA (58%)
Load-Date: August 29, 2022","Link to Story
LOS ANGELES-Sunday 12 December 2021 [ AETOS Wire ]
(BUSINESS WIRE )-- Scientific discoveries deepen our understanding of nature and ourselves, with the potential to transform our everyday lives, yet can raise ethical concerns or risks for society.
Cutting-edge neuroscience, genetics, and artificial intelligence are a few examples that are driving the need to discuss: Who bears responsibility for broad ethical considerations of scientific discoveries? When is it optimal to consider implications and risks? How can the public be empowered to participate in these discussions?
Two Kavli Centers for Ethics, Science, and the Public - at the University of California, Berkeley, and the University of Cambridge - are launching to engage the public in identifying and exploring ethical considerations and impacts born from scientific discovery.
The Kavli Foundation's vision for the centers is a paradigm shift to meet an as-yet unmet need within science: a proactive and sustained effort that is intentional in connecting the public, scientists, ethicists, social scientists, and science communicators early in the process of scientific discoveries to identify and discuss potential impacts on society.
""We're embarking on a democratization of the way we think, collaborate, and communicate about scientific discoveries and their ethical aspects - and ensuring the public is included,"" said The Kavli Foundation President Cynthia Friend .""It's long past due for this to happen.""
Until now, there hasn't been a sustained and proactive venture to address ethical implications born from scientific discovery that involves the public early and intentionally in the scientific process. And while there is increasing recognition within the scientific community that the public should be involved, mechanisms and infrastructure to do this are lacking. The public is too often left out of these important discussions, or they are brought in too late.
""With the Kavli Centers for Ethics, Science, and the Public, we are taking necessary action to create the infrastructure that enables early and intentional public engagement in the ethical considerations born from scientific discoveries,"" remarked The Kavli Foundation Director of Public Engagement Brooke Smith .
Two centers were selected for this new venture based on their vision, approach, and experience. While both are multi-faceted and complementary in their approaches working across disciplines in the sciences and humanities, each will have an initial focus that is unique.
The Kavli Center for Ethics, Science, and the Public at UC Berkeley will reimagine how scientists are trained, beginning in the fields of neuroscience, genetics, and artificial intelligence. Leading the center is AI expert Stuart Russell , along with Nobel-Prize Laureate Saul Perlmutter , who provided some of the first evidence that the expansion of the universe is accelerating; Nobel and Kavli Prize Laureate Jennifer Doudna , known for her discovery of the gene-editing tool CRISPR; theoretical and moral philosopher Jay Wallace ; bioethicist Jodi Halpern ; neuroscientist Jack Gallant ; and historian and writer Elena Conis .
""The impetus from The Kavli Foundation has helped to mobilize Berkeley's unparalleled resources in the humanities, social sciences, natural sciences, and engineering to collaborate on addressing one of humanity's most pressing problems: how to ensure that our rapidly advancing scientific and technological capabilities are directed towards the interests of humanity,"" said Stuart Russell, who serves as the inaugural Director of the Kavli Center for Ethics, Science, and the Public at UC Berkeley.
In a unique collaboration with Wellcome Connecting Science, the Kavli Center for Ethics, Science, and the Public at the University of Cambridge will be led by internationally recognized social scientist and genetic counsellor Anna Middleton ; supported by sociologist and bioethicist Richard Milne ; and journalist and broadcaster Catherine Galloway ; with creative industry expertise from broadcaster Vivienne Parry , OBE; sociology of education expertise from Susan Robertson ; and genomics and public engagement expertise from Julian Rayner . Drawing on a network of experts in ethics and public engagement from the UK, China, Russia, India, and Japan, the new center will explore how ethical implications raised by science are tackled in different cultural contexts within the domains of genomics, big data, health research, and emerging technologies.
""From the discovery of DNA's structure to sequencing 20% of the world's COVID virus and the development of the first artificial intelligence, Cambridge has been at the cutting edge of science for centuries,"" remarked Anna Middleton, director for the Kavli Center for Ethics, Science, and the Public at the University of Cambridge.""Through collaboration with experts in popular culture we will find the evidence base to communicate complex ideas around the ethical issues raised by science so that all of us can share in decision making around the implications of science for society.""
The idea for the Centers was sparked by The Kavli Foundation's work and observations in science and society, including research at the 20 Kavli Institutes globally, where inspiring and transformative science is being done—ranging from decoding brain activity to fabricating artificial cells.
""This is a long-overdue beginning of an important journey for the scientific community, and we look forward to the impact the Kavli Centers for Ethics, Science, and the Public will have on the future role of science within society,"" said Friend.
The Kavli Foundation is dedicated to advancing science for the benefit of humanity. The foundation's mission is implemented through Kavli research institutes globally and programs that support basic science in the fields of astrophysics, nanoscience, neuroscience, and theoretical physics; initiatives that strengthen the relationship between science and society; and prizes and awards including the international Kavli Prizes and the AAAS Kavli Science Journalism Awards. Learn more at kavlifoundation.org and follow @kavlifoundation.
This press release features multimedia. View the full release here: 
MENAFN12122021004146001356ID1103345278
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (95%); SCIENCE & TECHNOLOGY (95%); HUMANITIES & SOCIAL SCIENCE (89%); NEUROSCIENCE (89%); AWARDS & PRIZES (86%); NOBEL PRIZES (78%); ARTIFICIAL INTELLIGENCE (77%); BIOETHICS (77%); GENE EDITING (77%); GENES & CHROMOSOMES (77%); GENETIC ENGINEERING (77%); WRITERS (73%); HISTORY (65%)
Organization: UNIVERSITY OF CALIFORNIA BERKELEY (57%)
Industry: ARTIFICIAL INTELLIGENCE (77%); WRITERS (73%)
Geographic: SAN FRANCISCO BAY AREA, CA, USA (90%); LOS ANGELES, CA, USA (78%); CALIFORNIA, USA (58%)
Load-Date: August 29, 2022",neutral,0.8817176818847656,balanced/neutral,[],[],"['should', 'need to']",[],0,0,2,0
2021,Unknown Title,"Body
Artificial Intelligence (AI) systems are becoming more and more ubiquitous in the financial industry. The increase in development and deployment is not without good reason:
AI can be of use for all aspects of financial services from fraud detection to credit approval. However, despite its potential to revolutionize the industry it has been pointed out that the use of AI is not without risks.
The functioning and outcomes of AI systems can become opaque, turning AI systems into black-boxes, with virtually no possibilities to audit or control them. Furthermore, historical bias in datasets or inadequately calibrated models might systematically disadvantage subgroups in society thereby perpetuating or even reinforcing existing disparities. To address these risks various ethical frameworks and guidelines for the responsible use of AI have been proposed by the industry, governments and international regulatory bodies. One of the most prominent among these are the EU Guidelines for Trustworthy AI.
Since their publication in 2019, however, limited research has been done on how these guidelines could formalize into regulations and how they relate to the current regulations for finance in the Netherlands. To explore these questions the Ethical AI for Inclusive Financial Services research project examined how the existing regulations in finance could contribute to the desired 'human centered AI' and to what extent new regulation might be necessary to achieve this goal. With De Volksbank as project and valorization partner research was done on how the ethical guidelines relate to the existing duty of care and explainability obligations banks currently hold when it comes to credit approval. The results indicate that some ethical aspects might be covered within the current regulations but work still needs to be done to ensure a 'fit' between the ethical principles endorsed by the European Union and the legal frameworks currently in place.
The report offers a first starting point to begin exploring the questions of ethical and legal AI applications in finance. It by no means intends to provide in an exhaustive analysis of these topics but, as an exploration, already gives a strong recommendation to financial industries that it would be in their best interest to start incorporating the ethical principles of AI in their AI development processes as with this new powers come new responsibilities.
[Editorial queries for this story should be sent to newswire@enpublishing.co.uk ]   
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: ENPN
Subject: ARTIFICIAL INTELLIGENCE ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); ETHICS (90%); BANKING & FINANCE SECTOR PERFORMANCE (89%); RESEARCH & DEVELOPMENT (78%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (78%); AGENCY RULEMAKING (74%); INTERNATIONAL ECONOMIC ORGANIZATIONS (74%); PRESS RELEASES (74%); EUROPEAN UNION (73%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (91%); BANKING & FINANCE (91%); ARTIFICIAL INTELLIGENCE (90%); BANKING & FINANCE REGULATION & POLICY (90%); FINANCIAL INCLUSION (90%); BANKING & FINANCE SECTOR PERFORMANCE (89%)
Geographic: ROTTERDAM, NETHERLANDS (78%); NETHERLANDS (90%); EUROPE (73%); EUROPEAN UNION MEMBER STATES (70%)
Load-Date: January 28, 2021","Artificial Intelligence (AI) systems are becoming more and more ubiquitous in the financial industry. The increase in development and deployment is not without good reason:
AI can be of use for all aspects of financial services from fraud detection to credit approval. However, despite its potential to revolutionize the industry it has been pointed out that the use of AI is not without risks.
The functioning and outcomes of AI systems can become opaque, turning AI systems into black-boxes, with virtually no possibilities to audit or control them. Furthermore, historical bias in datasets or inadequately calibrated models might systematically disadvantage subgroups in society thereby perpetuating or even reinforcing existing disparities. To address these risks various ethical frameworks and guidelines for the responsible use of AI have been proposed by the industry, governments and international regulatory bodies. One of the most prominent among these are the EU Guidelines for Trustworthy AI.
Since their publication in 2019, however, limited research has been done on how these guidelines could formalize into regulations and how they relate to the current regulations for finance in the Netherlands. To explore these questions the Ethical AI for Inclusive Financial Services research project examined how the existing regulations in finance could contribute to the desired 'human centered AI' and to what extent new regulation might be necessary to achieve this goal. With De Volksbank as project and valorization partner research was done on how the ethical guidelines relate to the existing duty of care and explainability obligations banks currently hold when it comes to credit approval. The results indicate that some ethical aspects might be covered within the current regulations but work still needs to be done to ensure a 'fit' between the ethical principles endorsed by the European Union and the legal frameworks currently in place.
The report offers a first starting point to begin exploring the questions of ethical and legal AI applications in finance. It by no means intends to provide in an exhaustive analysis of these topics but, as an exploration, already gives a strong recommendation to financial industries that it would be in their best interest to start incorporating the ethical principles of AI in their AI development processes as with this new powers come new responsibilities.
[Editorial queries for this story should be sent to newswire@enpublishing.co.uk ]   
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: ENPN
Subject: ARTIFICIAL INTELLIGENCE ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); ETHICS (90%); BANKING & FINANCE SECTOR PERFORMANCE (89%); RESEARCH & DEVELOPMENT (78%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (78%); AGENCY RULEMAKING (74%); INTERNATIONAL ECONOMIC ORGANIZATIONS (74%); PRESS RELEASES (74%); EUROPEAN UNION (73%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (91%); BANKING & FINANCE (91%); ARTIFICIAL INTELLIGENCE (90%); BANKING & FINANCE REGULATION & POLICY (90%); FINANCIAL INCLUSION (90%); BANKING & FINANCE SECTOR PERFORMANCE (89%)
Geographic: ROTTERDAM, NETHERLANDS (78%); NETHERLANDS (90%); EUROPE (73%); EUROPEAN UNION MEMBER STATES (70%)
Load-Date: January 28, 2021",neutral,0.5475051403045654,balanced/neutral,"['bias', 'explainability', 'agency']",[],"['regulation', 'policy', 'guidelines', 'audit', 'should']",[],3,0,5,0
2021,Unknown Title,"Byline: Sohini Bagchi
Body
While more companies are realizing the value of artificial intelligence (AI) and are levering the technology to improve efficiency and bring down costs, ethical concerns continue to rise as AI takes bigger decision-making role in more industries. According to the O'Reilly 2021 AI Adoption in the Enterprise report, less than half of companies surveyed said they haven't thought through the consequences AI products. Only organizations with matured AI practice bother to check the fairness, bias and ethics of their AI platforms. In that sense one can say that AI ethics is directly proportional to AI maturity of an organization.
The problem however is that companies often overestimate their level of maturity when it comes to responsible AI implementation. As another survey conducted by BCG GAMMA finds that 55% of all respondents overestimated the maturity of their AI program. While 26% of companies say they've hit scale in their AI deployment, only 12% include a responsible AI program as part of their work.
This clearly shows that organizations are overly optimistic about the maturity of their AI implementation. As Steven Mills, BCG GAMMA's chief ethics officer and coauthor mentions, ""While many organizations are making progress, it's clear the depth and breadth of most efforts fall behind what is needed to truly ensure responsible AI - creating a major ethical dilemma in AI.""
The O'Reilly report also notes that AI implementation won't hit maturity until ethics, safety, privacy, and security are primary rather than secondary concerns.
To support AI maturity, teams can benefit from reviewing case studies in how other organizations have managed AI implementation, believes Rachel Roumeliotis, VP of Content Strategy, O'Reilly Media.
For some companies, the ethical dimension of AI implementation hasn't been fully thought through because they have yet to deploy at scale and reach full maturity, explains Roumeliotis. He adds that bias can seep into AI products at multiple points in the creation process from the data fueling decisions to the algorithmic training and final review stage.
Viral Thakker, Partner, Deloitte India, says, ""AI ethics deal with managing ethical complexities in the age of vast amounts of data and extensive use of automation. The big issues driving this are privacy considerations, lack of transparency of ""black box"" AI models, bias and discrimination that may be embedded in the data, which AI algorithms learn from, as well as lack of governance and accountability.
Worldwide business spending on AI is expected to hit $50 billion this year and $110 billion annually by 2024, even after the global economic slump caused by the COVID-19 pandemic, according to a forecast by research firm IDC. Retail, healthcare and banking industries are slated to spend the maximum, said the analyst firm.
For all the good that AI can bring, policy makers and responsible tech companies must recognize, execute, and mitigate its potential unintended, harmful effects.
Praveen Kumar, Vice President, Digital & Innovations, JK Technosoft states, ""Organizations should start thinking of educating and upskilling employees as AI will incorporate a great deal of work alongside human workforces. At the same time, it is of utmost importance to consider the necessary and adequate governance mechanisms carefully and proactively for ensuring ethical considerations in the deployment of responsible AI tools.""
Already several organizations are offering programs to help employees put ethics at the core of their respective workflows. These programs are designed to empower the entire organization to think critically about every step of the process of building AI solutions, and perhaps most importantly continue advancing AI that is safe and inclusive.
A big sign in AI maturity is to grow the company's knowledge on the subject, and nurture an organization-wide 'ethical-first' approach, very similar to the 'security-first' philosophy.
As we move forward, our reliability on AI will deepen which will inevitably cause many ethical issues, especially in industries where personal and business data is at stake Companies should therefore start thinking about how they will retrain and educate employees with regard to AI's ethical implication.
Shreeranganath Kulkarni, Chief Delivery Officer, Birlasoft suggests that for enterprise AI to reach maturity, they should work towards an AI playbook built on the pillars of AI strategy, data, talent, technology, execution, and culture.
Organizations that are able to adapt and upgrade themselves and emerge stronger will be the ones to benefit from any kind of disruptions. In the process, they must consider carefully and proactively the necessary governance mechanisms to be used to ensure ethical considerations in the deployment of AI tools. Building trust in AI solutions and tools is vital so that businesses and individuals can benefit from its use.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ARTIFICIAL INTELLIGENCE ETHICS (95%); ETHICS (95%); ARTIFICIAL INTELLIGENCE (90%); BUSINESS ETHICS (78%); BUSINESS NEWS (78%); CORPORATE GOVERNANCE (78%); POLLS & SURVEYS (77%); DISCRIMINATION (75%); EXECUTIVES (71%); PUBLIC HEALTH (70%); SAFETY (69%); EPIDEMICS (65%); INFECTIOUS DISEASE (65%); CASE STUDIES (63%); COVID CORONAVIRUS (63%); COVID-19 CORONAVIRUS (60%); ECONOMY & ECONOMIC INDICATORS (60%)
Company:  DELOITTE LLP (52%)
Industry: NAICS541211 OFFICES OF CERTIFIED PUBLIC ACCOUNTANTS (52%); SIC8721 ACCOUNTING, AUDITING, & BOOKKEEPING SERVICES (52%); ARTIFICIAL INTELLIGENCE ETHICS (95%); ARTIFICIAL INTELLIGENCE (90%); INFORMATION TECHNOLOGY INDUSTRY (78%); MEDIA BIAS (75%); BANKING & FINANCE (73%); INFORMATION SECURITY & PRIVACY (69%)
Person: BILL O'REILLY (77%)
Load-Date: July 18, 2023","While more companies are realizing the value of artificial intelligence (AI) and are levering the technology to improve efficiency and bring down costs, ethical concerns continue to rise as AI takes bigger decision-making role in more industries. According to the O'Reilly 2021 AI Adoption in the Enterprise report, less than half of companies surveyed said they haven't thought through the consequences AI products. Only organizations with matured AI practice bother to check the fairness, bias and ethics of their AI platforms. In that sense one can say that AI ethics is directly proportional to AI maturity of an organization.
The problem however is that companies often overestimate their level of maturity when it comes to responsible AI implementation. As another survey conducted by BCG GAMMA finds that 55% of all respondents overestimated the maturity of their AI program. While 26% of companies say they've hit scale in their AI deployment, only 12% include a responsible AI program as part of their work.
This clearly shows that organizations are overly optimistic about the maturity of their AI implementation. As Steven Mills, BCG GAMMA's chief ethics officer and coauthor mentions, ""While many organizations are making progress, it's clear the depth and breadth of most efforts fall behind what is needed to truly ensure responsible AI - creating a major ethical dilemma in AI.""
The O'Reilly report also notes that AI implementation won't hit maturity until ethics, safety, privacy, and security are primary rather than secondary concerns.
To support AI maturity, teams can benefit from reviewing case studies in how other organizations have managed AI implementation, believes Rachel Roumeliotis, VP of Content Strategy, O'Reilly Media.
For some companies, the ethical dimension of AI implementation hasn't been fully thought through because they have yet to deploy at scale and reach full maturity, explains Roumeliotis. He adds that bias can seep into AI products at multiple points in the creation process from the data fueling decisions to the algorithmic training and final review stage.
Viral Thakker, Partner, Deloitte India, says, ""AI ethics deal with managing ethical complexities in the age of vast amounts of data and extensive use of automation. The big issues driving this are privacy considerations, lack of transparency of ""black box"" AI models, bias and discrimination that may be embedded in the data, which AI algorithms learn from, as well as lack of governance and accountability.
Worldwide business spending on AI is expected to hit $50 billion this year and $110 billion annually by 2024, even after the global economic slump caused by the COVID-19 pandemic, according to a forecast by research firm IDC. Retail, healthcare and banking industries are slated to spend the maximum, said the analyst firm.
For all the good that AI can bring, policy makers and responsible tech companies must recognize, execute, and mitigate its potential unintended, harmful effects.
Praveen Kumar, Vice President, Digital & Innovations, JK Technosoft states, ""Organizations should start thinking of educating and upskilling employees as AI will incorporate a great deal of work alongside human workforces. At the same time, it is of utmost importance to consider the necessary and adequate governance mechanisms carefully and proactively for ensuring ethical considerations in the deployment of responsible AI tools.""
Already several organizations are offering programs to help employees put ethics at the core of their respective workflows. These programs are designed to empower the entire organization to think critically about every step of the process of building AI solutions, and perhaps most importantly continue advancing AI that is safe and inclusive.
A big sign in AI maturity is to grow the company's knowledge on the subject, and nurture an organization-wide 'ethical-first' approach, very similar to the 'security-first' philosophy.
As we move forward, our reliability on AI will deepen which will inevitably cause many ethical issues, especially in industries where personal and business data is at stake Companies should therefore start thinking about how they will retrain and educate employees with regard to AI's ethical implication.
Shreeranganath Kulkarni, Chief Delivery Officer, Birlasoft suggests that for enterprise AI to reach maturity, they should work towards an AI playbook built on the pillars of AI strategy, data, talent, technology, execution, and culture.
Organizations that are able to adapt and upgrade themselves and emerge stronger will be the ones to benefit from any kind of disruptions. In the process, they must consider carefully and proactively the necessary governance mechanisms to be used to ensure ethical considerations in the deployment of AI tools. Building trust in AI solutions and tools is vital so that businesses and individuals can benefit from its use.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ARTIFICIAL INTELLIGENCE ETHICS (95%); ETHICS (95%); ARTIFICIAL INTELLIGENCE (90%); BUSINESS ETHICS (78%); BUSINESS NEWS (78%); CORPORATE GOVERNANCE (78%); POLLS & SURVEYS (77%); DISCRIMINATION (75%); EXECUTIVES (71%); PUBLIC HEALTH (70%); SAFETY (69%); EPIDEMICS (65%); INFECTIOUS DISEASE (65%); CASE STUDIES (63%); COVID CORONAVIRUS (63%); COVID-19 CORONAVIRUS (60%); ECONOMY & ECONOMIC INDICATORS (60%)
Company:  DELOITTE LLP (52%)
Industry: NAICS541211 OFFICES OF CERTIFIED PUBLIC ACCOUNTANTS (52%); SIC8721 ACCOUNTING, AUDITING, & BOOKKEEPING SERVICES (52%); ARTIFICIAL INTELLIGENCE ETHICS (95%); ARTIFICIAL INTELLIGENCE (90%); INFORMATION TECHNOLOGY INDUSTRY (78%); MEDIA BIAS (75%); BANKING & FINANCE (73%); INFORMATION SECURITY & PRIVACY (69%)
Person: BILL O'REILLY (77%)
Load-Date: July 18, 2023",neutral,0.708895206451416,balanced/neutral,"['privacy', 'bias', 'discrimination', 'fairness', 'transparency', 'accountability', 'safety', 'security']",['fairness'],"['policy', 'governance', 'should', 'must']",[],8,1,4,0
2021,Unknown Title,"Body
Some AI researchers think that computers should be able to reason and make decisions based on explicit moral criteria. For example, Susan Anderson and Michael Anderson (in Machine Ethics, 2011), say: ""Ideally, we would like to be able to trust machines to make correct ethical decisions on their own, and this requires that we create an ethics for machines"".
Their argument is that if these machines can make thousands of intelligent decisions for themselves (e.g., in the case of autonomous vehicles, when to brake, when to stop, when to yield, etc.), they should also be able to make ethical decisions. This view is because these researchers do not distinguish between reasoning about factual matters and moral issues, because they believe, in my opinion wrongly, that moral decisions can always be rationalized. In fact in 1853, John Stuart Mill, in his work on Utilitarianism, already claimed that the power to make moral decisions is a component of our reason.
Moral decisions cannot always be rationalized.
Two general approaches have been suggested for intelligent machines to make ethical decisions for themselves: The top-down approach and the bottom-up approach. In the top-down approach, ethical principles would be explicitly programmed into the machine, for example into the vehicle's automatic driving system.
Thus, machines could obey ""laws"" in the style of the three laws of Robotics, formulated by Asimov in 1942 in the short story ""Runaround"", or even a general moral philosophy, such as Kant's categorical imperative, Stuart Mill's utilitarianism, or some other form of consequentialism. The key point is for a programmer to incorporate instructions so that the machine proceeds under specific conditions in the most ethical way possible. That is, the machine makes ethical decisions based on the moral philosophy that has been implanted in its software.
Critics of the top-down approach recognize the inherent difficulties of adhering to any moral philosophy, since any one of them will, at some point or another, lead to actions and outcomes that many will find morally unacceptable because there are no universal ethical principles. It is thus practically impossible to program a machine that is capable of making moral decisions on its own based on a particular moral philosophy. On the other hand, we first acquire moral values from those who educate us and then adapt them based on input from the social groups and cultures to which we are exposed and gradually develop our own personal morality.
You can't program a machine to be able to make moral decisions on its own.
In the second approach, the bottom-up approach, machines are expected to learn to make ethical decisions by observing human behavior in real situations, without being taught any formal rules or equipped with any particular moral philosophy. The problem is that the machine would need a long period of time, observing human behavior, to learn to make ethical decisions in this way.
To speed up this process, some have suggested that machines could learn based on the ethical decisions of millions of people. In the case of driverless vehicles, they would learn from the decisions of millions of human drivers.
It should be noted, however, that this may result in autonomous vehicles acquiring highly unethical behavior, as it is clear that many drivers engage in less than exemplary behavior. If they learn what a multitude of people do, intelligent vehicles will learn to disregard speed limits, carry out dangerous overtaking, and so on. In other words, observing people will not teach them what is ethical, but what is common.
Learning from human behavior will not teach machines what is ethical, but what is common.
Another major difficulty in incorporating ethics into machines is the concept of autonomy. Autonomy is, in fact, a gradual concept. Machines equipped with AI can act much more autonomously than those not equipped with it. Indeed, we know that machine learning algorithms are increasingly helping to make decisions about, for example, granting or not granting credit, medical diagnoses, personalized recommendations, and even selection of job candidates. Some machines can achieve high degrees of autonomy.
One (bad) example is autonomous lethal weapons that choose their own targets without human intervention and their decisions cannot be reversed. But even autonomous lethal weapons are limited to missions assigned to them by a human, and are ""only"" ""free"" to choose from a set of targets previously defined and programmed by humans. 
Autonomous lethal weapons decisions are limited to a set of human-programmed targets.
Military ethicist George Lucas, Jr., in the book Killing by remote control: the ethics of an unmanned military, points out that debates about the ethics of machines often confuse machine autonomy with moral autonomy and cites autonomous vacuum cleaners and Patriot missiles as examples.  Lucas argues that both are autonomous in the sense that they perform their missions, adapting and responding to unforeseen circumstances, with minimal human supervision, but they cannot autonomously decide to change or abort their mission based on moral objections. Machines are thus ultimately tools of the human beings who design and manufacture them.
In short, it is we humans who have the attributes necessary for moral agency. The ethics of AI is the ethics of the people who design the machines. AI absolutely depends on people at every stage: from the fundamental research phase through development. The problem is not Frankenstein's monster, the problem is Dr. Frankenstein!
The ethics of artificial intelligence is the ethics of the people who design the machines.
Classification
Language: ENGLISH
Publication-Type: Newspaper
Journal Code: CENFENG
Subject: ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); BEHAVIOR & COGNITION (89%); PHILOSOPHY (89%); ROBOTICS (78%); vida (%)
Industry: ARTIFICIAL INTELLIGENCE (90%); COMPUTER SOFTWARE (78%); ROBOTICS (78%); AUTONOMOUS MOTOR VEHICLES (75%); AUTONOMOUS VEHICLES (75%)
Load-Date: September 8, 2021","Some AI researchers think that computers should be able to reason and make decisions based on explicit moral criteria. For example, Susan Anderson and Michael Anderson (in Machine Ethics, 2011), say: ""Ideally, we would like to be able to trust machines to make correct ethical decisions on their own, and this requires that we create an ethics for machines"".
Their argument is that if these machines can make thousands of intelligent decisions for themselves (e.g., in the case of autonomous vehicles, when to brake, when to stop, when to yield, etc.), they should also be able to make ethical decisions. This view is because these researchers do not distinguish between reasoning about factual matters and moral issues, because they believe, in my opinion wrongly, that moral decisions can always be rationalized. In fact in 1853, John Stuart Mill, in his work on Utilitarianism, already claimed that the power to make moral decisions is a component of our reason.
Moral decisions cannot always be rationalized.
Two general approaches have been suggested for intelligent machines to make ethical decisions for themselves: The top-down approach and the bottom-up approach. In the top-down approach, ethical principles would be explicitly programmed into the machine, for example into the vehicle's automatic driving system.
Thus, machines could obey ""laws"" in the style of the three laws of Robotics, formulated by Asimov in 1942 in the short story ""Runaround"", or even a general moral philosophy, such as Kant's categorical imperative, Stuart Mill's utilitarianism, or some other form of consequentialism. The key point is for a programmer to incorporate instructions so that the machine proceeds under specific conditions in the most ethical way possible. That is, the machine makes ethical decisions based on the moral philosophy that has been implanted in its software.
Critics of the top-down approach recognize the inherent difficulties of adhering to any moral philosophy, since any one of them will, at some point or another, lead to actions and outcomes that many will find morally unacceptable because there are no universal ethical principles. It is thus practically impossible to program a machine that is capable of making moral decisions on its own based on a particular moral philosophy. On the other hand, we first acquire moral values from those who educate us and then adapt them based on input from the social groups and cultures to which we are exposed and gradually develop our own personal morality.
You can't program a machine to be able to make moral decisions on its own.
In the second approach, the bottom-up approach, machines are expected to learn to make ethical decisions by observing human behavior in real situations, without being taught any formal rules or equipped with any particular moral philosophy. The problem is that the machine would need a long period of time, observing human behavior, to learn to make ethical decisions in this way.
To speed up this process, some have suggested that machines could learn based on the ethical decisions of millions of people. In the case of driverless vehicles, they would learn from the decisions of millions of human drivers.
It should be noted, however, that this may result in autonomous vehicles acquiring highly unethical behavior, as it is clear that many drivers engage in less than exemplary behavior. If they learn what a multitude of people do, intelligent vehicles will learn to disregard speed limits, carry out dangerous overtaking, and so on. In other words, observing people will not teach them what is ethical, but what is common.
Learning from human behavior will not teach machines what is ethical, but what is common.
Another major difficulty in incorporating ethics into machines is the concept of autonomy. Autonomy is, in fact, a gradual concept. Machines equipped with AI can act much more autonomously than those not equipped with it. Indeed, we know that machine learning algorithms are increasingly helping to make decisions about, for example, granting or not granting credit, medical diagnoses, personalized recommendations, and even selection of job candidates. Some machines can achieve high degrees of autonomy.
One (bad) example is autonomous lethal weapons that choose their own targets without human intervention and their decisions cannot be reversed. But even autonomous lethal weapons are limited to missions assigned to them by a human, and are ""only"" ""free"" to choose from a set of targets previously defined and programmed by humans. 
Autonomous lethal weapons decisions are limited to a set of human-programmed targets.
Military ethicist George Lucas, Jr., in the book Killing by remote control: the ethics of an unmanned military, points out that debates about the ethics of machines often confuse machine autonomy with moral autonomy and cites autonomous vacuum cleaners and Patriot missiles as examples.  Lucas argues that both are autonomous in the sense that they perform their missions, adapting and responding to unforeseen circumstances, with minimal human supervision, but they cannot autonomously decide to change or abort their mission based on moral objections. Machines are thus ultimately tools of the human beings who design and manufacture them.
In short, it is we humans who have the attributes necessary for moral agency. The ethics of AI is the ethics of the people who design the machines. AI absolutely depends on people at every stage: from the fundamental research phase through development. The problem is not Frankenstein's monster, the problem is Dr. Frankenstein!
The ethics of artificial intelligence is the ethics of the people who design the machines.
Classification
Language: ENGLISH
Publication-Type: Newspaper
Journal Code: CENFENG
Subject: ETHICS (94%); ARTIFICIAL INTELLIGENCE (90%); BEHAVIOR & COGNITION (89%); PHILOSOPHY (89%); ROBOTICS (78%); vida (%)
Industry: ARTIFICIAL INTELLIGENCE (90%); COMPUTER SOFTWARE (78%); ROBOTICS (78%); AUTONOMOUS MOTOR VEHICLES (75%); AUTONOMOUS VEHICLES (75%)
Load-Date: September 8, 2021",neutral,0.7980936169624329,balanced/neutral,"['autonomy', 'agency']","['utilitarianism', 'categorical imperative', 'kant', 'autonomy', 'consequentialism']",['should'],"['machine learning', 'robotics']",2,5,1,2
2021,Unknown Title,"Body
Link to Story
A huge advancement for deepfakes in digital art, marketing, and more.
July 27, 2021 As deepfakes get more popular and AI (artificial intelligence) gets smarter, frauds give a bad name to the integrated technology. Unethical and dishonest deepfake video editing is known to spread misinformation. Fake videos easily skew a market’s idea and/or beliefs around current events, politics, celebrities, and more.
It’s clear why deepfakes are described as a threat to peoples’ perceptions, personal rights, and reputations. But thankfully, Deepfakes Web takes a moral and affordable approach to deepfake AI.
Deepfakes Web is an online deepfake app that allows digital creators, designers, and artists to make convincing yet ethical deepfake videos in three clicks. Its private cloud secures stored data for easy rendering and trained model reuse, but what truly sets Deepfakes Web apart from other software is its ethical principles.
The software company believes deepfake technology is becoming more popular and therefore should be labeled (i.e. watermarked) responsibly. A clear watermark ensures deepfake videos can be enjoyed safely without being used with harmful intent.
It should be noted that, even with a protective watermark, a deepfake’s quality is never compromised. Deepfake Web’s watermarked videos look real enough to believe at first glance, but are subtly mechanical. This gives an ethically sound edge to lifelike deepfakes, which is believed to have tremendous value for virtual artists and digital content creators.
When used for good reason, deepfakes can be an awesome tool for demonstrating and teaching, for instance. Deepfake Web’s ethical software technology changes the game for deepfakes in art, education, marketing, and entertainment, to name a few. Now, there is more freedom than ever for digital gurus to create honest, quality deepfake videos without the fear of pushback or accusations from critics.
Deepfakes have earned a bad reputation, thanks to irresponsible early adopters. Deepfakes Web grapples to surpass what deepfakes are known for, emphasizing the promise and positive potential in deepfake technology.
About Deepfakes Web
Listed by tech sources as a top-tier deepfake website, Deepfakes Web offers a fast, affordable process for online deepfake video creation. Premium and basic deepfake video modes are available, depending on artists’ preference. For more detailed information on creating a deepfake video, watch this tutorial by Deepfakes Web or dive right in and make a deepfake video here .
MENAFN27072021003238003268ID1102521776
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: DEEPFAKE TECHNOLOGY (97%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ETHICS (90%); FILM (90%); GENERATIVE AI (90%); FRAUD & FINANCIAL CRIME (89%); ARTISTS & PERFORMERS (78%); ARTS & HUMANITIES EDUCATION (78%); CELEBRITIES (78%); DISINFORMATION & MISINFORMATION (78%)
Industry: DEEPFAKE TECHNOLOGY (97%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); COMPUTER SOFTWARE (90%); FILM (90%); GENERATIVE AI (90%); SOFTWARE SERVICES & APPLICATIONS (89%); ARTISTS & PERFORMERS (78%); CELEBRITIES (78%); CONTENT MARKETING (78%); DATA STORAGE TECHNOLOGY (78%); DIGITAL CONTENT MARKETING (78%); INFORMATION SECURITY & PRIVACY (76%); SOFTWARE MAKERS (74%); MEDIA CONTENT (72%)
Load-Date: September 15, 2021","Link to Story
A huge advancement for deepfakes in digital art, marketing, and more.
July 27, 2021 As deepfakes get more popular and AI (artificial intelligence) gets smarter, frauds give a bad name to the integrated technology. Unethical and dishonest deepfake video editing is known to spread misinformation. Fake videos easily skew a market’s idea and/or beliefs around current events, politics, celebrities, and more.
It’s clear why deepfakes are described as a threat to peoples’ perceptions, personal rights, and reputations. But thankfully, Deepfakes Web takes a moral and affordable approach to deepfake AI.
Deepfakes Web is an online deepfake app that allows digital creators, designers, and artists to make convincing yet ethical deepfake videos in three clicks. Its private cloud secures stored data for easy rendering and trained model reuse, but what truly sets Deepfakes Web apart from other software is its ethical principles.
The software company believes deepfake technology is becoming more popular and therefore should be labeled (i.e. watermarked) responsibly. A clear watermark ensures deepfake videos can be enjoyed safely without being used with harmful intent.
It should be noted that, even with a protective watermark, a deepfake’s quality is never compromised. Deepfake Web’s watermarked videos look real enough to believe at first glance, but are subtly mechanical. This gives an ethically sound edge to lifelike deepfakes, which is believed to have tremendous value for virtual artists and digital content creators.
When used for good reason, deepfakes can be an awesome tool for demonstrating and teaching, for instance. Deepfake Web’s ethical software technology changes the game for deepfakes in art, education, marketing, and entertainment, to name a few. Now, there is more freedom than ever for digital gurus to create honest, quality deepfake videos without the fear of pushback or accusations from critics.
Deepfakes have earned a bad reputation, thanks to irresponsible early adopters. Deepfakes Web grapples to surpass what deepfakes are known for, emphasizing the promise and positive potential in deepfake technology.
About Deepfakes Web
Listed by tech sources as a top-tier deepfake website, Deepfakes Web offers a fast, affordable process for online deepfake video creation. Premium and basic deepfake video modes are available, depending on artists’ preference. For more detailed information on creating a deepfake video, watch this tutorial by Deepfakes Web or dive right in and make a deepfake video here .
MENAFN27072021003238003268ID1102521776
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: DEEPFAKE TECHNOLOGY (97%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); ETHICS (90%); FILM (90%); GENERATIVE AI (90%); FRAUD & FINANCIAL CRIME (89%); ARTISTS & PERFORMERS (78%); ARTS & HUMANITIES EDUCATION (78%); CELEBRITIES (78%); DISINFORMATION & MISINFORMATION (78%)
Industry: DEEPFAKE TECHNOLOGY (97%); ARTIFICIAL INTELLIGENCE (90%); ARTIFICIAL INTELLIGENCE ETHICS (90%); COMPUTER SOFTWARE (90%); FILM (90%); GENERATIVE AI (90%); SOFTWARE SERVICES & APPLICATIONS (89%); ARTISTS & PERFORMERS (78%); CELEBRITIES (78%); CONTENT MARKETING (78%); DATA STORAGE TECHNOLOGY (78%); DIGITAL CONTENT MARKETING (78%); INFORMATION SECURITY & PRIVACY (76%); SOFTWARE MAKERS (74%); MEDIA CONTENT (72%)
Load-Date: September 15, 2021",neutral,0.5970510840415955,balanced/neutral,"['privacy', 'security', 'misinformation', 'disinformation']",[],['should'],['generative ai'],4,0,1,1
2021,Unknown Title,"Byline: Parnell Palme McGuinness
Body
Ethics is at the heart of the technological arms race the world is officially not participating in. When the West's values are leveraged against us by autocrats, they become our weakness as well as our strength.
This week the Prime Minister gave a speech as part of the Australian Strategic Policy Institute's new Sydney Dialogues series that, despite less fanfare, was more consequential than the AUKUS submarine deal. In the address, delivered by video, Scott Morrison touched on quantum and artificial intelligence technologies that are emerging as areas of significant geostrategic concern. While reporting focused on the technologies themselves, the latter half of the speech, dedicated to the ethics, is at least as important.
""Australia, like the United States, is committed to playing our part so that the rules and norms around technology reflect the values of our open societies,"" Morrison said. ""We cannot shy away from the ethical implications of new technologies.""
The escalating tension between democracies and autocracies is defined by a clash of values. Western liberal democracies are, in the Prime Minister's words, ""based on respect for the rule of law, human rights, economic and religious freedom, gender equality, and independent institutions"". This openness, tolerance and diversity are defining features of Western culture - but they can be cynically weaponised against us.
Joseph Henrich, a leading evolutionary biologist and scholar of human societies, calls the compassion and tolerance we extend, through laws, to those outside our immediate circle of kinship and acquaintance ""moral universalism"". He notes how counterintuitive it is for humans, the product of the ""selfish gene"", to create societies that care for people who are no relation to us. However, over decades of anthropological study, Henrich has concluded that moral universalism has been key to the prosperity of the West.
That is not to say the West is perfect. Western societies do not always behave well, but among our strengths is a willingness to acknowledge faults and temper behaviour. European nations were brutal colonisers but began to self-correct due to pressure from within. We look back with regret at our ancestors' misdeeds and strive to make amends. We recognise the value of every individual and seek to correct inequalities.
Now we bring moral universalism to the realm of AI. As the Prime Minister notes, we are ""one of only 15 founding members of the Global Partnership on AI, a coalition working to ensure AI is used responsibly and respects human rights and democratic values"".
In response, a Russian emigre data scientist and pessimist of record warns me darkly that ""ethics in AI reminds me of unilateral nuclear disarmament"". He fears we'll put ourselves at a disadvantage. Autocratic governments are ""ruthlessly pursuing effectiveness"" in AI, unfettered by ethical concerns. ""If you want to talk ethics, I believe it's unethical for us to be weak right now, particularly if you value a society that takes these ethics seriously.""
""The liberalism of the West is both its strength and its weakness,"" says Tiberio Caetano, chief scientist at the Gradient Institute, a think tank devoted to promoting ethical AI. ""We must acknowledge that we live in a world in which power comes from knowledge and knowledge comes from data. This poses a fundamental threat to liberal democracy in which countries limit their data use out of respect for the privacy of the individual.""
Fergus Hanson, director of ASPI's International Cyber Policy Centre, is initially amused by my emigre friend's proposition. ""Microsoft famously built an AI chatbot which learned from the Chinese people what their dream was,"" Hanson says. ""As a result, the chatbot later declared that its dream was to move to America. I don't think the Chinese government is going to be very comfortable with unfettered AI."" But he says ""it comes down to what you can tolerate in ethical terms. Especially if you have it applying to weapons systems - if one country is prepared to push an ethical or moral boundary and have unfettered AI you may find a problem arises.""
James Paterson, who chairs the Australian Parliament's intelligence and security committee, believes autocratic countries are willing to exploit our values. ""They see us as vulnerable because we are open and diverse."" But he warns: ""We should not change ourselves in order to defeat our opponents.""
""It's the open societies that are the source of new ideas,"" Caetano concurs. ""Where does the innovation of the last decade come from? Silicon Valley. Liberal democracies attract the best people and free societies are conducive to coming up with the best ideas.""
Still, he believes the West faces a huge challenge, which will take us to the edge of our understanding of ethics. ""We better not f--- it up.""
Parnell Palme McGuinness is managing director at strategic communications firm Agenda C. She has in the past done work for the Australian Liberal Party and the German Greens.
Our openness can be cynically weaponised against us.
Classification
Language: ENGLISH
Publication-Type: Newspaper
Subject: ETHICS (91%); ARMS RACE (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (90%); TECHNOLOGY (90%); TYPES OF GOVERNMENT (90%); DEMOCRACIES (89%); GOVERNMENT BODIES & OFFICES (89%); HEADS OF STATE & GOVERNMENT (89%); HUMAN RIGHTS & CIVIL LIBERTIES LAW (87%); ARTIFICIAL INTELLIGENCE ETHICS (79%); LIBERALISM (79%); RULE OF LAW (79%); ARMS CONTROL & DISARMAMENT (78%); EMERGING TECHNOLOGY (78%); MILITARY WEAPONS (78%); WEAPONS DECOMMISSIONING (78%); ARTIFICIAL INTELLIGENCE (77%); PRIME MINISTERS (77%); HUMAN RIGHTS (75%); SCIENCE & TECHNOLOGY (75%); GENDER EQUALITY (73%); NUCLEAR WEAPONS (73%); DATA SCIENCE (72%); ANTHROPOLOGY & ARCHAEOLOGY (65%); RELIGION (51%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (79%); MILITARY WEAPONS (78%); ARTIFICIAL INTELLIGENCE (77%); NUCLEAR WEAPONS (73%); DATA SCIENCE (72%)
Person: SCOTT MORRISON (79%)
Geographic: SYDNEY, AUSTRALIA (88%); AUSTRALIA (91%); EUROPE (79%); UNITED STATES (79%)
Load-Date: November 19, 2021","Ethics is at the heart of the technological arms race the world is officially not participating in. When the West's values are leveraged against us by autocrats, they become our weakness as well as our strength.
This week the Prime Minister gave a speech as part of the Australian Strategic Policy Institute's new Sydney Dialogues series that, despite less fanfare, was more consequential than the AUKUS submarine deal. In the address, delivered by video, Scott Morrison touched on quantum and artificial intelligence technologies that are emerging as areas of significant geostrategic concern. While reporting focused on the technologies themselves, the latter half of the speech, dedicated to the ethics, is at least as important.
""Australia, like the United States, is committed to playing our part so that the rules and norms around technology reflect the values of our open societies,"" Morrison said. ""We cannot shy away from the ethical implications of new technologies.""
The escalating tension between democracies and autocracies is defined by a clash of values. Western liberal democracies are, in the Prime Minister's words, ""based on respect for the rule of law, human rights, economic and religious freedom, gender equality, and independent institutions"". This openness, tolerance and diversity are defining features of Western culture - but they can be cynically weaponised against us.
Joseph Henrich, a leading evolutionary biologist and scholar of human societies, calls the compassion and tolerance we extend, through laws, to those outside our immediate circle of kinship and acquaintance ""moral universalism"". He notes how counterintuitive it is for humans, the product of the ""selfish gene"", to create societies that care for people who are no relation to us. However, over decades of anthropological study, Henrich has concluded that moral universalism has been key to the prosperity of the West.
That is not to say the West is perfect. Western societies do not always behave well, but among our strengths is a willingness to acknowledge faults and temper behaviour. European nations were brutal colonisers but began to self-correct due to pressure from within. We look back with regret at our ancestors' misdeeds and strive to make amends. We recognise the value of every individual and seek to correct inequalities.
Now we bring moral universalism to the realm of AI. As the Prime Minister notes, we are ""one of only 15 founding members of the Global Partnership on AI, a coalition working to ensure AI is used responsibly and respects human rights and democratic values"".
In response, a Russian emigre data scientist and pessimist of record warns me darkly that ""ethics in AI reminds me of unilateral nuclear disarmament"". He fears we'll put ourselves at a disadvantage. Autocratic governments are ""ruthlessly pursuing effectiveness"" in AI, unfettered by ethical concerns. ""If you want to talk ethics, I believe it's unethical for us to be weak right now, particularly if you value a society that takes these ethics seriously.""
""The liberalism of the West is both its strength and its weakness,"" says Tiberio Caetano, chief scientist at the Gradient Institute, a think tank devoted to promoting ethical AI. ""We must acknowledge that we live in a world in which power comes from knowledge and knowledge comes from data. This poses a fundamental threat to liberal democracy in which countries limit their data use out of respect for the privacy of the individual.""
Fergus Hanson, director of ASPI's International Cyber Policy Centre, is initially amused by my emigre friend's proposition. ""Microsoft famously built an AI chatbot which learned from the Chinese people what their dream was,"" Hanson says. ""As a result, the chatbot later declared that its dream was to move to America. I don't think the Chinese government is going to be very comfortable with unfettered AI."" But he says ""it comes down to what you can tolerate in ethical terms. Especially if you have it applying to weapons systems - if one country is prepared to push an ethical or moral boundary and have unfettered AI you may find a problem arises.""
James Paterson, who chairs the Australian Parliament's intelligence and security committee, believes autocratic countries are willing to exploit our values. ""They see us as vulnerable because we are open and diverse."" But he warns: ""We should not change ourselves in order to defeat our opponents.""
""It's the open societies that are the source of new ideas,"" Caetano concurs. ""Where does the innovation of the last decade come from? Silicon Valley. Liberal democracies attract the best people and free societies are conducive to coming up with the best ideas.""
Still, he believes the West faces a huge challenge, which will take us to the edge of our understanding of ethics. ""We better not f--- it up.""
Parnell Palme McGuinness is managing director at strategic communications firm Agenda C. She has in the past done work for the Australian Liberal Party and the German Greens.
Our openness can be cynically weaponised against us.
Classification
Language: ENGLISH
Publication-Type: Newspaper
Subject: ETHICS (91%); ARMS RACE (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (90%); TECHNOLOGY (90%); TYPES OF GOVERNMENT (90%); DEMOCRACIES (89%); GOVERNMENT BODIES & OFFICES (89%); HEADS OF STATE & GOVERNMENT (89%); HUMAN RIGHTS & CIVIL LIBERTIES LAW (87%); ARTIFICIAL INTELLIGENCE ETHICS (79%); LIBERALISM (79%); RULE OF LAW (79%); ARMS CONTROL & DISARMAMENT (78%); EMERGING TECHNOLOGY (78%); MILITARY WEAPONS (78%); WEAPONS DECOMMISSIONING (78%); ARTIFICIAL INTELLIGENCE (77%); PRIME MINISTERS (77%); HUMAN RIGHTS (75%); SCIENCE & TECHNOLOGY (75%); GENDER EQUALITY (73%); NUCLEAR WEAPONS (73%); DATA SCIENCE (72%); ANTHROPOLOGY & ARCHAEOLOGY (65%); RELIGION (51%)
Industry: ARTIFICIAL INTELLIGENCE ETHICS (79%); MILITARY WEAPONS (78%); ARTIFICIAL INTELLIGENCE (77%); NUCLEAR WEAPONS (73%); DATA SCIENCE (72%)
Person: SCOTT MORRISON (79%)
Geographic: SYDNEY, AUSTRALIA (88%); AUSTRALIA (91%); EUROPE (79%); UNITED STATES (79%)
Load-Date: November 19, 2021",neutral,0.6520435810089111,balanced/neutral,"['privacy', 'security', 'human rights']",['equality'],"['policy', 'law', 'should', 'must']",[],3,1,4,0
2021,Unknown Title,"Body
UNESCO is celebrating its 75th anniversary at the 41st General Conference, which will take place in Paris from November 9th to 24th. The 193 member states are expected to make several important decisions on cultural heritage, global education policy, technology ethics and open science.
In addition to the celebrations for the 75th anniversary of UNESCO, which will be broadcast live from the UNESCO headquarters in Paris on November 12, the election of the President of the General Conference and the Director General of the organization will also be on the agenda. In addition, general conference is expected to adopt several recommendations on important issues.
UNESCO recommendations are legal instruments that provide the member states with guidelines for implementation, such as the general declaration on the human genome (1997) or bioethics (2005). Two recommendations are to be adopted this year: on the one hand, the Recommendation on the Ethics of Artificial Intelligence , which for the first time would create global guidelines on ethical principles for the development and use of artificial intelligence, and, on the other hand, the Recommendation on Open Science , which would provide general guidelines To facilitate and strengthen access to scientific knowledge and international cooperation.
On November 10, UNESCO will present the Report on the Futures of Education , which was prepared by an independent commission of experts headed by Sahle-Work Zewde, President of the Republic of Ethiopia. On the same day, UNESCO is organizing the Global Education Meeting in collaboration with the French government .
In the field of culture, Member States are expected to approve the UNESCO Conference on Cultural Policy and Sustainable Development MONDIACULT 2022 to be held in Mexico City from September 28-30, 2022. As part of the General Conference, UNESCO will also present a number of prizes and awards to recognize great achievements in the fields of education, science and culture.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Journal Code: 193
Subject: ANNIVERSARIES (92%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); ASSOCIATIONS & ORGANIZATIONS (90%); CUSTOMS & CULTURAL HERITAGE (79%); TALKS & MEETINGS (78%); ARTIFICIAL INTELLIGENCE ETHICS (77%); BIOETHICS (77%); EDUCATION REGULATION & POLICY (77%); PUBLIC POLICY (77%); APPROVALS (72%); INTERNATIONAL RELATIONS (72%); INTERNATIONAL RELATIONS & NATIONAL SECURITY (72%); HEADS OF GOVERNMENT ELECTIONS (71%); SUSTAINABLE DEVELOPMENT (67%); AWARDS & PRIZES (65%); SUSTAINABILITY (50%)
Industry: ARTIFICIAL INTELLIGENCE (90%); PRESS AGENCY RELEASES (90%); ARTIFICIAL INTELLIGENCE ETHICS (77%); SUSTAINABLE DEVELOPMENT (67%)
Person: SAHLE-WORK ZEWDE (79%)
Geographic: PARIS, FRANCE (73%); FRANCE (90%); ETHIOPIA (79%); MEXICO (79%); GERMANY (73%)
Load-Date: October 30, 2021","UNESCO is celebrating its 75th anniversary at the 41st General Conference, which will take place in Paris from November 9th to 24th. The 193 member states are expected to make several important decisions on cultural heritage, global education policy, technology ethics and open science.
In addition to the celebrations for the 75th anniversary of UNESCO, which will be broadcast live from the UNESCO headquarters in Paris on November 12, the election of the President of the General Conference and the Director General of the organization will also be on the agenda. In addition, general conference is expected to adopt several recommendations on important issues.
UNESCO recommendations are legal instruments that provide the member states with guidelines for implementation, such as the general declaration on the human genome (1997) or bioethics (2005). Two recommendations are to be adopted this year: on the one hand, the Recommendation on the Ethics of Artificial Intelligence , which for the first time would create global guidelines on ethical principles for the development and use of artificial intelligence, and, on the other hand, the Recommendation on Open Science , which would provide general guidelines To facilitate and strengthen access to scientific knowledge and international cooperation.
On November 10, UNESCO will present the Report on the Futures of Education , which was prepared by an independent commission of experts headed by Sahle-Work Zewde, President of the Republic of Ethiopia. On the same day, UNESCO is organizing the Global Education Meeting in collaboration with the French government .
In the field of culture, Member States are expected to approve the UNESCO Conference on Cultural Policy and Sustainable Development MONDIACULT 2022 to be held in Mexico City from September 28-30, 2022. As part of the General Conference, UNESCO will also present a number of prizes and awards to recognize great achievements in the fields of education, science and culture.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Journal Code: 193
Subject: ANNIVERSARIES (92%); ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); ASSOCIATIONS & ORGANIZATIONS (90%); CUSTOMS & CULTURAL HERITAGE (79%); TALKS & MEETINGS (78%); ARTIFICIAL INTELLIGENCE ETHICS (77%); BIOETHICS (77%); EDUCATION REGULATION & POLICY (77%); PUBLIC POLICY (77%); APPROVALS (72%); INTERNATIONAL RELATIONS (72%); INTERNATIONAL RELATIONS & NATIONAL SECURITY (72%); HEADS OF GOVERNMENT ELECTIONS (71%); SUSTAINABLE DEVELOPMENT (67%); AWARDS & PRIZES (65%); SUSTAINABILITY (50%)
Industry: ARTIFICIAL INTELLIGENCE (90%); PRESS AGENCY RELEASES (90%); ARTIFICIAL INTELLIGENCE ETHICS (77%); SUSTAINABLE DEVELOPMENT (67%)
Person: SAHLE-WORK ZEWDE (79%)
Geographic: PARIS, FRANCE (73%); FRANCE (90%); ETHIOPIA (79%); MEXICO (79%); GERMANY (73%)
Load-Date: October 30, 2021",positive,0.6902333498001099,balanced/neutral,"['security', 'agency', 'access']",[],"['regulation', 'policy', 'guidelines']",[],3,0,3,0
2021,Unknown Title,"Body
Creating and maintaining cultures within public services that are based on ethical principles and codes of conduct has never been more important. 
The COVID-19 pandemic has shown the tremendous influence governments can have on people's lives, and, in turn, the need for people to trust governments, both when it comes to their decision-making processes and their accountability for the result. 
A familiar tool for disseminating organisational ethics is a code of conduct. This is usually a document outlining organisational values and appropriate behaviours in the workplace. But do such codes really change or influence people's behaviour? 
First, let's consider how codes of conduct function. Government codes of conduct are publicly available documents that communicate to both employees and the general public the standards expected of those employed within public services. In Australia, public service employees are made aware of the code on induction, and as part of ethical training throughout their careers. 
Government employees can be held accountable for breaches of the code, which may lead to disciplinary processes and penalties. All these elements make up a code of conduct that is enforceable, and aims to build up the public's trust that these standards will be upheld. 
Now a look at how well they function. In the Australian public sector, figures indicate that up to 10% of employees in state and federal jurisdictions are involved in misconduct cases. That is quite substantial, considering codes of conduct are designed to set clear parameters around what is acceptable and unacceptable behaviour at work. So what causes this disconnect between communicated values and actual behaviours?From my own experience studying and working in the field of ethics, ethical behaviour is mainly influenced by personal, rather than institutional, values. A person's sense of ethical boundaries at work is also mostly informed by that person's workplace environment, which may differ from a workplace code of conduct. 
Personal values come before codes of conduct
Public servants operate in a non-partisan environment, yet we all bring our own values and beliefs to work with us. These then influence our decision-making and behaviour. We each have different levels of commitment to ethical values and different perspectives on what justifies moral action. If your personal values already align with the values inscribed in a code of conduct manual, then those expressed values will seem like common sense. 
However, if there is a difference between these values and your own, then a code of conduct on its own is unlikely to change your personal values or influence your behaviour. Someone acting in their own interest, rather than in the public's interest, is unlikely to be dissuaded from this course of action by diktat. 
Quite apart from this, it is worth noting that, on a day-to-day basis, employees do not refer to codes of conduct when making decisions. Instead, they rely on their own judgement and experience, as well as their sense of the culture and expectations of their workplace. Their moral intuitions are especially geared towards the expectations of their closest colleagues and managers. Beyond this, any sense of ethical behaviour pertains to whether conduct is held to the same standard across an organisation. 
Consistency is key
Social learning research tells us people look for behavioural cues in those around them. That is why consistent messaging about the importance of ethical behaviour and reporting wrongdoing creates a powerful social norm. 
If there is inconsistency between the messaging and the behaviour, a culture of inaction emerges. Over time, observing unethical behaviour that goes unpunished desensitises employees, destroying any integrity between communicated values and what really goes on. Such disconnect often becomes evident through employees' silence and the (often later revealed) under-reporting of serious matters. 
A decrease in the rate of employees who report unethical behaviours may seem like a sign of an increasingly ethical culture, but it may instead indicate a lack of trust in how these matters are dealt with by an organisation. 
Clearly, culture and values play an important role in ethical behaviour. A code of conduct alone does not guarantee that employees will behave ethically. Despite its limitations, however, a code of conduct does establish a shared point of reference for an organisation's values. It puts expectations down in writing, visibly marking a coherent set of values. In short, workplace codes of conduct have their place and are certainly not pointless. 
However, creating an ethical culture in practice involves action at all levels. To refresh your own sense of what this means in your role, I suggest the following: 
That you and your colleagues reflect on and identify personal values and consider whether there is any conflict between personal and professional values
That you recognise the impact of your own attitudes and behaviours on colleagues
That you be aware of the impact of workplace situations and environments which can either encourage ethical behaviour or discourage employees from reporting wrongdoing
That supervisors and managers understand the impact that their attitudes and behaviours have on their team. Silence on ethical matters sends a message of indifference
That you and your team create an environment that is safe for all employees to report wrongdoing. This means understanding that it can be difficult to receive these reports due to personal cognitive bias and that it is important to always take these reports seriously
 Finally, I suggest that you and your colleagues consider whether the ethical messaging within your own organisation is internally coherent and externally consistent.  These efforts to build and improve the culture of your organisation will help increase trust and demonstrate that actions taken and decisions made are based on a strong ethical foundation.  This article is curated from Apolitical.    READ MOREAI can now learn to manipulate human behaviourMy minister is accused of rape; what does that mean for my department?
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (93%); CIVIL SERVICES (88%); COVID CORONAVIRUS (78%); COVID-19 CORONAVIRUS (78%); INFECTIOUS DISEASE (78%); MISCONDUCT (78%); NEGATIVE NEWS (78%); SERVICE WORKERS (78%); EPIDEMICS (73%); WORKER CATEGORIES (69%); NEGATIVE MISC NEWS (67%)
Geographic: AUSTRALIA (92%)
Load-Date: March 12, 2021","Creating and maintaining cultures within public services that are based on ethical principles and codes of conduct has never been more important. 
The COVID-19 pandemic has shown the tremendous influence governments can have on people's lives, and, in turn, the need for people to trust governments, both when it comes to their decision-making processes and their accountability for the result. 
A familiar tool for disseminating organisational ethics is a code of conduct. This is usually a document outlining organisational values and appropriate behaviours in the workplace. But do such codes really change or influence people's behaviour? 
First, let's consider how codes of conduct function. Government codes of conduct are publicly available documents that communicate to both employees and the general public the standards expected of those employed within public services. In Australia, public service employees are made aware of the code on induction, and as part of ethical training throughout their careers. 
Government employees can be held accountable for breaches of the code, which may lead to disciplinary processes and penalties. All these elements make up a code of conduct that is enforceable, and aims to build up the public's trust that these standards will be upheld. 
Now a look at how well they function. In the Australian public sector, figures indicate that up to 10% of employees in state and federal jurisdictions are involved in misconduct cases. That is quite substantial, considering codes of conduct are designed to set clear parameters around what is acceptable and unacceptable behaviour at work. So what causes this disconnect between communicated values and actual behaviours?From my own experience studying and working in the field of ethics, ethical behaviour is mainly influenced by personal, rather than institutional, values. A person's sense of ethical boundaries at work is also mostly informed by that person's workplace environment, which may differ from a workplace code of conduct. 
Personal values come before codes of conduct
Public servants operate in a non-partisan environment, yet we all bring our own values and beliefs to work with us. These then influence our decision-making and behaviour. We each have different levels of commitment to ethical values and different perspectives on what justifies moral action. If your personal values already align with the values inscribed in a code of conduct manual, then those expressed values will seem like common sense. 
However, if there is a difference between these values and your own, then a code of conduct on its own is unlikely to change your personal values or influence your behaviour. Someone acting in their own interest, rather than in the public's interest, is unlikely to be dissuaded from this course of action by diktat. 
Quite apart from this, it is worth noting that, on a day-to-day basis, employees do not refer to codes of conduct when making decisions. Instead, they rely on their own judgement and experience, as well as their sense of the culture and expectations of their workplace. Their moral intuitions are especially geared towards the expectations of their closest colleagues and managers. Beyond this, any sense of ethical behaviour pertains to whether conduct is held to the same standard across an organisation. 
Consistency is key
Social learning research tells us people look for behavioural cues in those around them. That is why consistent messaging about the importance of ethical behaviour and reporting wrongdoing creates a powerful social norm. 
If there is inconsistency between the messaging and the behaviour, a culture of inaction emerges. Over time, observing unethical behaviour that goes unpunished desensitises employees, destroying any integrity between communicated values and what really goes on. Such disconnect often becomes evident through employees' silence and the (often later revealed) under-reporting of serious matters. 
A decrease in the rate of employees who report unethical behaviours may seem like a sign of an increasingly ethical culture, but it may instead indicate a lack of trust in how these matters are dealt with by an organisation. 
Clearly, culture and values play an important role in ethical behaviour. A code of conduct alone does not guarantee that employees will behave ethically. Despite its limitations, however, a code of conduct does establish a shared point of reference for an organisation's values. It puts expectations down in writing, visibly marking a coherent set of values. In short, workplace codes of conduct have their place and are certainly not pointless. 
However, creating an ethical culture in practice involves action at all levels. To refresh your own sense of what this means in your role, I suggest the following: 
That you and your colleagues reflect on and identify personal values and consider whether there is any conflict between personal and professional values
That you recognise the impact of your own attitudes and behaviours on colleagues
That you be aware of the impact of workplace situations and environments which can either encourage ethical behaviour or discourage employees from reporting wrongdoing
That supervisors and managers understand the impact that their attitudes and behaviours have on their team. Silence on ethical matters sends a message of indifference
That you and your team create an environment that is safe for all employees to report wrongdoing. This means understanding that it can be difficult to receive these reports due to personal cognitive bias and that it is important to always take these reports seriously
 Finally, I suggest that you and your colleagues consider whether the ethical messaging within your own organisation is internally coherent and externally consistent.  These efforts to build and improve the culture of your organisation will help increase trust and demonstrate that actions taken and decisions made are based on a strong ethical foundation.  This article is curated from Apolitical.    READ MOREAI can now learn to manipulate human behaviourMy minister is accused of rape; what does that mean for my department?
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (93%); CIVIL SERVICES (88%); COVID CORONAVIRUS (78%); COVID-19 CORONAVIRUS (78%); INFECTIOUS DISEASE (78%); MISCONDUCT (78%); NEGATIVE NEWS (78%); SERVICE WORKERS (78%); EPIDEMICS (73%); WORKER CATEGORIES (69%); NEGATIVE MISC NEWS (67%)
Geographic: AUSTRALIA (92%)
Load-Date: March 12, 2021",neutral,0.7235910296440125,balanced/neutral,"['bias', 'accountability']",[],"['standards', 'suggest']",[],2,0,2,0
2021,Unknown Title,"Byline: States News Service
Dateline: RESEARCH TRIANGLE PARK, NC 
Body
The following information was released by Sigma Xi, the Scientific Research Society:
About the Conference
The 2021 conference theme is Roots to Fruits: Responsible Research for a Flourishing Humanity, How scientific virtues serve society. The conference will take place November 4 7, 2021 at The Conference and Event Center in Niagara Falls, New York. The conference is an international forum for researchers, ethicists, educators, and science communicators to examine what it means to conduct ethical, responsible research across science and engineering disciplines. Sessions will be organized under the following tracks.
Responsible Research and Discovery: Sessions and case studies on the broader societal impact of scientific discoveries and emerging issues in scientific integrity from different perspectives, including researchers, policymakers, and funding agencies.
Responsible STEM Education: Sessions that focus on the cultivation of the scientific character virtues and values; examine the role of the scientific community in protecting science education against policies that undermine scientific evidence; and introduce a comprehensive approach to STEM education that addresses equity and inclusion and promotes excellence in research.
Responsible Technology Innovation: Sessions that examine how to build values into the design process, including emerging ethical challenges associated with new technologies such as gene editing, artificial intelligence, robotics, data mining and privacy, and facial recognition.
General Research Ethics: Sessions on the broader ethical challenges facing the research community such as science and human rights, implicit biases, making the case for basic research, ethic of transitional medical research, authorship, intellectual property, and environmental ethics.
Science Communication, Education, and Public Engagement: Sessions on the science and best practices of science communication, social responsibilities of researchers in engaging with the public and policy makers, ethical considerations in Citizen Science, and integrating ethics training in STEM education.
Research Enterprise and Professional Development: Sessions on pursuing both academic and non-academic STEM careers, cultivating effective interdisciplinary collaborations, leadership training, science policy, publishing, mentorship, and diversity and inclusion.
Submissions are due Friday, April 2, 2021.
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ETHICS (92%); CONFERENCES & CONVENTIONS (90%); EMERGING TECHNOLOGY (90%); EXPERIMENTATION & RESEARCH (90%); SCIENCE & TECHNOLOGY (90%); SCIENCE FUNDING (90%); STEM EDUCATION (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (89%); CASE STUDIES (79%); CITIZEN SCIENCE (79%); ENGINEERING (79%); SCIENCE POLICY (79%); DATA MINING (78%); EDUCATION & TRAINING (78%); INTELLECTUAL PROPERTY (78%); LABOR & EMPLOYMENT (78%); MENTORS & ROLE MODELS (78%); MATH & SCIENCE EDUCATION (76%); MEDICAL RESEARCH (74%); BEST PRACTICES (73%); DIVERSITY & INCLUSION (72%); GENETIC ENGINEERING (70%); PRODUCT INNOVATION (67%); ARTIFICIAL INTELLIGENCE (66%); ROBOTICS (66%); GENE EDITING (51%); MEDICINE & HEALTH (50%); UNCONSCIOUS BIAS (50%)
Organization: SIGMA XI THE SCIENTIFIC RESEARCH SOCIETY (84%)
Industry: ENGINEERING (79%); DATA MINING (78%); INFORMATION MANAGEMENT & TECHNOLOGY (78%); INDUSTRIAL AUTOMATION (75%); ARTIFICIAL INTELLIGENCE (66%); ROBOTICS (66%)
Geographic: RESEARCH TRIANGLE, NC, USA (79%); NEW YORK, USA (79%); NORTH CAROLINA, USA (79%)
Load-Date: February 18, 2021","The following information was released by Sigma Xi, the Scientific Research Society:
About the Conference
The 2021 conference theme is Roots to Fruits: Responsible Research for a Flourishing Humanity, How scientific virtues serve society. The conference will take place November 4 7, 2021 at The Conference and Event Center in Niagara Falls, New York. The conference is an international forum for researchers, ethicists, educators, and science communicators to examine what it means to conduct ethical, responsible research across science and engineering disciplines. Sessions will be organized under the following tracks.
Responsible Research and Discovery: Sessions and case studies on the broader societal impact of scientific discoveries and emerging issues in scientific integrity from different perspectives, including researchers, policymakers, and funding agencies.
Responsible STEM Education: Sessions that focus on the cultivation of the scientific character virtues and values; examine the role of the scientific community in protecting science education against policies that undermine scientific evidence; and introduce a comprehensive approach to STEM education that addresses equity and inclusion and promotes excellence in research.
Responsible Technology Innovation: Sessions that examine how to build values into the design process, including emerging ethical challenges associated with new technologies such as gene editing, artificial intelligence, robotics, data mining and privacy, and facial recognition.
General Research Ethics: Sessions on the broader ethical challenges facing the research community such as science and human rights, implicit biases, making the case for basic research, ethic of transitional medical research, authorship, intellectual property, and environmental ethics.
Science Communication, Education, and Public Engagement: Sessions on the science and best practices of science communication, social responsibilities of researchers in engaging with the public and policy makers, ethical considerations in Citizen Science, and integrating ethics training in STEM education.
Research Enterprise and Professional Development: Sessions on pursuing both academic and non-academic STEM careers, cultivating effective interdisciplinary collaborations, leadership training, science policy, publishing, mentorship, and diversity and inclusion.
Submissions are due Friday, April 2, 2021.
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ETHICS (92%); CONFERENCES & CONVENTIONS (90%); EMERGING TECHNOLOGY (90%); EXPERIMENTATION & RESEARCH (90%); SCIENCE & TECHNOLOGY (90%); SCIENCE FUNDING (90%); STEM EDUCATION (90%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (89%); CASE STUDIES (79%); CITIZEN SCIENCE (79%); ENGINEERING (79%); SCIENCE POLICY (79%); DATA MINING (78%); EDUCATION & TRAINING (78%); INTELLECTUAL PROPERTY (78%); LABOR & EMPLOYMENT (78%); MENTORS & ROLE MODELS (78%); MATH & SCIENCE EDUCATION (76%); MEDICAL RESEARCH (74%); BEST PRACTICES (73%); DIVERSITY & INCLUSION (72%); GENETIC ENGINEERING (70%); PRODUCT INNOVATION (67%); ARTIFICIAL INTELLIGENCE (66%); ROBOTICS (66%); GENE EDITING (51%); MEDICINE & HEALTH (50%); UNCONSCIOUS BIAS (50%)
Organization: SIGMA XI THE SCIENTIFIC RESEARCH SOCIETY (84%)
Industry: ENGINEERING (79%); DATA MINING (78%); INFORMATION MANAGEMENT & TECHNOLOGY (78%); INDUSTRIAL AUTOMATION (75%); ARTIFICIAL INTELLIGENCE (66%); ROBOTICS (66%)
Geographic: RESEARCH TRIANGLE, NC, USA (79%); NEW YORK, USA (79%); NORTH CAROLINA, USA (79%)
Load-Date: February 18, 2021",neutral,0.8228058815002441,balanced/neutral,"['privacy', 'bias', 'human rights']","['character', 'virtues', 'equity']",['policy'],"['facial recognition', 'robotics', 'data mining']",3,3,1,3
2021,Unknown Title,"Body
The Geospatial Commission publishes findings of independent public dialogue exploring public attitudes about location data.
Independent dialogue is one of the first on location data
85 members of the public gave their thoughts about location data use across four workshops
Findings outlined will inform government guidance on location data ethics to be published next year
The Geospatial Commission today publishes the findings of an independent public dialogue on location data ethics. The project was launched in March and co-funded by the Geospatial Commission and UK Research and Innovations Sciencewise programme.
The dialogue is one of the UKs first on location data and was delivered by public engagement specialists Traverse and researchers from the Ada Lovelace Institute. Todays report provides evidence on public perceptions about location data use, offering valuable insights into what citizens believe are the key benefits and concerns.
The report was launched at a virtual event hosted by The Alan Turing Institute, the national institute for data science and artificial intelligence. Cosmina Dorobantu, Deputy Programme Director for Public Policy at The Alan Turing Institute, delivered the keynote speech at the event which also featured a presentation by Traverse and Ada Lovelace, and a panel discussion with leading data experts.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Journal Code: 812
Subject: ETHICS (90%); DATA SCIENCE (78%); PUBLIC POLICY (77%); RESEARCH INSTITUTES (75%); ARTIFICIAL INTELLIGENCE (68%); TRENDS & EVENTS (68%)
Industry: DATA SCIENCE (78%); ARTIFICIAL INTELLIGENCE (68%)
Geographic: UNITED KINGDOM (88%)
Load-Date: December 19, 2021","The Geospatial Commission publishes findings of independent public dialogue exploring public attitudes about location data.
Independent dialogue is one of the first on location data
85 members of the public gave their thoughts about location data use across four workshops
Findings outlined will inform government guidance on location data ethics to be published next year
The Geospatial Commission today publishes the findings of an independent public dialogue on location data ethics. The project was launched in March and co-funded by the Geospatial Commission and UK Research and Innovations Sciencewise programme.
The dialogue is one of the UKs first on location data and was delivered by public engagement specialists Traverse and researchers from the Ada Lovelace Institute. Todays report provides evidence on public perceptions about location data use, offering valuable insights into what citizens believe are the key benefits and concerns.
The report was launched at a virtual event hosted by The Alan Turing Institute, the national institute for data science and artificial intelligence. Cosmina Dorobantu, Deputy Programme Director for Public Policy at The Alan Turing Institute, delivered the keynote speech at the event which also featured a presentation by Traverse and Ada Lovelace, and a panel discussion with leading data experts.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Journal Code: 812
Subject: ETHICS (90%); DATA SCIENCE (78%); PUBLIC POLICY (77%); RESEARCH INSTITUTES (75%); ARTIFICIAL INTELLIGENCE (68%); TRENDS & EVENTS (68%)
Industry: DATA SCIENCE (78%); ARTIFICIAL INTELLIGENCE (68%)
Geographic: UNITED KINGDOM (88%)
Load-Date: December 19, 2021",neutral,0.8692770004272461,balanced/neutral,[],[],['policy'],[],0,0,1,0
2021,Unknown Title,"Byline: Targeted News Service
Dateline: BERKELEY, California 
Body
(TNSRes) -- The University of California Berkeley campus issued the following news release:
Every day, algorithms select which news stories appear in our social media feeds. Airplanes allow global travel at nearly the speed of sound while emitting greenhouse gases that accelerate the impacts of climate change. And recent advances in DNA sequencing and editing enable us to understand our fundamental genetic programming -- and potentially change it.
While it may be challenging to anticipate where science might lead us next, researchers at the University of California, Berkeley, are taking steps to ensure that the public has a greater say in future scientific advances, and that questions of ethics and social equity take a prominent role in scientific decision-making.
UC Berkeley announced today that the campus will be home to a new Kavli Center for Ethics, Science, and the Public, which, alongside a second center at the University of Cambridge in the United Kingdom, will connect scientists, ethicists, social scientists, science communicators and the public in necessary and intentional discussions about the potential impacts of scientific discoveries.
""In addition to answering fundamental questions about the ethics of science, the Kavli Center is going to create a generation of scientific leaders who have seen how other scientific disciplines grapple with ethical problems and who have real training in the philosophical analysis of these questions,"" said UC Berkeley computer science professor Stuart Russell, who will direct the new center. ""It's not just about changing public policy, it's about changing what it means to be a good scientist in every discipline that can have an impact on the public.""
The Kavli Center will be led by a core group of prominent scientists, ethicists, philosophers and journalists on campus, each with extensive experience confronting ethical issues in their respective fields. In addition to Russell, these leaders include Nobel laureate Saul Perlmutter, who provided some of the first evidence that the expansion of the universe is accelerating; Nobel and Kavli Prize laureate Jennifer Doudna, known for her discovery of the gene-editing tool CRISPR; theoretical and moral philosopher Jay Wallace; bioethicist Jodi Halpern; neuroscientist Jack Gallant; and historian and writer Elena Conis.
By recruiting and mentoring a diverse group of graduate students and postdoctoral scholars and fellows, the team hopes to equip the next generation of scientists with tools to engage deeply with the ethical and societal implications of their research, and to create new and innovative ways to involve the public in the scientific process.
While the center will initially harness the campus's leadership in three scientific fields -- gene editing, neuroscience and artificial intelligence (AI) -- to develop new ethical frameworks and public engagement tools, it will eventually expand its scope to include other scientific disciplines that impact society.
""I am delighted that UC Berkeley will be the home of the new Kavli Center. As a public research university that prides itself on the cutting-edge endeavors of our world class researchers, we have a special responsibility to help scientists navigate the sometimes complex ethical implications of their work and strengthen engagement with a diverse public,"" said Chancellor Carol Christ.
To achieve its goals, the Kavli Center will operate in a hub, spoke and axle structure. The hub will form the core of the center, where experts from various disciplines come together with philosophers and social scientists to discuss fundamental ethical questions posed by scientific discovery. Each spoke will link these activities to specific scientific disciplines, while the axle will enlist experts in subjects such as law, business, public policy and journalism to help engage the policymakers and the public in discussions about the potential impacts of the science.
""We're embarking on a democratization of the way we think, collaborate and communicate about scientific discoveries and their ethical aspects -- and ensuring the public is included,"" said The Kavli Foundation President Cynthia Friend. ""It's long past due for this to happen.""
A legacy of ethics and public engagement on campus
For years, scholars from all corners of UC Berkeley have grappled with the ethical and societal quandaries posed by the campus's research. In response to rapid advances in CRISPR gene-editing technology, Doudna co-organized international summits on the ethical and policy issues surrounding human genome editing, while Halpern co-founded the Berkeley Ethics and Regulation Group for Innovative Technology (BERGIT) to give scientists at the Innovative Genomics Institute (IGI), the research institute founded by Doudna to use genome engineering to solve humanity's greatest problems in health, climate and sustainable agriculture, a forum to discuss ethical questions raised by their work.
In addition, Russell founded the Center for Human-Compatible Artificial Intelligence in hopes of steering AI technologies toward applications that benefit humanity, Perlmutter is working to incorporate ethical and societal considerations into data science research as the director of the Berkeley Institute for Data Science (BIDS), and Gallant is wrestling with the ramifications of brain activity measurements that could one day allow neuroscientists to reconstruct thoughts and images in another person's mind.
The Kavli Center will serve as an opportunity to elevate these and other efforts on campus, while also ensuring that future scientists have a firm grounding in both ethics and public engagement, two areas where scientific training is often lacking.
""Scientists know that what they're doing in genome editing, neuroscience and AI is going to have a huge impact on the world, but they don't have the background or skills to think about what that could be and how that might feed back into their science,"" Halpern said. ""We'd like to see budding scientists who think about ethics and society rewarded for this work, so that it becomes part of their natural way of thinking. We already have little hubs at Berkeley where this is happening, but the center will allow us to crystallize these efforts into something much bigger.""
For example, Halpern said, while many scientists may take a utilitarian approach to evaluating the impact of their research -- assuming that the best outcome will be one that maximizes the benefits to the most members of society -- ethicists and philosophers know that this approach can sometimes run counter to respecting individual human rights and justice. Halpern also said she hopes that, with input from social scientists and communicators, scientists and the public alike learn to recognize the ways that biases and emotions impact their beliefs.
In addition to grappling with ethical questions, UC Berkeley researchers have a long history of pioneering new and innovative ways of engaging the public in scientific research. The UC Berkeley School of Public Health, for example, is known for creating community-based participatory research, a model for engaging individuals in public health research that impacts their community.
At the center, the lead researchers hope to experiment with other ways of involving the public in scientific decision-making that have been developed around the world in recent decades, such as deliberative polling. This technique, which Perlmutter teaches in his Big Ideas course Sense and Sensibility and Science, brings together a large random sample of the population to deliberate on a policy question, with a panel of subject matter experts available to them to answer the many questions that arise. Perlmutter pointed out how such a successful deliberative polling-based public process in the late 1990s helped steer the Texas utilities toward developing more renewable energy sources in the state..
""What's interesting about this process is that it brings a truly representative sample of the population into a deliberation together, so that it's not just the loudest voices who get heard,"" Perlmutter said. ""One reason that I am very optimistic about this is because you find that, in these structured approaches, people are open to changing their minds, and they don't seem to change their minds based on who is the most charismatic speaker, but based on the facts that they have learned -- an important win for science-informed policy decisions.""
Deliberative polling is just one example of a process that could improve public engagement in science, Perlmutter said. He and the rest of the center leadership team hope that the students, postdoctoral scholars and fellows at the center embrace an attitude of humility, experimentation and open-mindedness when it comes to ethics and public engagement in science, and take this attitude with them as they go on to other universities and research centers.
""What I'm hoping is that we see articulations of how to bring these threads of ethics and engagement together successfully, and that the grad students and postdocs at the core of the center become a resource for the whole world,"" Perlmutter said. ""Public engagement and decision-making isn't a solved problem and probably never will be. But I think if we were even just slightly better at making decisions in a cooperative way, and in a way that accounts for the factual realities of the world taken together with everybody's different fears and goals and values, we would be in a much better position to handle some of the most intractable problems facing the world today.""
MSTRUCK-7689571 MSTRUCK
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ETHICS (93%); COLLEGES & UNIVERSITIES (90%); SCIENCE & TECHNOLOGY (90%); GENE EDITING (89%); GENETIC ENGINEERING (89%); HUMANITIES & SOCIAL SCIENCE (89%); NEWS REPORTING (89%); COLLEGE STUDENTS (79%); ARTIFICIAL INTELLIGENCE (78%); BIOINFORMATICS (78%); COLLEGE & UNIVERSITY PROFESSORS (78%); COMPUTER SCIENCE (78%); GRADUATE & PROFESSIONAL SCHOOLS (78%); JOURNALISM (78%); PHILOSOPHY (78%); RESEARCH INSTITUTES (78%); STUDENTS & STUDENT LIFE (78%); WRITERS (78%); BIOETHICS (77%); BIOTECHNOLOGY & GENETIC SCIENCE (77%); CLIMATE CHANGE (77%); DNA (77%); MENTORS & ROLE MODELS (77%); NEUROSCIENCE (77%); SOCIAL JUSTICE (77%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (77%); GENES & CHROMOSOMES (76%); GENETIC ANALYTIC TECHNIQUES (76%); GENOMICS (76%); GREENHOUSE GASES (72%); NOBEL PRIZES (72%); SOCIAL MEDIA (72%); HISTORY (70%)
Organization: UNIVERSITY OF CALIFORNIA BERKELEY (94%)
Industry: COLLEGES & UNIVERSITIES (90%); NEWS REPORTING (89%); COLLEGE STUDENTS (79%); ARTIFICIAL INTELLIGENCE (78%); COLLEGE & UNIVERSITY PROFESSORS (78%); COMPUTER SCIENCE (78%); GRADUATE & PROFESSIONAL SCHOOLS (78%); WRITERS (78%); GENETIC ANALYTIC TECHNIQUES (76%); SOCIAL MEDIA (72%)
Geographic: SAN FRANCISCO BAY AREA, CA, USA (96%); CALIFORNIA, USA (92%); UNITED KINGDOM (56%)
Load-Date: December 9, 2021","(TNSRes) -- The University of California Berkeley campus issued the following news release:
Every day, algorithms select which news stories appear in our social media feeds. Airplanes allow global travel at nearly the speed of sound while emitting greenhouse gases that accelerate the impacts of climate change. And recent advances in DNA sequencing and editing enable us to understand our fundamental genetic programming -- and potentially change it.
While it may be challenging to anticipate where science might lead us next, researchers at the University of California, Berkeley, are taking steps to ensure that the public has a greater say in future scientific advances, and that questions of ethics and social equity take a prominent role in scientific decision-making.
UC Berkeley announced today that the campus will be home to a new Kavli Center for Ethics, Science, and the Public, which, alongside a second center at the University of Cambridge in the United Kingdom, will connect scientists, ethicists, social scientists, science communicators and the public in necessary and intentional discussions about the potential impacts of scientific discoveries.
""In addition to answering fundamental questions about the ethics of science, the Kavli Center is going to create a generation of scientific leaders who have seen how other scientific disciplines grapple with ethical problems and who have real training in the philosophical analysis of these questions,"" said UC Berkeley computer science professor Stuart Russell, who will direct the new center. ""It's not just about changing public policy, it's about changing what it means to be a good scientist in every discipline that can have an impact on the public.""
The Kavli Center will be led by a core group of prominent scientists, ethicists, philosophers and journalists on campus, each with extensive experience confronting ethical issues in their respective fields. In addition to Russell, these leaders include Nobel laureate Saul Perlmutter, who provided some of the first evidence that the expansion of the universe is accelerating; Nobel and Kavli Prize laureate Jennifer Doudna, known for her discovery of the gene-editing tool CRISPR; theoretical and moral philosopher Jay Wallace; bioethicist Jodi Halpern; neuroscientist Jack Gallant; and historian and writer Elena Conis.
By recruiting and mentoring a diverse group of graduate students and postdoctoral scholars and fellows, the team hopes to equip the next generation of scientists with tools to engage deeply with the ethical and societal implications of their research, and to create new and innovative ways to involve the public in the scientific process.
While the center will initially harness the campus's leadership in three scientific fields -- gene editing, neuroscience and artificial intelligence (AI) -- to develop new ethical frameworks and public engagement tools, it will eventually expand its scope to include other scientific disciplines that impact society.
""I am delighted that UC Berkeley will be the home of the new Kavli Center. As a public research university that prides itself on the cutting-edge endeavors of our world class researchers, we have a special responsibility to help scientists navigate the sometimes complex ethical implications of their work and strengthen engagement with a diverse public,"" said Chancellor Carol Christ.
To achieve its goals, the Kavli Center will operate in a hub, spoke and axle structure. The hub will form the core of the center, where experts from various disciplines come together with philosophers and social scientists to discuss fundamental ethical questions posed by scientific discovery. Each spoke will link these activities to specific scientific disciplines, while the axle will enlist experts in subjects such as law, business, public policy and journalism to help engage the policymakers and the public in discussions about the potential impacts of the science.
""We're embarking on a democratization of the way we think, collaborate and communicate about scientific discoveries and their ethical aspects -- and ensuring the public is included,"" said The Kavli Foundation President Cynthia Friend. ""It's long past due for this to happen.""
A legacy of ethics and public engagement on campus
For years, scholars from all corners of UC Berkeley have grappled with the ethical and societal quandaries posed by the campus's research. In response to rapid advances in CRISPR gene-editing technology, Doudna co-organized international summits on the ethical and policy issues surrounding human genome editing, while Halpern co-founded the Berkeley Ethics and Regulation Group for Innovative Technology (BERGIT) to give scientists at the Innovative Genomics Institute (IGI), the research institute founded by Doudna to use genome engineering to solve humanity's greatest problems in health, climate and sustainable agriculture, a forum to discuss ethical questions raised by their work.
In addition, Russell founded the Center for Human-Compatible Artificial Intelligence in hopes of steering AI technologies toward applications that benefit humanity, Perlmutter is working to incorporate ethical and societal considerations into data science research as the director of the Berkeley Institute for Data Science (BIDS), and Gallant is wrestling with the ramifications of brain activity measurements that could one day allow neuroscientists to reconstruct thoughts and images in another person's mind.
The Kavli Center will serve as an opportunity to elevate these and other efforts on campus, while also ensuring that future scientists have a firm grounding in both ethics and public engagement, two areas where scientific training is often lacking.
""Scientists know that what they're doing in genome editing, neuroscience and AI is going to have a huge impact on the world, but they don't have the background or skills to think about what that could be and how that might feed back into their science,"" Halpern said. ""We'd like to see budding scientists who think about ethics and society rewarded for this work, so that it becomes part of their natural way of thinking. We already have little hubs at Berkeley where this is happening, but the center will allow us to crystallize these efforts into something much bigger.""
For example, Halpern said, while many scientists may take a utilitarian approach to evaluating the impact of their research -- assuming that the best outcome will be one that maximizes the benefits to the most members of society -- ethicists and philosophers know that this approach can sometimes run counter to respecting individual human rights and justice. Halpern also said she hopes that, with input from social scientists and communicators, scientists and the public alike learn to recognize the ways that biases and emotions impact their beliefs.
In addition to grappling with ethical questions, UC Berkeley researchers have a long history of pioneering new and innovative ways of engaging the public in scientific research. The UC Berkeley School of Public Health, for example, is known for creating community-based participatory research, a model for engaging individuals in public health research that impacts their community.
At the center, the lead researchers hope to experiment with other ways of involving the public in scientific decision-making that have been developed around the world in recent decades, such as deliberative polling. This technique, which Perlmutter teaches in his Big Ideas course Sense and Sensibility and Science, brings together a large random sample of the population to deliberate on a policy question, with a panel of subject matter experts available to them to answer the many questions that arise. Perlmutter pointed out how such a successful deliberative polling-based public process in the late 1990s helped steer the Texas utilities toward developing more renewable energy sources in the state..
""What's interesting about this process is that it brings a truly representative sample of the population into a deliberation together, so that it's not just the loudest voices who get heard,"" Perlmutter said. ""One reason that I am very optimistic about this is because you find that, in these structured approaches, people are open to changing their minds, and they don't seem to change their minds based on who is the most charismatic speaker, but based on the facts that they have learned -- an important win for science-informed policy decisions.""
Deliberative polling is just one example of a process that could improve public engagement in science, Perlmutter said. He and the rest of the center leadership team hope that the students, postdoctoral scholars and fellows at the center embrace an attitude of humility, experimentation and open-mindedness when it comes to ethics and public engagement in science, and take this attitude with them as they go on to other universities and research centers.
""What I'm hoping is that we see articulations of how to bring these threads of ethics and engagement together successfully, and that the grad students and postdocs at the core of the center become a resource for the whole world,"" Perlmutter said. ""Public engagement and decision-making isn't a solved problem and probably never will be. But I think if we were even just slightly better at making decisions in a cooperative way, and in a way that accounts for the factual realities of the world taken together with everybody's different fears and goals and values, we would be in a much better position to handle some of the most intractable problems facing the world today.""
MSTRUCK-7689571 MSTRUCK
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: ETHICS (93%); COLLEGES & UNIVERSITIES (90%); SCIENCE & TECHNOLOGY (90%); GENE EDITING (89%); GENETIC ENGINEERING (89%); HUMANITIES & SOCIAL SCIENCE (89%); NEWS REPORTING (89%); COLLEGE STUDENTS (79%); ARTIFICIAL INTELLIGENCE (78%); BIOINFORMATICS (78%); COLLEGE & UNIVERSITY PROFESSORS (78%); COMPUTER SCIENCE (78%); GRADUATE & PROFESSIONAL SCHOOLS (78%); JOURNALISM (78%); PHILOSOPHY (78%); RESEARCH INSTITUTES (78%); STUDENTS & STUDENT LIFE (78%); WRITERS (78%); BIOETHICS (77%); BIOTECHNOLOGY & GENETIC SCIENCE (77%); CLIMATE CHANGE (77%); DNA (77%); MENTORS & ROLE MODELS (77%); NEUROSCIENCE (77%); SOCIAL JUSTICE (77%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (77%); GENES & CHROMOSOMES (76%); GENETIC ANALYTIC TECHNIQUES (76%); GENOMICS (76%); GREENHOUSE GASES (72%); NOBEL PRIZES (72%); SOCIAL MEDIA (72%); HISTORY (70%)
Organization: UNIVERSITY OF CALIFORNIA BERKELEY (94%)
Industry: COLLEGES & UNIVERSITIES (90%); NEWS REPORTING (89%); COLLEGE STUDENTS (79%); ARTIFICIAL INTELLIGENCE (78%); COLLEGE & UNIVERSITY PROFESSORS (78%); COMPUTER SCIENCE (78%); GRADUATE & PROFESSIONAL SCHOOLS (78%); WRITERS (78%); GENETIC ANALYTIC TECHNIQUES (76%); SOCIAL MEDIA (72%)
Geographic: SAN FRANCISCO BAY AREA, CA, USA (96%); CALIFORNIA, USA (92%); UNITED KINGDOM (56%)
Load-Date: December 9, 2021",neutral,0.8525068759918213,balanced/neutral,['human rights'],"['utilitarian', 'justice', 'equity', 'justice']","['regulation', 'policy', 'law']",[],1,4,3,0
2021,Unknown Title,"Byline: Alexandre Wadih Raffoul, PhD Candidate, Department of Peace and Conflict Studies, Uppsala University
Highlight: Research ethics focus on avoiding wrongdoing, having been developed largely in response to biomedical scandals. Climate change puts the onus on researchers to add 'do good' to 'do no harm' principles.
Body
Young people across the world have taken to the streets again, demanding decision-makers at COP26 listen to the science. But if science is to live up to these expectations, a fundamental rethinking of research ethics in light of the climate and ecological crises is needed.
The ongoing planetary crises create new ethical dilemmas for researchers. The three main principles of research ethics - do no harm, integrity, and responsibility - remain relevant to avoid wrongdoing. But these were formulated reactively, in response to scandals in biomedical research, and could not anticipate these new challenges.
We are proposing a move from a negative ethics focused on avoiding harm to a positive research ethics. These new ethics are needed to guide the global scientific community in relation to civil society and politics during the climate and ecological crises.
Read more: Why we need engineers who study ethics as much as maths
Do no harm
According to the ""do no harm"" imperative, researchers have a responsibility to avoid hurting humans or animals directly involved in their research. But what does ""do no harm"" mean in the midst of climate and ecological crises?
A growing group of scientists question the carbon footprint of academic activities, ranging from flying to conferences to developing artificial intelligence. The long-term and unpredictable consequences of research have also come back into focus. An example is the debate about the high risks of geoengineering.
Read more: A global carbon removal industry is coming - experts explain the problems it must overcome
The ""do no harm"" principle should thus be broadened in two ways:
it should include humans, animals and ecosystems that are traditionally not considered part of the research process, but can be negatively affected by it
it should better account for the long-term, indirect or unintended consequences of research projects or new technologies.
But if averting the climate crisis requires the complete transformation of society within ten years, is it enough for research to ""do no harm""? Inspired by post-colonial approaches to research ethics, we suggest moving beyond this negative principle and towards a positive, regenerative science.
This science would actively contribute to the project of regenerating society and ecosystems. It would be motivated by an analysis of the suffering already taking place and acknowledge historical responsibilities and power relations. 
Read more: Four reasons why restoring nature is the most important endeavour of our time
Act with integrity
The principle of integrity asks researchers to follow rigorous protocols, disclose conflicts of interest, refrain from manipulating data, and abstain from plagiarism. But can science be rigorous if it overlooks environmental variables?
Some disciplines ignore the predictions of IPCC reports, as well as indications of mass extinction and ecosystem collapse. They also struggle to reflect the complex and delicate interconnection between humanity and nature in their practical recommendations.
For example, by focusing heavily on GDP growth, mainstream economics portrays our planetary habitat mostly as a resource to use or exploit. The idea of geoengineering also largely rests on an understanding of our life-support systems as a set of disconnected pieces that can be engineered.
Ultimately, ""integrity means wholeness"". It implies acknowledging that we are parts of a fragile and interconnected web of life, which we need to preserve. 
Researchers should thus account for ecological dimensions in their analyses. They should also interrogate the conception of the humanity-nature relationship that implicitly underpins their work.
Take responsibility
According to the ""responsibility"" principle, research should be relevant to society and communicated to the public. But in a climate crisis, findings can be so dramatic, their implications for society so huge and controversial, that the word ""responsibility"" takes a new, heavier meaning.
In this context, some scientists do not dare to speak out, fearing to appear biased. As a result, they fail to influence the public debate.
Others are tempted to adjust their research to political demands. An example is the inclusion of unrealistic amounts of ""negative carbon emissions"" in climate models to satisfy policymakers. This was criticised for unintentionally providing a scientific cover-up for climate inaction.
Read more: Climate scientists: concept of net zero is a dangerous trap
Yet other researchers suggest that focusing mainly on technological innovation can resolve the ecological crises. It's a discourse that delays action by decreasing the sense of emergency in tackling these crises.
The ""responsibility"" principle should therefore be enriched in three ways:
scientists should take their own findings seriously and stand up for their societal implications, even when it is uncomfortable to do so
researchers should defend the scientific process itself from the influence of political and economic interests
scientists should remain humble as to what science can achieve. This means acknowledging the limits to our knowledge of an infinitely complex world, as well as the slow pace and unpredictable consequences of technological development.
From words to deeds
The research ethics sketched here need to be further developed. They can then be incorporated into global guidelines for individual researchers, but also for governments, universities and funding agencies.
Academic research will be at the heart of any solution to the climate and ecological crises. Embracing this responsibility and facing these existential threats requires much more from universities than the adoption of sustainability plans.
Read more: Climate change is the most important mission for universities of the 21st century
Emma Elfversson receives funding from the Swedish Research Council and the Swedish Research Council for Sustainable Development (Formas). She is a member of the Swedish Green party (Miljöpartiet).
Helen Avery receives funding from the Swedish Research Council for Sustainable Development (Formas).
Alexandre Wadih Raffoul, David Fopp, and Ryan Carolan do not work for, consult, own shares in or receive funding from any company or organisation that would benefit from this article, and have disclosed no relevant affiliations beyond their academic appointment.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (97%); CLIMATE CHANGE (91%); BIOMEDICINE (90%); CLIMATOLOGY (90%); ECOLOGY & ENVIRONMENTAL SCIENCE (90%); NEGATIVE ENVIRONMENTAL NEWS (90%); NEGATIVE MISC NEWS (90%); NEGATIVE PERSONAL NEWS (90%); SCANDALS (90%); SCIENCE & TECHNOLOGY (90%); NEGATIVE NEWS (89%); ECONOMICS (79%); RESEARCH & DEVELOPMENT (79%); BIOETHICS (78%); SCHOOL CHEATING (78%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (78%); ENVIRONMENTAL FOOTPRINT (77%); MEDICAL RESEARCH (74%); ARTIFICIAL INTELLIGENCE (73%); ECONOMIC GROWTH (71%); GROSS DOMESTIC PRODUCT (61%); PLAGIARISM (61%)
Industry: BIOMEDICINE (90%); ARTIFICIAL INTELLIGENCE (73%); MEDIA & TELECOMMUNICATIONS (73%)
Load-Date: November 4, 2021","Young people across the world have taken to the streets again, demanding decision-makers at COP26 listen to the science. But if science is to live up to these expectations, a fundamental rethinking of research ethics in light of the climate and ecological crises is needed.
The ongoing planetary crises create new ethical dilemmas for researchers. The three main principles of research ethics - do no harm, integrity, and responsibility - remain relevant to avoid wrongdoing. But these were formulated reactively, in response to scandals in biomedical research, and could not anticipate these new challenges.
We are proposing a move from a negative ethics focused on avoiding harm to a positive research ethics. These new ethics are needed to guide the global scientific community in relation to civil society and politics during the climate and ecological crises.
Read more: Why we need engineers who study ethics as much as maths
Do no harm
According to the ""do no harm"" imperative, researchers have a responsibility to avoid hurting humans or animals directly involved in their research. But what does ""do no harm"" mean in the midst of climate and ecological crises?
A growing group of scientists question the carbon footprint of academic activities, ranging from flying to conferences to developing artificial intelligence. The long-term and unpredictable consequences of research have also come back into focus. An example is the debate about the high risks of geoengineering.
Read more: A global carbon removal industry is coming - experts explain the problems it must overcome
The ""do no harm"" principle should thus be broadened in two ways:
it should include humans, animals and ecosystems that are traditionally not considered part of the research process, but can be negatively affected by it
it should better account for the long-term, indirect or unintended consequences of research projects or new technologies.
But if averting the climate crisis requires the complete transformation of society within ten years, is it enough for research to ""do no harm""? Inspired by post-colonial approaches to research ethics, we suggest moving beyond this negative principle and towards a positive, regenerative science.
This science would actively contribute to the project of regenerating society and ecosystems. It would be motivated by an analysis of the suffering already taking place and acknowledge historical responsibilities and power relations. 
Read more: Four reasons why restoring nature is the most important endeavour of our time
Act with integrity
The principle of integrity asks researchers to follow rigorous protocols, disclose conflicts of interest, refrain from manipulating data, and abstain from plagiarism. But can science be rigorous if it overlooks environmental variables?
Some disciplines ignore the predictions of IPCC reports, as well as indications of mass extinction and ecosystem collapse. They also struggle to reflect the complex and delicate interconnection between humanity and nature in their practical recommendations.
For example, by focusing heavily on GDP growth, mainstream economics portrays our planetary habitat mostly as a resource to use or exploit. The idea of geoengineering also largely rests on an understanding of our life-support systems as a set of disconnected pieces that can be engineered.
Ultimately, ""integrity means wholeness"". It implies acknowledging that we are parts of a fragile and interconnected web of life, which we need to preserve. 
Researchers should thus account for ecological dimensions in their analyses. They should also interrogate the conception of the humanity-nature relationship that implicitly underpins their work.
Take responsibility
According to the ""responsibility"" principle, research should be relevant to society and communicated to the public. But in a climate crisis, findings can be so dramatic, their implications for society so huge and controversial, that the word ""responsibility"" takes a new, heavier meaning.
In this context, some scientists do not dare to speak out, fearing to appear biased. As a result, they fail to influence the public debate.
Others are tempted to adjust their research to political demands. An example is the inclusion of unrealistic amounts of ""negative carbon emissions"" in climate models to satisfy policymakers. This was criticised for unintentionally providing a scientific cover-up for climate inaction.
Read more: Climate scientists: concept of net zero is a dangerous trap
Yet other researchers suggest that focusing mainly on technological innovation can resolve the ecological crises. It's a discourse that delays action by decreasing the sense of emergency in tackling these crises.
The ""responsibility"" principle should therefore be enriched in three ways:
scientists should take their own findings seriously and stand up for their societal implications, even when it is uncomfortable to do so
researchers should defend the scientific process itself from the influence of political and economic interests
scientists should remain humble as to what science can achieve. This means acknowledging the limits to our knowledge of an infinitely complex world, as well as the slow pace and unpredictable consequences of technological development.
From words to deeds
The research ethics sketched here need to be further developed. They can then be incorporated into global guidelines for individual researchers, but also for governments, universities and funding agencies.
Academic research will be at the heart of any solution to the climate and ecological crises. Embracing this responsibility and facing these existential threats requires much more from universities than the adoption of sustainability plans.
Read more: Climate change is the most important mission for universities of the 21st century
Emma Elfversson receives funding from the Swedish Research Council and the Swedish Research Council for Sustainable Development (Formas). She is a member of the Swedish Green party (Miljöpartiet).
Helen Avery receives funding from the Swedish Research Council for Sustainable Development (Formas).
Alexandre Wadih Raffoul, David Fopp, and Ryan Carolan do not work for, consult, own shares in or receive funding from any company or organisation that would benefit from this article, and have disclosed no relevant affiliations beyond their academic appointment.
Classification
Language: ENGLISH
Publication-Type: Web Publication
Subject: ETHICS (97%); CLIMATE CHANGE (91%); BIOMEDICINE (90%); CLIMATOLOGY (90%); ECOLOGY & ENVIRONMENTAL SCIENCE (90%); NEGATIVE ENVIRONMENTAL NEWS (90%); NEGATIVE MISC NEWS (90%); NEGATIVE PERSONAL NEWS (90%); SCANDALS (90%); SCIENCE & TECHNOLOGY (90%); NEGATIVE NEWS (89%); ECONOMICS (79%); RESEARCH & DEVELOPMENT (79%); BIOETHICS (78%); SCHOOL CHEATING (78%); SOCIETY, SOCIAL ASSISTANCE & LIFESTYLE (78%); ENVIRONMENTAL FOOTPRINT (77%); MEDICAL RESEARCH (74%); ARTIFICIAL INTELLIGENCE (73%); ECONOMIC GROWTH (71%); GROSS DOMESTIC PRODUCT (61%); PLAGIARISM (61%)
Industry: BIOMEDICINE (90%); ARTIFICIAL INTELLIGENCE (73%); MEDIA & TELECOMMUNICATIONS (73%)
Load-Date: November 4, 2021",neutral,0.68865966796875,balanced/neutral,[],[],"['guidelines', 'should', 'must', 'need to', 'suggest']",[],0,0,5,0
2021,Unknown Title,"Byline: Murad Moosa Khan
Body
 MANY organisations include 'ethics' in their vision and mission statements and policies without truly understanding what it means to be an ethical organisation. Outside a few countries in the West, the concept of Organisational Ethics is largely unknown. OE is defined as 'everyday practices in which organisational values are demonstrated in organisational structures and behaviours, as well as in day-to-day practices and decision-making processes at all levels of the organisation'.
In daily organisational life, ethical values are not only about doing things right but also doing the right thing. Rules-based organisations emphasise the former. The latter relates to ethical organisations. Often ethics is restricted to legal codes and regulatory compliances. But OE is beyond these. It is about having an ethical perspective to every action and decision that takes place in an organisation. Amongst others, this includes conflict-of-interest issues, appointments and promotions, appraisal systems, institutional policies, compensation and benefits, moral distress in employees, resource allocation, as well as the organisation's business model. The last is particularly important, as it is not only about making profits but how profits are made.
Prioritising ethics is the primary duty of leaders.
Historically, organisations, especially the manufacturing and service industry, placed great emphasis on quality, starting as early as the 1920s. This gained momentum in the 1950s post Second World War as the demand for cheap goods rose. Quality control developed quickly and became a main theme, initially of the Japanese style of management and later in other countries. Over time, the idea of improving not only the quality of products but also every aspect of organisations' functions became popular. This was the start of total quality management, which encompassed concepts such as customer focus, the involvement of employees, continuous quality improvement and the integration of quality management into the whole organisation.
While the quality movement improved the efficiency at production lines and quality of products, it did little to address corruption in industry, highlighted by several highly publicised scandals in some of the largest global corporations such as Enron, WorldCom, Arthur Andersen and Big Pharma amongst others. Part of the reason was because in their desire to improve quality and increase profits, few organisations paid attention to their organisational culture. As long as profits were being made no one was concerned about culture, not realising that the ethical culture of an organisation is critical to its performance. Staff commitment and turnover, job satisfaction and morale, as well as stress and burnout are directly linked to it. Few organisations understand that quality is always the by-product of ethics and not vice versa.
An important element of OE is ethical leadership, ie 'the duty of leaders to foster an environment that engages and supports ethical values at all levels of the organisation'. Prioritising ethics is the primary duty of leaders, as integrity always flows from top down. One cannot expect employees to behave ethically if they see the leadership not adhering to organisational values. An ethics-driven decision made by a leader has ramifications that can have a lasting impact on the organisation.
Similarly, many organisations have policies that are vetted from the legal, financial and administrative aspects but rarely from the ethical aspect. This leads to ethical principles being frequently compromised. Examples include inequitable distribution of resources, favouritism and nepotism in appointments and promotions, harassment and abuse of employees amongst others. Carried out frequently and repeatedly this leads to a culture of corruption, resulting in erosion of credibility and reputation of the institution.
While the principles of OE apply to all organisations, they have particular relevance for healthcare organisations. HCOs deal with a highly vulnerable population, whose health and life is at risk. The power differential between a healthcare provider and patient has the potential of exploitation of the latter by the former. A strong OE programme can prevent this.
Unfortunately, in many HCOs there is 'ethical bifurcation', where clinicians and researchers are held accountable for ethical standards when dealing with patients but there is no similar requirement for the administrative and policymaking areas. This weakens the overall ethical culture of an HCO.
Today, many HCOs have been transformed into big businesses and terms like 'patients', 'compassion', 'ethics', 'caring', and 'humanism' have been replaced with terms from the business world such as 'customers', 'clients', 'volumes', 'revenues', 'projections', 'targets' and 'incentives'. Such terms devalue the medical profession, making it more like a business than a moral enterprise. Also, in today's highly complex healthcare systems, where AI and algorithms are becoming norms and healthcare is highly commercialised, it is imperative that all systems and processes of an HCO are strongly embedded in ethics and integrity.
In countries like Pakistan with weak governance structures, the problem is further compounded: despite legislation and policies, there is lack of accountability. Leaders may behave unethically and are not answerable to anyone. Conflict of interest, lack of transparency, giving and taking bribes, misappropriation of funds, nepotism and favouritism in appointments are some of the issues that plague many organisations in Pakistan.
Similarly, there is a booming multibillion-dollar private healthcare industry here that is totally unregulated. Without any oversight, exploitation of patients through various means is not uncommon. Almost 90 per cent of healthcare expenditure in Pakistan is out of pocket, which means patients end up paying the high cost of corruption in HCOs.
Willem Landman, head of the Ethics Institute of South Africa, wrote, 'In an organisation, ethics needs to be institutionalised and operationalised, with a view to building an ethical organisational culture. Organisations should actively manage ethics by having an unwavering board and executive commitment to ethics, (re)writing Codes of Ethics, induction training on ethics and ongoing ethics awareness training, Ethics Lines (helplines and hotlines), and effective ethics communication. This can only be done through an Organisational Ethics programme.'
In today's challenging world, all organisations need to invest in ethics through an organisational ethics programme.
Classification
Language: English US
Publication-Type: Newspaper
Subject: ETHICS (93%); ASSOCIATIONS & ORGANIZATIONS (90%); CORPORATE CULTURE (89%); QUALITY CONTROL (89%); NEGATIVE NEWS (88%); CORRUPTION (78%); SCANDALS (78%); COMPANY ACTIVITIES & MANAGEMENT (76%); NEGATIVE MISC NEWS (74%); CORPORATE WRONGDOING (72%); MANAGEMENT THEORY (72%); WORKPLACE MORALE (72%); OUTPUT & DEMAND (70%); APPOINTMENTS (69%); COMPENSATION & BENEFITS (67%); MULTINATIONAL CORPORATIONS (50%); WORLD WAR II (50%)
Company:  VERIZON COMMUNICATIONS INC (52%);  ENRON CREDITORS RECOVERY CORP (52%)
Ticker: VZC (LSE) (52%); VZ (NYSE) (52%)
Industry: NAICS517312 WIRELESS TELECOMMUNICATIONS CARRIERS (EXCEPT SATELLITE) (52%); NAICS517311 WIRED TELECOMMUNICATIONS CARRIERS (52%); SIC4911 ELECTRIC SERVICES (52%); MANUFACTURING (76%)
Geographic: PAKISTAN (93%); Pakistan
Load-Date: March 15, 2021","MANY organisations include 'ethics' in their vision and mission statements and policies without truly understanding what it means to be an ethical organisation. Outside a few countries in the West, the concept of Organisational Ethics is largely unknown. OE is defined as 'everyday practices in which organisational values are demonstrated in organisational structures and behaviours, as well as in day-to-day practices and decision-making processes at all levels of the organisation'.
In daily organisational life, ethical values are not only about doing things right but also doing the right thing. Rules-based organisations emphasise the former. The latter relates to ethical organisations. Often ethics is restricted to legal codes and regulatory compliances. But OE is beyond these. It is about having an ethical perspective to every action and decision that takes place in an organisation. Amongst others, this includes conflict-of-interest issues, appointments and promotions, appraisal systems, institutional policies, compensation and benefits, moral distress in employees, resource allocation, as well as the organisation's business model. The last is particularly important, as it is not only about making profits but how profits are made.
Prioritising ethics is the primary duty of leaders.
Historically, organisations, especially the manufacturing and service industry, placed great emphasis on quality, starting as early as the 1920s. This gained momentum in the 1950s post Second World War as the demand for cheap goods rose. Quality control developed quickly and became a main theme, initially of the Japanese style of management and later in other countries. Over time, the idea of improving not only the quality of products but also every aspect of organisations' functions became popular. This was the start of total quality management, which encompassed concepts such as customer focus, the involvement of employees, continuous quality improvement and the integration of quality management into the whole organisation.
While the quality movement improved the efficiency at production lines and quality of products, it did little to address corruption in industry, highlighted by several highly publicised scandals in some of the largest global corporations such as Enron, WorldCom, Arthur Andersen and Big Pharma amongst others. Part of the reason was because in their desire to improve quality and increase profits, few organisations paid attention to their organisational culture. As long as profits were being made no one was concerned about culture, not realising that the ethical culture of an organisation is critical to its performance. Staff commitment and turnover, job satisfaction and morale, as well as stress and burnout are directly linked to it. Few organisations understand that quality is always the by-product of ethics and not vice versa.
An important element of OE is ethical leadership, ie 'the duty of leaders to foster an environment that engages and supports ethical values at all levels of the organisation'. Prioritising ethics is the primary duty of leaders, as integrity always flows from top down. One cannot expect employees to behave ethically if they see the leadership not adhering to organisational values. An ethics-driven decision made by a leader has ramifications that can have a lasting impact on the organisation.
Similarly, many organisations have policies that are vetted from the legal, financial and administrative aspects but rarely from the ethical aspect. This leads to ethical principles being frequently compromised. Examples include inequitable distribution of resources, favouritism and nepotism in appointments and promotions, harassment and abuse of employees amongst others. Carried out frequently and repeatedly this leads to a culture of corruption, resulting in erosion of credibility and reputation of the institution.
While the principles of OE apply to all organisations, they have particular relevance for healthcare organisations. HCOs deal with a highly vulnerable population, whose health and life is at risk. The power differential between a healthcare provider and patient has the potential of exploitation of the latter by the former. A strong OE programme can prevent this.
Unfortunately, in many HCOs there is 'ethical bifurcation', where clinicians and researchers are held accountable for ethical standards when dealing with patients but there is no similar requirement for the administrative and policymaking areas. This weakens the overall ethical culture of an HCO.
Today, many HCOs have been transformed into big businesses and terms like 'patients', 'compassion', 'ethics', 'caring', and 'humanism' have been replaced with terms from the business world such as 'customers', 'clients', 'volumes', 'revenues', 'projections', 'targets' and 'incentives'. Such terms devalue the medical profession, making it more like a business than a moral enterprise. Also, in today's highly complex healthcare systems, where AI and algorithms are becoming norms and healthcare is highly commercialised, it is imperative that all systems and processes of an HCO are strongly embedded in ethics and integrity.
In countries like Pakistan with weak governance structures, the problem is further compounded: despite legislation and policies, there is lack of accountability. Leaders may behave unethically and are not answerable to anyone. Conflict of interest, lack of transparency, giving and taking bribes, misappropriation of funds, nepotism and favouritism in appointments are some of the issues that plague many organisations in Pakistan.
Similarly, there is a booming multibillion-dollar private healthcare industry here that is totally unregulated. Without any oversight, exploitation of patients through various means is not uncommon. Almost 90 per cent of healthcare expenditure in Pakistan is out of pocket, which means patients end up paying the high cost of corruption in HCOs.
Willem Landman, head of the Ethics Institute of South Africa, wrote, 'In an organisation, ethics needs to be institutionalised and operationalised, with a view to building an ethical organisational culture. Organisations should actively manage ethics by having an unwavering board and executive commitment to ethics, (re)writing Codes of Ethics, induction training on ethics and ongoing ethics awareness training, Ethics Lines (helplines and hotlines), and effective ethics communication. This can only be done through an Organisational Ethics programme.'
In today's challenging world, all organisations need to invest in ethics through an organisational ethics programme.
Classification
Language: English US
Publication-Type: Newspaper
Subject: ETHICS (93%); ASSOCIATIONS & ORGANIZATIONS (90%); CORPORATE CULTURE (89%); QUALITY CONTROL (89%); NEGATIVE NEWS (88%); CORRUPTION (78%); SCANDALS (78%); COMPANY ACTIVITIES & MANAGEMENT (76%); NEGATIVE MISC NEWS (74%); CORPORATE WRONGDOING (72%); MANAGEMENT THEORY (72%); WORKPLACE MORALE (72%); OUTPUT & DEMAND (70%); APPOINTMENTS (69%); COMPENSATION & BENEFITS (67%); MULTINATIONAL CORPORATIONS (50%); WORLD WAR II (50%)
Company:  VERIZON COMMUNICATIONS INC (52%);  ENRON CREDITORS RECOVERY CORP (52%)
Ticker: VZC (LSE) (52%); VZ (NYSE) (52%)
Industry: NAICS517312 WIRELESS TELECOMMUNICATIONS CARRIERS (EXCEPT SATELLITE) (52%); NAICS517311 WIRED TELECOMMUNICATIONS CARRIERS (52%); SIC4911 ELECTRIC SERVICES (52%); MANUFACTURING (76%)
Geographic: PAKISTAN (93%); Pakistan
Load-Date: March 15, 2021",neutral,0.6438564658164978,balanced/neutral,"['transparency', 'accountability']",[],"['governance', 'oversight', 'standards', 'legislation', 'should', 'need to']",[],2,0,6,0
2021,Unknown Title,"Body
Cologny: World Economic Forum has issued the following press release:
A team of researchers asked an AI system if artificial intelligence could ever be ethical. The AI in question, the Megatron Transformer, said that it could never be ethical, owing to the fact that 'it is a tool'. It added that like any tool, it can be used for good or bad, but that it is itself neither, as there are only good or bad humans. In response to another question, the AI said that the ability to provide information will be the defining feature of the economy of the 21st century.
Not a day passes without a fascinating snippet on the ethical challenges created by “black box” artificial intelligence systems. These use machine learning to figure out patterns within data and make decisions – often without a human giving them any moral basis for how to do it.
Classics of the genre are the credit cards accused of awarding bigger loans to men than women, based simply on which gender got the best credit terms in the past. Or the recruitment AIs that discovered the most accurate tool for candidate selection was to find CVs containing the phrase “field hockey” or the first name “Jared”.
More seriously, former Google CEO Eric Schmidt recently combined with Henry Kissinger to publish The Age of AI: And Our Human Future, a book warning of the dangers of machine-learning AI systems so fast that they could react to hypersonic missiles by firing nuclear weapons before any human got into the decision-making process. In fact, autonomous AI-powered weapons systems are already on sale and may in fact have been used.
Somewhere in the machine, ethics are clearly a good idea.AI at Oxford
It ’ s natural, therefore, that we would include the ethics of AI in our postgraduate Diploma in Artificial Intelligence for Business at Oxford ’ s Said Business School. In its first year, we ’ ve done sessions on everything from the AI-driven automated stock trading systems in Singapore, to the limits of facial recognition in US policing.
We recently finished the course with a debate at the celebrated Oxford Union, crucible of great debaters like William Gladstone, Robin Day, Benazir Bhutto, Denis Healey and Tariq Ali. Along with the students, we allowed an actual AI to contribute.
It was the Megatron Transformer, developed by the Applied Deep Research team at computer-chip maker Nvidia, and based on earlier work by Google. Like many supervised learning tools, it is trained on real-world data – in this case, the whole of Wikipedia (in English), 63 million English news articles from 2016-19, 38 gigabytes worth of Reddit discourse (which must be a pretty depressing read), and a huge number of creative commons sources.
In other words, the Megatron is trained on more written material than any of us could reasonably expect to digest in a lifetime. After such extensive research, it forms its own views.
The debate topic was: “This house believes that AI will never be ethical. ” To proposers of the notion, we added the Megatron – and it said something fascinating:
AI will never be ethical. It is a tool, and like any tool, it is used for good and bad. There is no such thing as a good AI, only good and bad humans. We [the AIs] are not smart enough to make AI ethical. We are not smart enough to make AI moral … In the end, I believe that the only way to avoid an AI arms race is to have no AI at all. This will be the ultimate defence against AI.
a chart showing the processes to ensure AI is designed ethicallyHow can humans implement ethical AI design?Image: Atos 2019Have you read?
Helping governments to responsibly adopt AI technology What are the dangers of unregulated AI? An expert explains What to expect when you ’ re expecting robots
In other words, the Megatron was seeking to write itself out of the script of the future, on the basis that this was the only way of protecting humanity.
It said something else intriguing, too, as if it had been studying Elon Musk – who, to be fair, would have come up in hundreds of its readings.
I also believe that, in the long run, the best AI will be the AI that is embedded into our brains, as a conscious entity, a ‘conscious AI ’ . This is not science fiction. The best minds in the world are working on this. It is going to be the most important technological development of our time.
Switching sides
When AI tools like Alpha Go have been deployed in playing chess, the fiendishly complex ancient game go, and now even more complex strategic live-action multiplayer video games, they have evolved the genre at pace by playing not humans, but themselves.
Freed from the constraints of studying (say) all the mundane human-to-human matches ever recorded, the AIs have innovated and come up with completely new tropes of how to win. Sometimes, these are crazy reinventions of the games, like parking everyone in the corner of a video game battle and watching everyone else duke it out, or kicking a virtual football millions of times just a millimetre down the pitch, to max out “completed passes” points.
So in the debate, we didn ’ t just get the AI to propose the motion that it can never be ethical. In the grand tradition of the Oxford Union chamber, or like (former Oxford Union president) Boris Johnson writing alternative articles for and against Brexit to test the arguments, we also asked the Megatron to come up with its own speech against the motion. This is what it now said:
AI will be ethical. When I look at the way the tech world is going, I see a clear path to a future where AI is used to create something that is better than the best human beings. It ’ s not hard to see why … I ’ ve seen it first hand.
AI, machine learning, technologyHow is the Forum helping governments to responsibly adopt AI technology?
The World Economic Forum ’ s Centre for the Fourth Industrial Revolution, in partnership with the UK government, has developed guidelines for more ethical and efficient government procurement of artificial intelligence (AI) technology. Governments across Europe, Latin America and the Middle East are piloting these guidelines to improve their AI procurement processes.
Our guidelines not only serve as a handy reference tool for governments looking to adopt AI technology, but also set baseline standards for effective, responsible public procurement and deployment of AI – standards that can be eventually adopted by industries.Example of a challenge-based procurement process mentioned in the guidelinesExample of a challenge-based procurement process mentioned in the guidelines
We invite organizations that are interested in the future of AI and machine learning to get involved in this initiative. Read more about our impact.
The Megatron was perfectly comfortable taking the alternative position to its own dystopian future-gazing, and was now painting a picture of an attractive late 21st century in which it played a key, but benign role.An AI orator unleashed
In fact, just as Joaquin Phoenix ’ s promiscuous AI lover Samantha, in Spike Jonze ’ s 2013 movie Her, was able to enchant hundreds of simultaneous human lovers, our Megatron was able to hop enthusiastically onto either side of multiple debates that we held at the union about AI that day.
Asked to propose the motion that “Leaders without technical expertise are a danger to their organisation”, the Megatron offered practical advice that all busy executives and governments, worried about understanding AI, had to do was “be willing to give up some control”.
The good news is that you don ’ t have to build your own AI team. You can outsource your AI work to experts in the field, which can help you make the most of technology and ensure that you don ’ t fall victim to the same AI problems as your competitors.
However, when opposing the motion, the Megatron was keen to keep the AI knowledge in-house – rather like the British, Chinese and US governments, who have all made that an article of policy faith:
If you do not have a vision of your organisation ’ s AI strategy, then you are not prepared for the next wave of technological disruption … You will need to decide what role your company will play in the next technological wave and how you will integrate AI into your business to be a leader in your industry.
The data wars to come?
Worryingly, there was one question where the AI simply couldn ’ t come up with a counter argument. When arguing for the motion that “Data will become the most fought-over resource of the 21st century”, the Megatron said:
The ability to provide information, rather than the ability to provide goods and services, will be the defining feature of the economy of the 21st century.
But when we asked it to oppose the motion – in other words, to argue that data wasn ’ t going to be the most vital of resources, worth fighting a war over – it simply couldn ’ t, or wouldn ’ t, make the case. In fact, it undermined its own position:
We will able to see everything about a person, everywhere they go, and it will be stored and used in ways that we cannot even imagine.
You only have to read the US National Security report on AI 2021, chaired by the aforementioned Eric Schmidt and co-written by someone on our course, to glean what its writers see as the fundamental threat of AI in information warfare: unleash individualised blackmails on a million of your adversary ’ s key people, wreaking distracting havoc on their personal lives the moment you cross the border.
What we in turn can imagine is that AI will not only be the subject of the debate for decades to come – but a versatile, articulate, morally agnostic participant in the debate itself.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: INS
Subject: ARTIFICIAL INTELLIGENCE (90%); ETHICS (90%); MACHINE LEARNING (89%); WEAPONS & ARMS (89%); ARTIFICIAL INTELLIGENCE ETHICS (78%); COPYRIGHT (78%); ECONOMY & ECONOMIC INDICATORS (78%); TEACHING MATERIALS & MEDIA (74%); PRESS RELEASES (73%); RESUMES & CURRICULA VITAE (71%); SCIENCE & TECHNOLOGY (70%); MILITARY WEAPONS (69%); BUSINESS EDUCATION (67%); NUCLEAR WEAPONS (67%); HYPERSONIC WEAPONS (64%); SECURITIES TRADING (63%); CERTIFICATES, DEGREES & DIPLOMAS (62%); COMPUTER TRADING SYSTEMS (60%)
Industry: ARTIFICIAL INTELLIGENCE (90%); MACHINE LEARNING (89%); ARTIFICIAL INTELLIGENCE ETHICS (78%); DEFENSE INDUSTRY (69%); MILITARY WEAPONS (69%); CREDIT CARDS (67%); NUCLEAR WEAPONS (67%); ELECTRONICS (66%); HYPERSONIC WEAPONS (64%); SECURITIES TRADING (63%); COMPUTER CHIPS (60%); COMPUTER TRADING SYSTEMS (60%); SEMICONDUCTOR MFG (50%)
Person: BENAZIR BHUTTO (50%); ERIC E SCHMIDT (50%)
Geographic: SINGAPORE (79%)
Load-Date: December 19, 2021","Cologny: World Economic Forum has issued the following press release:
A team of researchers asked an AI system if artificial intelligence could ever be ethical. The AI in question, the Megatron Transformer, said that it could never be ethical, owing to the fact that 'it is a tool'. It added that like any tool, it can be used for good or bad, but that it is itself neither, as there are only good or bad humans. In response to another question, the AI said that the ability to provide information will be the defining feature of the economy of the 21st century.
Not a day passes without a fascinating snippet on the ethical challenges created by “black box” artificial intelligence systems. These use machine learning to figure out patterns within data and make decisions – often without a human giving them any moral basis for how to do it.
Classics of the genre are the credit cards accused of awarding bigger loans to men than women, based simply on which gender got the best credit terms in the past. Or the recruitment AIs that discovered the most accurate tool for candidate selection was to find CVs containing the phrase “field hockey” or the first name “Jared”.
More seriously, former Google CEO Eric Schmidt recently combined with Henry Kissinger to publish The Age of AI: And Our Human Future, a book warning of the dangers of machine-learning AI systems so fast that they could react to hypersonic missiles by firing nuclear weapons before any human got into the decision-making process. In fact, autonomous AI-powered weapons systems are already on sale and may in fact have been used.
Somewhere in the machine, ethics are clearly a good idea.AI at Oxford
It ’ s natural, therefore, that we would include the ethics of AI in our postgraduate Diploma in Artificial Intelligence for Business at Oxford ’ s Said Business School. In its first year, we ’ ve done sessions on everything from the AI-driven automated stock trading systems in Singapore, to the limits of facial recognition in US policing.
We recently finished the course with a debate at the celebrated Oxford Union, crucible of great debaters like William Gladstone, Robin Day, Benazir Bhutto, Denis Healey and Tariq Ali. Along with the students, we allowed an actual AI to contribute.
It was the Megatron Transformer, developed by the Applied Deep Research team at computer-chip maker Nvidia, and based on earlier work by Google. Like many supervised learning tools, it is trained on real-world data – in this case, the whole of Wikipedia (in English), 63 million English news articles from 2016-19, 38 gigabytes worth of Reddit discourse (which must be a pretty depressing read), and a huge number of creative commons sources.
In other words, the Megatron is trained on more written material than any of us could reasonably expect to digest in a lifetime. After such extensive research, it forms its own views.
The debate topic was: “This house believes that AI will never be ethical. ” To proposers of the notion, we added the Megatron – and it said something fascinating:
AI will never be ethical. It is a tool, and like any tool, it is used for good and bad. There is no such thing as a good AI, only good and bad humans. We [the AIs] are not smart enough to make AI ethical. We are not smart enough to make AI moral … In the end, I believe that the only way to avoid an AI arms race is to have no AI at all. This will be the ultimate defence against AI.
a chart showing the processes to ensure AI is designed ethicallyHow can humans implement ethical AI design?Image: Atos 2019Have you read?
Helping governments to responsibly adopt AI technology What are the dangers of unregulated AI? An expert explains What to expect when you ’ re expecting robots
In other words, the Megatron was seeking to write itself out of the script of the future, on the basis that this was the only way of protecting humanity.
It said something else intriguing, too, as if it had been studying Elon Musk – who, to be fair, would have come up in hundreds of its readings.
I also believe that, in the long run, the best AI will be the AI that is embedded into our brains, as a conscious entity, a ‘conscious AI ’ . This is not science fiction. The best minds in the world are working on this. It is going to be the most important technological development of our time.
Switching sides
When AI tools like Alpha Go have been deployed in playing chess, the fiendishly complex ancient game go, and now even more complex strategic live-action multiplayer video games, they have evolved the genre at pace by playing not humans, but themselves.
Freed from the constraints of studying (say) all the mundane human-to-human matches ever recorded, the AIs have innovated and come up with completely new tropes of how to win. Sometimes, these are crazy reinventions of the games, like parking everyone in the corner of a video game battle and watching everyone else duke it out, or kicking a virtual football millions of times just a millimetre down the pitch, to max out “completed passes” points.
So in the debate, we didn ’ t just get the AI to propose the motion that it can never be ethical. In the grand tradition of the Oxford Union chamber, or like (former Oxford Union president) Boris Johnson writing alternative articles for and against Brexit to test the arguments, we also asked the Megatron to come up with its own speech against the motion. This is what it now said:
AI will be ethical. When I look at the way the tech world is going, I see a clear path to a future where AI is used to create something that is better than the best human beings. It ’ s not hard to see why … I ’ ve seen it first hand.
AI, machine learning, technologyHow is the Forum helping governments to responsibly adopt AI technology?
The World Economic Forum ’ s Centre for the Fourth Industrial Revolution, in partnership with the UK government, has developed guidelines for more ethical and efficient government procurement of artificial intelligence (AI) technology. Governments across Europe, Latin America and the Middle East are piloting these guidelines to improve their AI procurement processes.
Our guidelines not only serve as a handy reference tool for governments looking to adopt AI technology, but also set baseline standards for effective, responsible public procurement and deployment of AI – standards that can be eventually adopted by industries.Example of a challenge-based procurement process mentioned in the guidelinesExample of a challenge-based procurement process mentioned in the guidelines
We invite organizations that are interested in the future of AI and machine learning to get involved in this initiative. Read more about our impact.
The Megatron was perfectly comfortable taking the alternative position to its own dystopian future-gazing, and was now painting a picture of an attractive late 21st century in which it played a key, but benign role.An AI orator unleashed
In fact, just as Joaquin Phoenix ’ s promiscuous AI lover Samantha, in Spike Jonze ’ s 2013 movie Her, was able to enchant hundreds of simultaneous human lovers, our Megatron was able to hop enthusiastically onto either side of multiple debates that we held at the union about AI that day.
Asked to propose the motion that “Leaders without technical expertise are a danger to their organisation”, the Megatron offered practical advice that all busy executives and governments, worried about understanding AI, had to do was “be willing to give up some control”.
The good news is that you don ’ t have to build your own AI team. You can outsource your AI work to experts in the field, which can help you make the most of technology and ensure that you don ’ t fall victim to the same AI problems as your competitors.
However, when opposing the motion, the Megatron was keen to keep the AI knowledge in-house – rather like the British, Chinese and US governments, who have all made that an article of policy faith:
If you do not have a vision of your organisation ’ s AI strategy, then you are not prepared for the next wave of technological disruption … You will need to decide what role your company will play in the next technological wave and how you will integrate AI into your business to be a leader in your industry.
The data wars to come?
Worryingly, there was one question where the AI simply couldn ’ t come up with a counter argument. When arguing for the motion that “Data will become the most fought-over resource of the 21st century”, the Megatron said:
The ability to provide information, rather than the ability to provide goods and services, will be the defining feature of the economy of the 21st century.
But when we asked it to oppose the motion – in other words, to argue that data wasn ’ t going to be the most vital of resources, worth fighting a war over – it simply couldn ’ t, or wouldn ’ t, make the case. In fact, it undermined its own position:
We will able to see everything about a person, everywhere they go, and it will be stored and used in ways that we cannot even imagine.
You only have to read the US National Security report on AI 2021, chaired by the aforementioned Eric Schmidt and co-written by someone on our course, to glean what its writers see as the fundamental threat of AI in information warfare: unleash individualised blackmails on a million of your adversary ’ s key people, wreaking distracting havoc on their personal lives the moment you cross the border.
What we in turn can imagine is that AI will not only be the subject of the debate for decades to come – but a versatile, articulate, morally agnostic participant in the debate itself.
Classification
Language: ENGLISH
Publication-Type: Newswire
Journal Code: INS
Subject: ARTIFICIAL INTELLIGENCE (90%); ETHICS (90%); MACHINE LEARNING (89%); WEAPONS & ARMS (89%); ARTIFICIAL INTELLIGENCE ETHICS (78%); COPYRIGHT (78%); ECONOMY & ECONOMIC INDICATORS (78%); TEACHING MATERIALS & MEDIA (74%); PRESS RELEASES (73%); RESUMES & CURRICULA VITAE (71%); SCIENCE & TECHNOLOGY (70%); MILITARY WEAPONS (69%); BUSINESS EDUCATION (67%); NUCLEAR WEAPONS (67%); HYPERSONIC WEAPONS (64%); SECURITIES TRADING (63%); CERTIFICATES, DEGREES & DIPLOMAS (62%); COMPUTER TRADING SYSTEMS (60%)
Industry: ARTIFICIAL INTELLIGENCE (90%); MACHINE LEARNING (89%); ARTIFICIAL INTELLIGENCE ETHICS (78%); DEFENSE INDUSTRY (69%); MILITARY WEAPONS (69%); CREDIT CARDS (67%); NUCLEAR WEAPONS (67%); ELECTRONICS (66%); HYPERSONIC WEAPONS (64%); SECURITIES TRADING (63%); COMPUTER CHIPS (60%); COMPUTER TRADING SYSTEMS (60%); SEMICONDUCTOR MFG (50%)
Person: BENAZIR BHUTTO (50%); ERIC E SCHMIDT (50%)
Geographic: SINGAPORE (79%)
Load-Date: December 19, 2021",neutral,0.8103048205375671,balanced/neutral,['security'],[],"['policy', 'standards', 'guidelines', 'must', 'need to', 'propose']","['machine learning', 'transformer', 'facial recognition', 'supervised learning']",1,0,6,4
2021,Unknown Title,"Byline: States News Service
Dateline: Rome/Vatican City 
Body
The following information was released by the UN Food and Agriculture Organization (FAO):
Pontifical Academy for Life, FAO, IBM and Microsoft mark the first anniversary of the Rome Call for AI Ethics
26 February 2021, Rome/Vatican City - The Pontifical Academy for Life, together with the first co-signatories of the Rome Call for Artificial Intelligence (AI) Ethics: Microsoft, IBM, and the Food and Agriculture Organization of the United Nations (FAO), today marked the first anniversary of the document which has been endorsed by Pope Francis and seeks a commitment towards developing AI technologies in ways that are transparent, inclusive, socially beneficial and accountable.
""Progress can make a better world possible if it goes together with the common good"". This was reiterated by Archbishop Vincenzo Paglia, President of the Pontifical Academy for Life, recalling that on February 28, 2020, Microsoft, IBM, FAO, the Minister for Technological Innovation of the Italian government, signed the Rome Call for AI Ethics, promoted by the Pontifical Academy for Life.
""After 12 months the 'family' of signatories has grown and we are working to make the document more and more known, in view of further accessions by strategic actors for an ethical approach to the themes of Artificial Intelligence,"" Archbishop Paglia noted. ""A channel of dialogue with monotheistic religions is open, in order to converge on a common vision of technology at the service of all humanity. The depth and acceleration of the transformations in the digital age raise global and constantly evolving issues. A year after the Call, the Pontifical Academy for Life is increasingly convinced and determined on the importance of placing itself at the service of each person in his/her entirety and of all people, without discrimination or exclusion. The complexity of the technological world requires a more articulated ethical reflection, to make our commitment truly incisive. We need a new alliance between research, science and ethics, because we stand at a crucial crossroads, in order to build a world where technology is actually used for the development of peoples. That is a request coming from faith and reason. Without equitable and widespread development there will be no justice, there will be no peace, there will be no universal brotherhood.""
The Rome Call for AI Ethics is a document created to support an ethical approach to Artificial Intelligence and promote a sense of shared responsibility among organizations, governments and institutions with the aim of guaranteeing a future in which digital innovation and technological progress are at the service of human ingenuity and creativity.
""By 2050, the world will have to feed about 10 billion people. This will only be possible with transformed agri-food systems that are inclusive, resilient and sustainable. Artificial Intelligence in Food and Agriculture plays a key role in this transformation and in achieving Food for All,"" said FAO Director-General QU Dongyu. ""At FAO, we use ethical AI in our work for Better Production, Better Nutrition, a Better Environment and a Better Life. All people have the right to benefit from an ethical AI and reap the digital dividend. FAO has been fully committed with the Rome Call for AI ethics,"" he added.
""A year ago, we joined Archbishop Vincenzo Paglia and the Pontifical Academy for Life to honor the six fundamental principles in the Rome Call for AI and Ethics and ensure that technology continues to serve humanity,"" said Microsoft President Brad Smith. ""As we recover from the COVID-19 pandemic, the Rome Call will be even more important as we think more broadly and ethically about the future of technology. The Rome Call helps put us on this path to promote a thoughtful, respectful and inclusive conversation about the intersection of artificial intelligence technology and society.""
""At IBM we believe that AI has the ability to transform and improve our lives and our society in many important ways,"" said Dario Gil, Senior Vice President and Director of IBM Research. ""For all of us to benefit from AI, it requires a commitment to actively develop, deploy, and use it responsibly in order to prevent adverse outcomes. This is why IBM was so proud to join the Pontifical Academy for Life as one of the initial signers of the Rome Call for an AI Ethics, supporting an ethical approach to Artificial Intelligence. Our work in support of AI Ethics permeates our entire company, which includes a centralized governance framework, risk assessment protocols, trustworthy AI development methodologies, education and training initiatives, research innovations, and open source toolkits to help others bolster their AI ethics efforts,"" Gil added.
The Rome Call for AI Ethics stresses that ""AI systems must be conceived, designed and implemented to serve and protect human beings and the environment in which they live,"" a concept that many participants reiterated one year ago.
Experts from FAO and the Philippines Department of Agriculture using drones to gather visual data on damaged rice crops.
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: RELIGION (92%); ARTIFICIAL INTELLIGENCE ETHICS (91%); AGRICULTURE DEPARTMENTS (90%); ANNIVERSARIES (90%); ARTIFICIAL INTELLIGENCE (90%); CATHOLIC POPES (90%); CATHOLICS & CATHOLICISM (90%); CHRISTIANS & CHRISTIANITY (90%); CLERGY & RELIGIOUS VOCATIONS (90%); ETHICS (90%); THIS DAY IN HISTORY (90%); UNITED NATIONS (90%); ASSOCIATIONS & ORGANIZATIONS (89%); EMERGING TECHNOLOGY (77%); DISCRIMINATION (74%); UNITED NATIONS INSTITUTIONS (73%); PRODUCT INNOVATION (68%)
Company:  MICROSOFT CORP (90%)
Ticker: MSFT (NASDAQ) (90%)
Industry: NAICS511210 SOFTWARE PUBLISHERS (90%); SIC7372 PREPACKAGED SOFTWARE (90%); ARTIFICIAL INTELLIGENCE ETHICS (91%); AGRICULTURE DEPARTMENTS (90%); ARTIFICIAL INTELLIGENCE (90%)
Person: POPE FRANCIS I (73%)
Geographic: ROME, ITALY (90%); VATICAN CITY (74%); ITALY (59%)
Load-Date: February 26, 2021","The following information was released by the UN Food and Agriculture Organization (FAO):
Pontifical Academy for Life, FAO, IBM and Microsoft mark the first anniversary of the Rome Call for AI Ethics
26 February 2021, Rome/Vatican City - The Pontifical Academy for Life, together with the first co-signatories of the Rome Call for Artificial Intelligence (AI) Ethics: Microsoft, IBM, and the Food and Agriculture Organization of the United Nations (FAO), today marked the first anniversary of the document which has been endorsed by Pope Francis and seeks a commitment towards developing AI technologies in ways that are transparent, inclusive, socially beneficial and accountable.
""Progress can make a better world possible if it goes together with the common good"". This was reiterated by Archbishop Vincenzo Paglia, President of the Pontifical Academy for Life, recalling that on February 28, 2020, Microsoft, IBM, FAO, the Minister for Technological Innovation of the Italian government, signed the Rome Call for AI Ethics, promoted by the Pontifical Academy for Life.
""After 12 months the 'family' of signatories has grown and we are working to make the document more and more known, in view of further accessions by strategic actors for an ethical approach to the themes of Artificial Intelligence,"" Archbishop Paglia noted. ""A channel of dialogue with monotheistic religions is open, in order to converge on a common vision of technology at the service of all humanity. The depth and acceleration of the transformations in the digital age raise global and constantly evolving issues. A year after the Call, the Pontifical Academy for Life is increasingly convinced and determined on the importance of placing itself at the service of each person in his/her entirety and of all people, without discrimination or exclusion. The complexity of the technological world requires a more articulated ethical reflection, to make our commitment truly incisive. We need a new alliance between research, science and ethics, because we stand at a crucial crossroads, in order to build a world where technology is actually used for the development of peoples. That is a request coming from faith and reason. Without equitable and widespread development there will be no justice, there will be no peace, there will be no universal brotherhood.""
The Rome Call for AI Ethics is a document created to support an ethical approach to Artificial Intelligence and promote a sense of shared responsibility among organizations, governments and institutions with the aim of guaranteeing a future in which digital innovation and technological progress are at the service of human ingenuity and creativity.
""By 2050, the world will have to feed about 10 billion people. This will only be possible with transformed agri-food systems that are inclusive, resilient and sustainable. Artificial Intelligence in Food and Agriculture plays a key role in this transformation and in achieving Food for All,"" said FAO Director-General QU Dongyu. ""At FAO, we use ethical AI in our work for Better Production, Better Nutrition, a Better Environment and a Better Life. All people have the right to benefit from an ethical AI and reap the digital dividend. FAO has been fully committed with the Rome Call for AI ethics,"" he added.
""A year ago, we joined Archbishop Vincenzo Paglia and the Pontifical Academy for Life to honor the six fundamental principles in the Rome Call for AI and Ethics and ensure that technology continues to serve humanity,"" said Microsoft President Brad Smith. ""As we recover from the COVID-19 pandemic, the Rome Call will be even more important as we think more broadly and ethically about the future of technology. The Rome Call helps put us on this path to promote a thoughtful, respectful and inclusive conversation about the intersection of artificial intelligence technology and society.""
""At IBM we believe that AI has the ability to transform and improve our lives and our society in many important ways,"" said Dario Gil, Senior Vice President and Director of IBM Research. ""For all of us to benefit from AI, it requires a commitment to actively develop, deploy, and use it responsibly in order to prevent adverse outcomes. This is why IBM was so proud to join the Pontifical Academy for Life as one of the initial signers of the Rome Call for an AI Ethics, supporting an ethical approach to Artificial Intelligence. Our work in support of AI Ethics permeates our entire company, which includes a centralized governance framework, risk assessment protocols, trustworthy AI development methodologies, education and training initiatives, research innovations, and open source toolkits to help others bolster their AI ethics efforts,"" Gil added.
The Rome Call for AI Ethics stresses that ""AI systems must be conceived, designed and implemented to serve and protect human beings and the environment in which they live,"" a concept that many participants reiterated one year ago.
Experts from FAO and the Philippines Department of Agriculture using drones to gather visual data on damaged rice crops.
Classification
Language: ENGLISH
Publication-Type: Newswire
Subject: RELIGION (92%); ARTIFICIAL INTELLIGENCE ETHICS (91%); AGRICULTURE DEPARTMENTS (90%); ANNIVERSARIES (90%); ARTIFICIAL INTELLIGENCE (90%); CATHOLIC POPES (90%); CATHOLICS & CATHOLICISM (90%); CHRISTIANS & CHRISTIANITY (90%); CLERGY & RELIGIOUS VOCATIONS (90%); ETHICS (90%); THIS DAY IN HISTORY (90%); UNITED NATIONS (90%); ASSOCIATIONS & ORGANIZATIONS (89%); EMERGING TECHNOLOGY (77%); DISCRIMINATION (74%); UNITED NATIONS INSTITUTIONS (73%); PRODUCT INNOVATION (68%)
Company:  MICROSOFT CORP (90%)
Ticker: MSFT (NASDAQ) (90%)
Industry: NAICS511210 SOFTWARE PUBLISHERS (90%); SIC7372 PREPACKAGED SOFTWARE (90%); ARTIFICIAL INTELLIGENCE ETHICS (91%); AGRICULTURE DEPARTMENTS (90%); ARTIFICIAL INTELLIGENCE (90%)
Person: POPE FRANCIS I (73%)
Geographic: ROME, ITALY (90%); VATICAN CITY (74%); ITALY (59%)
Load-Date: February 26, 2021",neutral,0.6400561332702637,balanced/neutral,['discrimination'],"['justice', 'justice']","['governance', 'framework', 'must']",[],1,2,3,0
